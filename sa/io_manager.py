"""ê°„ì†Œí™”ëœ ë³‘ë ¬ ì²˜ë¦¬ - ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥"""

import os
import time
import logging
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
import traceback
import hashlib

# ë¬´ê²°ì„± ëª¨ë“ˆ import
try:
    from punctuation import integrity_guard, safe_mask_brackets, safe_restore_brackets, get_integrity_status
except ImportError:
    # í´ë°± ì²˜ë¦¬
    integrity_guard = None
    def safe_mask_brackets(text, text_type='source'):
        return text, []
    def safe_restore_brackets(text, masks):
        return text
    def get_integrity_status():
        return {}

logger = logging.getLogger(__name__)

class SafeFileProcessor:
    """ë¬´ê²°ì„± ë³´ì¥ íŒŒì¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, max_workers: int = 4, chunk_size: int = 100):
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.processed_count = 0
        self.error_count = 0
        self.integrity_failures = 0
        self.integrity_enabled = integrity_guard is not None  # ğŸ”§ ë¬´ê²°ì„± ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
    
    def process_file_with_integrity(
        self, 
        input_file: str, 
        output_file: str,
        processing_function,
        **kwargs
    ) -> bool:
        """ë¬´ê²°ì„± ë³´ì¥ íŒŒì¼ ì²˜ë¦¬"""
        
        logger.info(f"ë¬´ê²°ì„± ë³´ì¥ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
        
        try:
            # 1. ì…ë ¥ íŒŒì¼ ë¡œë“œ ë° ë¬´ê²°ì„± ë“±ë¡
            df = pd.read_excel(input_file)
            logger.info(f"ì…ë ¥ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ í–‰")
            
            # ì „ì²´ ë°ì´í„° ë¬´ê²°ì„± ë“±ë¡
            file_id = f"file_{hashlib.md5(input_file.encode()).hexdigest()[:8]}"
            self._register_file_integrity(df, file_id)
            
            # 2. ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            results = []
            chunks = self._create_chunks(df)
            
            logger.info(f"ë°ì´í„°ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬")
            
            for i, chunk in enumerate(chunks):
                logger.info(f"ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
                
                chunk_results = self._process_chunk_with_integrity(
                    chunk, processing_function, f"{file_id}_chunk_{i}", **kwargs
                )
                
                if chunk_results:
                    results.extend(chunk_results)
                    self.processed_count += len(chunk_results)
                else:
                    self.error_count += len(chunk)
            
            # 3. ê²°ê³¼ ì €ì¥ ë° ê²€ì¦
            if results:
                result_df = pd.DataFrame(results)
                
                # ìµœì¢… ë¬´ê²°ì„± ê²€ì¦
                final_integrity = self._verify_final_integrity(df, result_df, file_id)
                
                if final_integrity['valid']:
                    result_df.to_excel(output_file, index=False)
                    logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
                    logger.info(f"ì²˜ë¦¬ í†µê³„: ì„±ê³µ {self.processed_count}, ì‹¤íŒ¨ {self.error_count}, ë¬´ê²°ì„±ì‹¤íŒ¨ {self.integrity_failures}")
                    return True
                else:
                    logger.error(f"ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {final_integrity['message']}")
                    # ë³µêµ¬ëœ ê²°ê³¼ë¡œ ì €ì¥
                    if final_integrity.get('restored_df') is not None:
                        final_integrity['restored_df'].to_excel(output_file, index=False)
                        logger.info(f"ë³µêµ¬ëœ ê²°ê³¼ë¡œ ì €ì¥: {output_file}")
                        return True
                    else:
                        return False
            else:
                logger.error("ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _register_file_integrity(self, df: pd.DataFrame, file_id: str):
        """íŒŒì¼ ì „ì²´ ë¬´ê²°ì„± ë“±ë¡ (ì•ˆì „ ë²„ì „)"""
        
        if not self.integrity_enabled:
            logger.debug("ë¬´ê²°ì„± ëª¨ë“ˆ ë¹„í™œì„±í™”ë¨, ë“±ë¡ ìŠ¤í‚µ")
            return
        
        # ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ì „ì²´ ê²°í•©
        if 'ì›ë¬¸' in df.columns and 'ë²ˆì—­ë¬¸' in df.columns:
            total_src = ''.join(df['ì›ë¬¸'].fillna('').astype(str))
            total_tgt = ''.join(df['ë²ˆì—­ë¬¸'].fillna('').astype(str))
            
            try:
                integrity_guard.register_original(f"{file_id}_src_all", total_src, "file_input_src")
                integrity_guard.register_original(f"{file_id}_tgt_all", total_tgt, "file_input_tgt")
                logger.info(f"íŒŒì¼ ë¬´ê²°ì„± ë“±ë¡: ì›ë¬¸ {len(total_src)}ì, ë²ˆì—­ë¬¸ {len(total_tgt)}ì")
            except Exception as e:
                logger.warning(f"ë¬´ê²°ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
                self.integrity_enabled = False
    
    def _create_chunks(self, df: pd.DataFrame) -> List[pd.DataFrame]:
        """ë°ì´í„°í”„ë ˆì„ì„ ì²­í¬ë¡œ ë¶„í• """
        
        chunks = []
        for i in range(0, len(df), self.chunk_size):
            chunk = df.iloc[i:i + self.chunk_size].copy()
            chunks.append(chunk)
        
        return chunks
    
    def _process_chunk_with_integrity(
        self, 
        chunk: pd.DataFrame, 
        processing_function,
        chunk_id: str,
        **kwargs
    ) -> List[Dict]:
        """ì²­í¬ ë‹¨ìœ„ ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ (ì•ˆì „ ë²„ì „)"""
        
        results = []
        
        for idx, row in chunk.iterrows():
            try:
                # í–‰ë³„ ë¬´ê²°ì„± ë“±ë¡ (ì•ˆì „)
                row_id = f"{chunk_id}_row_{idx}"
                src_text = str(row.get('ì›ë¬¸', ''))
                tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))
                
                if src_text.strip() and tgt_text.strip():
                    if self.integrity_enabled:
                        try:
                            integrity_guard.register_original(f"{row_id}_src", src_text, "row_input_src")
                            integrity_guard.register_original(f"{row_id}_tgt", tgt_text, "row_input_tgt")
                        except Exception as e:
                            logger.warning(f"í–‰ ë¬´ê²°ì„± ë“±ë¡ ì‹¤íŒ¨: {e}")
                            self.integrity_enabled = False
                    
                    # ì²˜ë¦¬ í•¨ìˆ˜ ì‹¤í–‰
                    row_result = processing_function(row, row_id=row_id, **kwargs)
                    
                    if row_result:
                        # í–‰ë³„ ë¬´ê²°ì„± ê²€ì¦ (ì•ˆì „)
                        if self.integrity_enabled and self._verify_row_integrity(row, row_result, row_id):
                            results.extend(row_result if isinstance(row_result, list) else [row_result])
                        elif not self.integrity_enabled:
                            # ë¬´ê²°ì„± ë¹„í™œì„±í™”ì‹œ ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                            results.extend(row_result if isinstance(row_result, list) else [row_result])
                        else:
                            self.integrity_failures += 1
                            logger.warning(f"í–‰ {idx} ë¬´ê²°ì„± ì‹¤íŒ¨, ìŠ¤í‚µ")
                
            except Exception as e:
                logger.error(f"í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                self.error_count += 1
                continue
        
        return results
    
    def _verify_row_integrity(self, original_row: pd.Series, processed_results: List[Dict], row_id: str) -> bool:
        """í–‰ë³„ ë¬´ê²°ì„± ê²€ì¦ (ì•ˆì „ ë²„ì „)"""
        
        if not self.integrity_enabled:
            return True  # ë¬´ê²°ì„± ë¹„í™œì„±í™”ì‹œ í•­ìƒ ì„±ê³µ
        
        try:
            original_src = str(original_row.get('ì›ë¬¸', ''))
            original_tgt = str(original_row.get('ë²ˆì—­ë¬¸', ''))
            
            if isinstance(processed_results, dict):
                processed_results = [processed_results]
            
            # ì²˜ë¦¬ëœ ê²°ê³¼ì—ì„œ ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ê²°í•©
            processed_src = ''.join([str(result.get('ì›ë¬¸', '')) for result in processed_results])
            processed_tgt = ''.join([str(result.get('ë²ˆì—­ë¬¸', '')) for result in processed_results])
            
            # ë¬´ê²°ì„± ê²€ì¦
            src_valid, src_msg, _ = integrity_guard.verify_integrity(f"{row_id}_src", processed_src, "row_output_src")
            tgt_valid, tgt_msg, _ = integrity_guard.verify_integrity(f"{row_id}_tgt", processed_tgt, "row_output_tgt")
            
            if not src_valid:
                logger.warning(f"ì›ë¬¸ ë¬´ê²°ì„± ì‹¤íŒ¨: {src_msg}")
            
            if not tgt_valid:
                logger.warning(f"ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ì‹¤íŒ¨: {tgt_msg}")
            
            return src_valid and tgt_valid
            
        except Exception as e:
            logger.warning(f"ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            self.integrity_enabled = False
            return True  # ì˜¤ë¥˜ì‹œ í†µê³¼ ì²˜ë¦¬
    
    def _verify_final_integrity(self, original_df: pd.DataFrame, result_df: pd.DataFrame, file_id: str) -> Dict:
        """ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ (ì•ˆì „ ë²„ì „)"""
        
        if not self.integrity_enabled:
            logger.info("ë¬´ê²°ì„± ëª¨ë“ˆ ë¹„í™œì„±í™”ë¨, ìµœì¢… ê²€ì¦ ìŠ¤í‚µ")
            return {
                'valid': True,
                'message': 'ë¬´ê²°ì„± ê²€ì¦ ë¹„í™œì„±í™”ë¨',
                'src_info': {},
                'tgt_info': {}
            }
        
        try:
            logger.info("ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘...")
            
            # ì›ë³¸ ë°ì´í„° ê²°í•©
            original_src_all = ''.join(original_df['ì›ë¬¸'].fillna('').astype(str))
            original_tgt_all = ''.join(original_df['ë²ˆì—­ë¬¸'].fillna('').astype(str))
            
            # ê²°ê³¼ ë°ì´í„° ê²°í•©
            result_src_all = ''.join(result_df['ì›ë¬¸'].fillna('').astype(str))
            result_tgt_all = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna('').astype(str))
            
            # ë¬´ê²°ì„± ê²€ì¦
            src_valid, src_msg, src_info = integrity_guard.verify_integrity(
                f"{file_id}_src_all", result_src_all, "file_output_src"
            )
            tgt_valid, tgt_msg, tgt_info = integrity_guard.verify_integrity(
                f"{file_id}_tgt_all", result_tgt_all, "file_output_tgt"
            )
            
            overall_valid = src_valid and tgt_valid
            
            if overall_valid:
                return {
                    'valid': True,
                    'message': 'ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ',
                    'src_info': src_info,
                    'tgt_info': tgt_info
                }
            else:
                logger.warning("ìµœì¢… ë¬´ê²°ì„± ì‹¤íŒ¨, ë³µêµ¬ ì‹œë„...")
                
                # ë³µêµ¬ ì‹œë„
                restored_df = self._restore_final_integrity(
                    original_df, result_df, file_id, src_valid, tgt_valid
                )
                
                return {
                    'valid': False,
                    'message': f'ë¬´ê²°ì„± ì‹¤íŒ¨ - ì›ë¬¸: {src_msg}, ë²ˆì—­ë¬¸: {tgt_msg}',
                    'src_info': src_info,
                    'tgt_info': tgt_info,
                    'restored_df': restored_df
                }
                
        except Exception as e:
            logger.error(f"ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            self.integrity_enabled = False
            return {
                'valid': True,  # ì˜¤ë¥˜ì‹œ í†µê³¼ ì²˜ë¦¬
                'message': f'ë¬´ê²°ì„± ê²€ì¦ ì˜¤ë¥˜ë¡œ ìŠ¤í‚µ: {e}',
                'src_info': {},
                'tgt_info': {}
            }
    
    def _restore_final_integrity(
        self, 
        original_df: pd.DataFrame, 
        result_df: pd.DataFrame, 
        file_id: str,
        src_valid: bool,
        tgt_valid: bool
    ) -> pd.DataFrame:
        """ìµœì¢… ë¬´ê²°ì„± ë³µêµ¬ (ì•ˆì „ ë²„ì „)"""
        
        if not self.integrity_enabled:
            return result_df
        
        try:
            restored_df = result_df.copy()
            
            # ì›ë¬¸ ë³µêµ¬
            if not src_valid:
                logger.info("ì›ë¬¸ ë¬´ê²°ì„± ë³µêµ¬ ì¤‘...")
                
                result_src_parts = result_df['ì›ë¬¸'].fillna('').astype(str).tolist()
                original_src_all = ''.join(original_df['ì›ë¬¸'].fillna('').astype(str))
                
                restored_src_parts, success = integrity_guard.restore_integrity(
                    f"{file_id}_src_all", result_src_parts, "sequence_matcher"
                )
                
                if success and len(restored_src_parts) == len(result_df):
                    restored_df['ì›ë¬¸'] = restored_src_parts
                    logger.info("ì›ë¬¸ ë³µêµ¬ ì„±ê³µ")
                else:
                    logger.warning("ì›ë¬¸ ë³µêµ¬ ì‹¤íŒ¨, ì›ë³¸ìœ¼ë¡œ ëŒ€ì²´")
                    # ì›ë³¸ì„ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ ë¶„í• 
                    src_parts = self._split_text_by_count(original_src_all, len(result_df))
                    restored_df['ì›ë¬¸'] = src_parts
            
            # ë²ˆì—­ë¬¸ ë³µêµ¬
            if not tgt_valid:
                logger.info("ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ë³µêµ¬ ì¤‘...")
                
                result_tgt_parts = result_df['ë²ˆì—­ë¬¸'].fillna('').astype(str).tolist()
                original_tgt_all = ''.join(original_df['ë²ˆì—­ë¬¸'].fillna('').astype(str))
                
                restored_tgt_parts, success = integrity_guard.restore_integrity(
                    f"{file_id}_tgt_all", result_tgt_parts, "sequence_matcher"
                )
                
                if success and len(restored_tgt_parts) == len(result_df):
                    restored_df['ë²ˆì—­ë¬¸'] = restored_tgt_parts
                    logger.info("ë²ˆì—­ë¬¸ ë³µêµ¬ ì„±ê³µ")
                else:
                    logger.warning("ë²ˆì—­ë¬¸ ë³µêµ¬ ì‹¤íŒ¨, ì›ë³¸ìœ¼ë¡œ ëŒ€ì²´")
                    # ì›ë³¸ì„ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ ë¶„í• 
                    tgt_parts = self._split_text_by_count(original_tgt_all, len(result_df))
                    restored_df['ë²ˆì—­ë¬¸'] = tgt_parts
            
            return restored_df
            
        except Exception as e:
            logger.error(f"ë¬´ê²°ì„± ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return result_df  # ì˜¤ë¥˜ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
    
    def _split_text_by_count(self, text: str, count: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê°œìˆ˜ë¡œ ê· ë“± ë¶„í• """
        
        if count <= 0:
            return []
        
        if count == 1:
            return [text]
        
        text_length = len(text)
        part_length = text_length // count
        remainder = text_length % count
        
        parts = []
        start = 0
        
        for i in range(count):
            current_length = part_length + (1 if i < remainder else 0)
            end = start + current_length
            
            if end > text_length:
                end = text_length
            
            if start < text_length:
                parts.append(text[start:end])
            else:
                parts.append('')
            
            start = end
        
        return parts

# ===== ê¸°ì¡´ SA í˜¸í™˜ í•¨ìˆ˜ë“¤ =====

def process_file(
    input_file: str,
    output_file: str,
    embedder_name: str = "bge",
    max_workers: int = 4,
    chunk_size: int = 100,
    use_parallel: bool = True,
    **kwargs
) -> bool:
    """ê¸°ì¡´ SA í˜¸í™˜ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ë¬´ê²°ì„± ë³´ì¥)"""
    
    logger.info(f"SA íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    logger.info(f"ì„¤ì •: ì„ë² ë”={embedder_name}, ë³‘ë ¬={use_parallel}, ì›Œì»¤={max_workers}")
    
    try:
        # SA ì²˜ë¦¬ í•¨ìˆ˜ import
        from sa_tokenizers.jieba_mecab import process_single_row
        
        # ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ê¸° ìƒì„±
        processor = SafeFileProcessor(max_workers=max_workers, chunk_size=chunk_size)
        
        # ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ ì‹¤í–‰
        success = processor.process_file_with_integrity(
            input_file=input_file,
            output_file=output_file,
            processing_function=safe_process_sa_row,
            embedder_name=embedder_name,
            **kwargs
        )
        
        if success:
            logger.info(f"âœ… SA íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
        else:
            logger.error(f"âŒ SA íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")
        
        return success
        
    except ImportError as e:
        logger.error(f"SA ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ë³¸ ì²˜ë¦¬
        success = process_file_fallback(input_file, output_file, **kwargs)
        return success  # ğŸ”§ bool ë°˜í™˜
    
    except Exception as e:
        logger.error(f"SA íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return False

def process_file_fallback(input_file: str, output_file: str, **kwargs) -> bool:
    """í´ë°± íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜"""
    
    logger.warning("í´ë°± ëª¨ë“œë¡œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    
    try:
        df = pd.read_excel(input_file)
        
        results = []
        for idx, row in df.iterrows():
            src_text = str(row.get('ì›ë¬¸', ''))
            tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))
            
            if src_text.strip() and tgt_text.strip():
                results.append({
                    'ë¬¸ì¥ì‹ë³„ì': idx + 1,
                    'ì›ë¬¸': src_text,
                    'ë²ˆì—­ë¬¸': tgt_text,
                    'ë¶„í• ë°©ë²•': 'fallback_mode',
                    'ìœ ì‚¬ë„': 1.0,
                    'ì›ë¬¸_í† í°ìˆ˜': len(src_text.split()),
                    'ë²ˆì—­ë¬¸_í† í°ìˆ˜': len(tgt_text.split())
                })
        
        if results:
            result_df = pd.DataFrame(results)
            result_df.to_excel(output_file, index=False)
            logger.info(f"í´ë°± ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return True  # ğŸ”§ ì„±ê³µì‹œ True ë°˜í™˜
        else:
            logger.error("ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")
            return False
            
    except Exception as e:
        logger.error(f"í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return False

# ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def safe_process_single_text(text: str, processing_function, text_id: str = None, **kwargs) -> Any:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ (ì•ˆì „ ë²„ì „)"""
    
    if not integrity_guard:
        return processing_function(text, **kwargs)
    
    if text_id is None:
        text_id = f"single_{id(text)}"
    
    try:
        integrity_guard.register_original(text_id, text, "single_input")
        
        result = processing_function(text, **kwargs)
        
        # ê²°ê³¼ ê²€ì¦
        if isinstance(result, str):
            is_valid, message, info = integrity_guard.verify_integrity(text_id, result, "single_output")
            
            if not is_valid:
                logger.warning(f"ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
                # ë³µêµ¬ ì‹œë„
                restored_parts, success = integrity_guard.restore_integrity(text_id, [result])
                if success and restored_parts:
                    result = restored_parts[0]
                else:
                    result = text  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
        return result
        
    except Exception as e:
        logger.error(f"ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return processing_function(text, **kwargs)  # ë¬´ê²°ì„± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²˜ë¦¬

def safe_process_text_list(text_list: List[str], processing_function, list_id: str = None, **kwargs) -> List[str]:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë¬´ê²°ì„± ë³´ì¥ ì²˜ë¦¬ (ì•ˆì „ ë²„ì „)"""
    
    if not integrity_guard:
        return [processing_function(text, **kwargs) for text in text_list]
    
    if list_id is None:
        list_id = f"list_{id(text_list)}"
    
    try:
        # ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë¬´ê²°ì„± ë“±ë¡
        combined_text = ''.join(text_list)
        integrity_guard.register_original(list_id, combined_text, "list_input")
        
        results = []
        
        for i, text in enumerate(text_list):
            if text.strip():
                item_id = f"{list_id}_item_{i}"
                result = safe_process_single_text(text, processing_function, item_id, **kwargs)
                results.append(result)
            else:
                results.append(text)
        
        # ì „ì²´ ê²°ê³¼ ê²€ì¦
        combined_result = ''.join(results)
        is_valid, message, info = integrity_guard.verify_integrity(list_id, combined_result, "list_output")
        
        if not is_valid:
            logger.warning(f"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
            # ë³µêµ¬ ì‹œë„
            restored_parts, success = integrity_guard.restore_integrity(list_id, results)
            if success:
                results = restored_parts
            else:
                results = text_list  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        
        return results
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return [processing_function(text, **kwargs) for text in text_list]

# ë³‘ë ¬ ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í˜¸í™˜ì„±)
def process_chunks_parallel(chunks: List[Any], processing_function, max_workers: int = 4, **kwargs) -> List[Any]:
    """ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬"""
    
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(chunks)}ê°œ ì²­í¬, {max_workers}ê°œ ì›Œì»¤")
    
    results = []
    
    if max_workers == 1:
        # ìˆœì°¨ ì²˜ë¦¬
        for i, chunk in enumerate(chunks):
            logger.info(f"ì²­í¬ {i+1}/{len(chunks)} ìˆœì°¨ ì²˜ë¦¬ ì¤‘...")
            chunk_result = processing_function(chunk, **kwargs)
            if chunk_result:
                results.extend(chunk_result if isinstance(chunk_result, list) else [chunk_result])
    else:
        # ë³‘ë ¬ ì²˜ë¦¬
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_chunk = {
                executor.submit(processing_function, chunk, **kwargs): i 
                for i, chunk in enumerate(chunks)
            }
            
            for future in as_completed(future_to_chunk):
                chunk_idx = future_to_chunk[future]
                try:
                    chunk_result = future.result()
                    if chunk_result:
                        results.extend(chunk_result if isinstance(chunk_result, list) else [chunk_result])
                    logger.info(f"ì²­í¬ {chunk_idx + 1} ì²˜ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ì²­í¬ {chunk_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
    return results

def split_dataframe_into_chunks(df: pd.DataFrame, chunk_size: int = 100) -> List[pd.DataFrame]:
    """ë°ì´í„°í”„ë ˆì„ì„ ì²­í¬ë¡œ ë¶„í• """
    
    chunks = []
    for i in range(0, len(df), chunk_size):
        chunk = df.iloc[i:i + chunk_size].copy()
        chunks.append(chunk)
    
    logger.info(f"ë°ì´í„°í”„ë ˆì„ ë¶„í• : {len(df)}í–‰ â†’ {len(chunks)}ê°œ ì²­í¬")
    return chunks

def safe_process_sa_row(row: pd.Series, row_id: str = None, **kwargs) -> List[Dict]:
    """SA í–‰ ì²˜ë¦¬ ë¬´ê²°ì„± ë³´ì¥ ë˜í¼"""
    
    try:
        # SA í† í¬ë‚˜ì´ì € ì²˜ë¦¬ í•¨ìˆ˜ import
        from sa_tokenizers.jieba_mecab import process_single_row  # ğŸ”§ ì˜¬ë°”ë¥¸ í•¨ìˆ˜ëª…
        
        # ì›ë³¸ ë°ì´í„° ì¶”ì¶œ
        src_text = str(row.get('ì›ë¬¸', ''))
        tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))
        
        if not src_text.strip() or not tgt_text.strip():
            return []
        
        # ë¬´ê²°ì„± ë³´ì¥ ê´„í˜¸ ì²˜ë¦¬ (ì•ˆì „ ë²„ì „)
        if integrity_guard:
            masked_src, src_masks = safe_mask_brackets(src_text, 'source')
            masked_tgt, tgt_masks = safe_mask_brackets(tgt_text, 'target')
            
            # SA ì²˜ë¦¬ ì‹¤í–‰
            masked_row = row.copy()
            masked_row['ì›ë¬¸'] = masked_src
            masked_row['ë²ˆì—­ë¬¸'] = masked_tgt
            
            results = process_single_row(masked_row, row_id=row_id, **kwargs)
            
            if not results:
                return []
            
            # ê´„í˜¸ ë³µì›
            restored_results = []
            for result in results:
                if isinstance(result, dict):
                    restored_src = safe_restore_brackets(str(result.get('ì›ë¬¸', '')), src_masks)
                    restored_tgt = safe_restore_brackets(str(result.get('ë²ˆì—­ë¬¸', '')), tgt_masks)
                    
                    restored_result = result.copy()
                    restored_result['ì›ë¬¸'] = restored_src
                    restored_result['ë²ˆì—­ë¬¸'] = restored_tgt
                    
                    restored_results.append(restored_result)
                else:
                    restored_results.append(result)
            
            return restored_results
        else:
            # ë¬´ê²°ì„± ëª¨ë“ˆ ì—†ìœ¼ë©´ ì§ì ‘ ì²˜ë¦¬
            return process_single_row(row, row_id=row_id, **kwargs)
        
    except Exception as e:
        logger.error(f"SA í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # í´ë°±: ì›ë³¸ ë°ì´í„° ë°˜í™˜
        return [{
            'ì›ë¬¸': str(row.get('ì›ë¬¸', '')),
            'ë²ˆì—­ë¬¸': str(row.get('ë²ˆì—­ë¬¸', '')),
            'ë¬¸ì¥ì‹ë³„ì': f"{row_id}_error" if row_id else getattr(row, 'name', 0),
            'ë¶„í• ë°©ë²•': 'error_fallback',
            'ìœ ì‚¬ë„': 1.0,
            'ì›ë¬¸_í† í°ìˆ˜': len(str(row.get('ì›ë¬¸', '')).split()),
            'ë²ˆì—­ë¬¸_í† í°ìˆ˜': len(str(row.get('ë²ˆì—­ë¬¸', '')).split())
        }]