{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 원문-번역문 의미 기반 구 단위 매칭 파이프라인\n",
        "\n",
        "이 노트북은 한문 원문과 번역문 간의 구 단위 정렬을 위한 파이프라인을 구현합니다. 다음과 같은 주요 기능을 포함합니다:\n",
        "\n",
        "1. **텍스트 토크나이징**: 원문과 번역문을 의미 단위로 분할\n",
        "2. **임베딩 계산**: 각 의미 단위에 대한 벡터 표현 생성\n",
        "3. **구 단위 정렬**: 동적 프로그래밍 기반 정렬 알고리즘\n",
        "4. **파일 입출력**: Excel 파일 처리\n",
        "\n",
        "이 파이프라인은 원문-번역문 쌍을 입력으로 받아 구 단위로 정렬된 결과를 출력합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 패키지 설치 및 임포트\n",
        "\n",
        "아래 셀을 실행하여 필요한 패키지를 설치하고 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages (GPU version with CUDA 12.1)\n",
        "%pip install regex pandas numpy tqdm openpyxl konlpy\n",
        "%pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install sentence-transformers FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import torch\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any, Optional, Callable\n",
        "from tqdm.notebook import tqdm\n",
        "from konlpy.tag import Kkma\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "import sentencepiece as spm\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "\n",
        "# 환경 변수 설정\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# 한국어 형태소 분석기\n",
        "tokenizer = Kkma()\n",
        "\n",
        "# 모델 로딩 with 다중 fallback\n",
        "def load_embedding_model():\n",
        "    \"\"\"여러 옵션을 시도하여 임베딩 모델 로드\"\"\"\n",
        "    \n",
        "    # Option 1: BGE 모델\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        print(\"✅ BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        print(f\"❌ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        print(\"✅ SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        print(f\"❌ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: 더미 모델\n",
        "    print(\"🔄 Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# 모델 로드\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# 범용 임베딩 함수\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"모델 타입에 따른 임베딩 계산\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "print(f\"✅ Setup complete. Using {model_type} embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 임베딩 모듈 (embedder.py)\n",
        "\n",
        "텍스트 구에 대한 임베딩을 계산하고 캐시를 관리하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# 로거 설정 (먼저 설정해야 함)\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 환경 변수 설정\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# 한국어 형태소 분석기\n",
        "tokenizer = Kkma()\n",
        "\n",
        "# 모델 로딩 with 다중 fallback\n",
        "def load_embedding_model():\n",
        "    \"\"\"여러 옵션을 시도하여 임베딩 모델 로드\"\"\"\n",
        "    \n",
        "    # Option 1: BGE 모델\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"✅ BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"✅ SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: 더미 모델\n",
        "    logger.info(\"🔄 Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# 모델 로드\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# 범용 임베딩 함수\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"모델 타입에 따른 임베딩 계산\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "logger.info(f\"✅ Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# 테스트\n",
        "test_texts = [\"테스트 문장\", \"또 다른 문장\"]\n",
        "test_embeddings = compute_embeddings(test_texts)\n",
        "logger.info(f\"Test embeddings shape: {test_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 구두점 처리 모듈 (punctuation.py)\n",
        "\n",
        "괄호 처리 및 마스킹 기능을 제공하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Punctuation handling module\n",
        "MASK_TEMPLATE = '[MASK{}]'\n",
        "\n",
        "HALF_WIDTH_BRACKETS = [\n",
        "    ('(', ')'),\n",
        "    ('[', ']'),\n",
        "]\n",
        "FULL_WIDTH_BRACKETS = [\n",
        "    ('（', '）'),\n",
        "    ('［', '］'),\n",
        "]\n",
        "TRANS_BRACKETS = [\n",
        "    ('<', '>'),\n",
        "    ('《', '》'),\n",
        "    ('〈', '〉'),\n",
        "    ('「', '」'),\n",
        "    ('『', '』'),\n",
        "    ('〔', '〕'),\n",
        "    ('【', '】'),\n",
        "    ('〖', '〗'),\n",
        "    ('〘', '〙'),\n",
        "    ('〚', '〛'),\n",
        "]\n",
        "\n",
        "ALL_BRACKETS = HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS + TRANS_BRACKETS\n",
        "\n",
        "def mask_brackets(text: str, text_type: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Mask content within brackets according to rules.\"\"\"\n",
        "    assert text_type in {'source', 'target'}, \"text_type must be 'source' or 'target'\"\n",
        "\n",
        "    masks: List[str] = []\n",
        "    mask_id = [0]\n",
        "\n",
        "    def safe_sub(pattern, repl, s):\n",
        "        def safe_replacer(m):\n",
        "            if '[MASK' in m.group(0):\n",
        "                return m.group(0)\n",
        "            return repl(m)\n",
        "        return pattern.sub(safe_replacer, s)\n",
        "\n",
        "    patterns: List[Tuple[re.Pattern, bool]] = []\n",
        "\n",
        "    if text_type == 'source':\n",
        "        for left, right in HALF_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "    elif text_type == 'target':\n",
        "        for left, right in HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in TRANS_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "\n",
        "    def mask_content(s: str, pattern: re.Pattern, content_mask: bool) -> str:\n",
        "        def replacer(match: re.Match) -> str:\n",
        "            token = MASK_TEMPLATE.format(mask_id[0])\n",
        "            masks.append(match.group())\n",
        "            mask_id[0] += 1\n",
        "            return token\n",
        "        return safe_sub(pattern, replacer, s)\n",
        "\n",
        "    for pattern, content_mask in patterns:\n",
        "        if content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "    for pattern, content_mask in patterns:\n",
        "        if not content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "\n",
        "    return text, masks\n",
        "\n",
        "def restore_masks(text: str, masks: List[str]) -> str:\n",
        "    \"\"\"Restore masked tokens to their original content.\"\"\"\n",
        "    for i, original in enumerate(masks):\n",
        "        text = text.replace(MASK_TEMPLATE.format(i), original)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 토크나이저 모듈 (tokenizer.py)\n",
        "\n",
        "원문과 번역문을 의미 단위로 분할하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"원문과 번역문을 의미 단위로 분할하는 모듈\"\"\"\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import regex\n",
        "import re\n",
        "from typing import List, Callable\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "# Kkma 형태소 분석기\n",
        "kkma = Kkma()\n",
        "\n",
        "# 미리 컴파일된 정규식\n",
        "hanja_re    = regex.compile(r'\\p{Han}+')\n",
        "hangul_re   = regex.compile(r'^\\p{Hangul}+$')\n",
        "combined_re = regex.compile(\n",
        "    r'(\\p{Han}+)+(?:\\p{Hangul}+)(?:은|는|이|가|을|를|에|에서|으로|로|와|과|도|만|으며|고|하고|의|때)?'\n",
        ")\n",
        "\n",
        "def split_src_meaning_units(text: str) -> list[str]:\n",
        "    text = text.replace('\\n', ' ').replace('：', '： ')\n",
        "    tokens = regex.findall(r'\\S+', text)\n",
        "    units: list[str] = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "        m = combined_re.match(tok)\n",
        "        if m:\n",
        "            units.append(m.group(0))\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if hanja_re.search(tok):\n",
        "            unit = tok\n",
        "            j = i + 1\n",
        "            while j < len(tokens) and hangul_re.match(tokens[j]):\n",
        "                unit += tokens[j]\n",
        "                j += 1\n",
        "            units.append(unit)\n",
        "            i = j\n",
        "            continue\n",
        "\n",
        "        if hangul_re.match(tok):\n",
        "            korean_tokens = kkma.morphs(tok)\n",
        "            units.extend(korean_tokens)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        units.append(tok)\n",
        "        i += 1\n",
        "\n",
        "    return units\n",
        "\n",
        "def split_inside_chunk(chunk: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    조사, 어미, 그리고 '：' 기준으로 의미 단위 분할\n",
        "    원형 보존, 공백 삽입 없이 분리\n",
        "    \"\"\"\n",
        "    delimiters = ['을', '를', '이', '가', '은', '는', '에', '에서', '로', '으로',\n",
        "                  '와', '과', '고', '며', '하고', '때', '의', '도', '만', '：']\n",
        "    \n",
        "    # lookbehind 패턴 생성\n",
        "    pattern = '|'.join([f'(?<={re.escape(d)})' for d in delimiters])\n",
        "    try:\n",
        "        parts = re.split(pattern, chunk)\n",
        "        return [p.strip() for p in parts if p.strip()]\n",
        "    except:\n",
        "        return [p for p in chunk.split() if p.strip()]\n",
        "\n",
        "def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:\n",
        "    hanja_chars = regex.findall(r'\\p{Han}+', src_unit)\n",
        "    if not hanja_chars:\n",
        "        return 0\n",
        "    last = hanja_chars[-1]\n",
        "    idx = remaining_tgt.rfind(last)\n",
        "    if idx == -1:\n",
        "        return len(remaining_tgt)\n",
        "    end = idx + len(last)\n",
        "    next_space = remaining_tgt.find(' ', end)\n",
        "    return next_space + 1 if next_space != -1 else len(remaining_tgt)\n",
        "\n",
        "def find_target_span_end_semantic(\n",
        "    src_unit: str,\n",
        "    remaining_tgt: str,\n",
        "    embed_func=compute_embeddings_with_cache,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50,\n",
        "    similarity_threshold: float = 0.4\n",
        ") -> int:\n",
        "    \"\"\"최적화된 타겟 스팬 탐색 함수\"\"\"\n",
        "    # 예외 처리 강화\n",
        "    if not src_unit or not remaining_tgt:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        # 1) 원문 임베딩 (단일 계산)\n",
        "        src_emb = embed_func([src_unit])[0]\n",
        "        \n",
        "        # 2) 번역문 토큰 분리 및 누적 길이 계산\n",
        "        tgt_tokens = remaining_tgt.split()\n",
        "        if not tgt_tokens:\n",
        "            return 0\n",
        "            \n",
        "        upper = min(len(tgt_tokens), max_tokens)\n",
        "        cumulative_lengths = [0]\n",
        "        current_length = 0\n",
        "        \n",
        "        for tok in tgt_tokens:\n",
        "            current_length += len(tok) + 1  # 토큰 + 공백\n",
        "            cumulative_lengths.append(current_length)\n",
        "            \n",
        "        # 3) 후보 세그먼트 생성 (메모리 효율적 방식)\n",
        "        candidates = []\n",
        "        candidate_indices = []\n",
        "        \n",
        "        # 적은 수의 후보만 생성하여 효율성 향상\n",
        "        step_size = 1 if upper <= 10 else 2  # 토큰 10개 이하면 모든 후보 검사, 이상이면 2칸씩\n",
        "        \n",
        "        for end_i in range(min_tokens-1, upper, step_size):\n",
        "            cand = \" \".join(tgt_tokens[:end_i+1])\n",
        "            candidates.append(cand)\n",
        "            candidate_indices.append(end_i)\n",
        "            \n",
        "        # 4) 배치 임베딩 (한 번에 계산)\n",
        "        cand_embs = embed_func(candidates)\n",
        "        \n",
        "        # 5) 최적 매칭 탐색 (코사인 유사도 + 길이 패널티)\n",
        "        best_score = -1.0\n",
        "        best_end_idx = cumulative_lengths[-1]  # 기본값은 전체 길이\n",
        "        \n",
        "        for i, emb in enumerate(cand_embs):\n",
        "            # 코사인 유사도 계산\n",
        "            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)\n",
        "            \n",
        "            # 길이 패널티 (너무 짧은 매칭 방지)\n",
        "            end_i = candidate_indices[i]\n",
        "            length_ratio = (end_i + 1) / len(tgt_tokens)\n",
        "            length_penalty = min(1.0, length_ratio * 2)  # 최대 1.0\n",
        "            \n",
        "            adjusted_score = score * length_penalty\n",
        "            \n",
        "            # 임계값 확인 및 최대값 갱신\n",
        "            if adjusted_score > best_score and score >= similarity_threshold:\n",
        "                best_score = adjusted_score\n",
        "                best_end_idx = cumulative_lengths[end_i + 1]\n",
        "                \n",
        "        return best_end_idx\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"의미 매칭 오류, 단순 매칭으로 대체: {e}\")\n",
        "        return find_target_span_end_simple(src_unit, remaining_tgt)\n",
        "\n",
        "\n",
        "def split_tgt_by_src_units(src_units: list[str], tgt_text: str) -> list[str]:\n",
        "    results = []\n",
        "    cursor = 0\n",
        "    total = len(tgt_text)\n",
        "    for src_u in src_units:\n",
        "        remaining = tgt_text[cursor:]\n",
        "        end_len = find_target_span_end_simple(src_u, remaining)\n",
        "        chunk = tgt_text[cursor:cursor+end_len]\n",
        "        results.extend(split_inside_chunk(chunk))\n",
        "        cursor += end_len\n",
        "    if cursor < total:\n",
        "        results.extend(split_inside_chunk(tgt_text[cursor:]))\n",
        "    return results\n",
        "\n",
        "def split_tgt_by_src_units_semantic(src_units, tgt_text, embed_func=compute_embeddings_with_cache, min_tokens=1):\n",
        "    tgt_tokens = tgt_text.split()\n",
        "    N, T = len(src_units), len(tgt_tokens)\n",
        "    if N == 0 or T == 0:\n",
        "        return []\n",
        "\n",
        "    dp = np.full((N+1, T+1), -np.inf)\n",
        "    back = np.zeros((N+1, T+1), dtype=int)\n",
        "    dp[0, 0] = 0.0\n",
        "\n",
        "    # 원문 임베딩 계산\n",
        "    src_embs = embed_func(src_units)\n",
        "\n",
        "    # DP 테이블 채우기 (j 루프 범위 주의!)\n",
        "    for i in range(1, N+1):\n",
        "        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):\n",
        "            for k in range((i-1)*min_tokens, j-min_tokens+1):\n",
        "                span = \" \".join(tgt_tokens[k:j])\n",
        "                tgt_emb = embed_func([span])[0]\n",
        "                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))\n",
        "                score = dp[i-1, k] + sim\n",
        "                if score > dp[i, j]:\n",
        "                    dp[i, j] = score\n",
        "                    back[i, j] = k\n",
        "\n",
        "    # Traceback\n",
        "    cuts = [T]\n",
        "    curr = T\n",
        "    for i in range(N, 0, -1):\n",
        "        prev = int(back[i, curr])\n",
        "        cuts.append(prev)\n",
        "        curr = prev\n",
        "    cuts = cuts[::-1]\n",
        "    assert cuts[0] == 0 and cuts[-1] == T and len(cuts) == N + 1\n",
        "\n",
        "    # Build actual spans\n",
        "    tgt_spans = []\n",
        "    for i in range(N):\n",
        "        span = \" \".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()\n",
        "        tgt_spans.append(span)\n",
        "    return tgt_spans\n",
        "\n",
        "def split_tgt_meaning_units(\n",
        "    src_text: str,\n",
        "    tgt_text: str,\n",
        "    use_semantic: bool = True,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50\n",
        ") -> list[str]:\n",
        "    src_units = split_src_meaning_units(src_text)\n",
        "\n",
        "    if use_semantic:\n",
        "        return split_tgt_by_src_units_semantic(\n",
        "            src_units,\n",
        "            tgt_text,\n",
        "            embed_func=compute_embeddings_with_cache,\n",
        "            min_tokens=min_tokens\n",
        "        )\n",
        "    else:\n",
        "        return split_tgt_by_src_units(src_units, tgt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 정렬 모듈 (aligner.py)\n",
        "\n",
        "원문과 번역문 구 간의 정렬을 위한 알고리즘입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aligner module\n",
        "def cosine_similarity(vec1: Any, vec2: Any) -> float:\n",
        "    \"\"\"Calculate cosine similarity (handling zero vectors).\"\"\"\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
        "\n",
        "def align_src_tgt(src_units, tgt_units, embed_func=compute_embeddings_with_cache):\n",
        "    \"\"\"Align source and target units.\"\"\"\n",
        "    logger.info(f\"Source units: {len(src_units)} items, Target units: {len(tgt_units)} items\")\n",
        "\n",
        "    if len(src_units) != len(tgt_units):\n",
        "        try:\n",
        "            flatten_tgt = \" \".join(tgt_units)\n",
        "            new_tgt_units = split_tgt_by_src_units_semantic(src_units, flatten_tgt, embed_func, min_tokens=1)\n",
        "            if len(new_tgt_units) == len(src_units):\n",
        "                logger.info(\"Semantic re-alignment successful\")\n",
        "                return list(zip(src_units, new_tgt_units))\n",
        "            else:\n",
        "                logger.warning(f\"Length mismatch after re-alignment: Source={len(src_units)}, Target={len(new_tgt_units)}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during semantic re-alignment: {e}\")\n",
        "\n",
        "        if len(src_units) > len(tgt_units):\n",
        "            tgt_units.extend([\"\"] * (len(src_units) - len(tgt_units)))\n",
        "        else:\n",
        "            src_units.extend([\"\"] * (len(tgt_units) - len(src_units)))\n",
        "\n",
        "    return list(zip(src_units, tgt_units))\n",
        "\n",
        "def calculate_alignment_matrix(src_embs, tgt_embs, batch_size=512):\n",
        "    \"\"\"Optimized function for calculating large similarity matrices.\"\"\"\n",
        "    src_len, tgt_len = len(src_embs), len(tgt_embs)\n",
        "    similarity_matrix = np.zeros((src_len, tgt_len))\n",
        "\n",
        "    for i in range(0, src_len, batch_size):\n",
        "        batch_src = src_embs[i:i + batch_size]\n",
        "        for j in range(0, tgt_len, batch_size):\n",
        "            batch_tgt = tgt_embs[j:j + batch_size]\n",
        "            batch_src_norm = np.linalg.norm(batch_src, axis=1, keepdims=True)\n",
        "            batch_tgt_norm = np.linalg.norm(batch_tgt, axis=1, keepdims=True)\n",
        "\n",
        "            dots = np.matmul(batch_src, batch_tgt.T)\n",
        "            norms = np.matmul(batch_src_norm, batch_tgt_norm.T)\n",
        "            batch_sim = dots / (norms + 1e-8)\n",
        "\n",
        "            similarity_matrix[i:i + batch_size, j:j + batch_size] = batch_sim\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. I/O 모듈 (io_manager.py)\n",
        "\n",
        "Excel 파일을 읽고 처리한 후 결과를 저장하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I/O module\n",
        "def process_file(input_path: str, output_path: str, batch_size: int = 128, verbose: bool = False) -> None:\n",
        "    \"\"\"청크 단위로 최적화된 파일 처리 함수\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "        return\n",
        "\n",
        "    if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "        logger.error(\"[IO] Missing '원문' or '번역문' columns.\")\n",
        "        return\n",
        "\n",
        "    outputs: List[Dict[str, Any]] = []\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    # 처리할 행을 작은 청크로 분할\n",
        "    chunk_size = min(50, total_rows)  # 최대 50행씩 처리\n",
        "    \n",
        "    # 임베딩 캐시 초기화 - 메모리 관리\n",
        "    global _embedding_cache\n",
        "    \n",
        "    for chunk_start in tqdm(range(0, total_rows, chunk_size), desc=\"Processing chunks\"):\n",
        "        chunk_end = min(chunk_start + chunk_size, total_rows)\n",
        "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
        "        \n",
        "        # 청크별 임베딩 캐시 관리 (메모리 효율성)\n",
        "        if len(_embedding_cache) > 10000:  # 캐시가 너무 크면\n",
        "            _embedding_cache = {}  # 초기화\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # 청크 내 행 처리\n",
        "        for idx, row in enumerate(chunk_df.itertuples(index=False), start=chunk_start+1):\n",
        "            src_text = str(getattr(row, '원문', '') or '')\n",
        "            tgt_text = str(getattr(row, '번역문', '') or '')\n",
        "            if verbose:\n",
        "                print(f\"\\n[========= ROW {idx} =========]\")\n",
        "                print(\"Source (input):\", src_text)\n",
        "                print(\"Target (input):\", tgt_text)\n",
        "\n",
        "            try:\n",
        "                masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "                masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "                src_units = split_src_meaning_units(masked_src)\n",
        "                tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "\n",
        "                restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "                restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "                aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Alignment result: (source to target comparison)\")\n",
        "                    for src_gu, tgt_gu in zip(aligned_src_units, aligned_tgt_units):\n",
        "                        print(f\"SRC: {src_gu} | TGT: {tgt_gu}\")\n",
        "\n",
        "                for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                    outputs.append({\n",
        "                        \"문장식별자\": idx,\n",
        "                        \"구식별자\": gu_idx,\n",
        "                        \"원문구\": src_gu,\n",
        "                        \"번역구\": tgt_gu,\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"[IO] Failed to process row {idx}: {e}\")\n",
        "                outputs.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "\n",
        "    try:\n",
        "        output_df = pd.DataFrame(outputs, columns=[\"문장식별자\", \"구식별자\", \"원문구\", \"번역구\"])\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        if verbose:\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "        \n",
        "\n",
        "def process_file_parallel(input_path: str, output_path: str, num_workers: int = None, chunk_size: int = 20, gpu_strategy: str = \"single\") -> None:\n",
        "    \"\"\"\n",
        "    병렬 처리를 사용한 파일 처리 함수 - GPU 최적화 버전\n",
        "    \n",
        "    Args:\n",
        "        input_path: 입력 Excel 파일 경로\n",
        "        output_path: 출력 Excel 파일 경로\n",
        "        num_workers: 병렬 처리에 사용할 워커 수 (None이면 CPU 코어 수에 따라 자동 설정)\n",
        "        chunk_size: 각 워커에게 할당할 행 수\n",
        "        gpu_strategy: GPU 활용 전략 \n",
        "            - \"single\": 첫 번째 워커만 GPU 사용 (안전)\n",
        "            - \"shared\": 모든 워커가 GPU 공유 (고성능 GPU 필요)\n",
        "            - \"multi\": 여러 GPU에 분산 (다중 GPU 환경)\n",
        "            - \"none\": GPU 사용 안 함 (CPU만 사용)\n",
        "    \"\"\"\n",
        "    import concurrent.futures\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    from tqdm import tqdm\n",
        "    import logging\n",
        "    \n",
        "    # 로거 사용 (기존 코드와 동일한 로거 사용)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # 파일 읽기 (기존 코드와 동일한 방식)\n",
        "        try:\n",
        "            df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "            return\n",
        "\n",
        "        if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "            logger.error(\"[IO] Missing '원문' or '번역문' columns.\")\n",
        "            return\n",
        "        \n",
        "        # 워커 수 결정\n",
        "        if num_workers is None:\n",
        "            cpu_cores = os.cpu_count() or 4\n",
        "            if gpu_strategy == \"multi\" and torch.cuda.is_available():\n",
        "                # 멀티 GPU 환경에서는 GPU 수에 맞게 조정\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                num_workers = max(min(cpu_cores, gpu_count * 2 if gpu_count > 0 else 4), 1)\n",
        "            else:\n",
        "                # 일반 환경에서는 CPU 코어 수 기준으로 설정\n",
        "                num_workers = min(cpu_cores, 4)\n",
        "        \n",
        "        # GPU 설정 준비\n",
        "        gpu_count = 0\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            logger.info(f\"Available GPUs: {gpu_count}\")\n",
        "            # GPU 메모리 정리\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logger.info(\"No GPU available, using CPU only\")\n",
        "            if gpu_strategy != \"none\":\n",
        "                logger.warning(\"GPU strategy requested but no GPU available, falling back to CPU\")\n",
        "                gpu_strategy = \"none\"\n",
        "        \n",
        "        # 데이터 분할\n",
        "        total_rows = len(df)\n",
        "        row_indices = list(range(1, total_rows + 1))  # 1부터 시작하는 인덱스\n",
        "        \n",
        "        # 데이터 준비 - 작업량이 균등하도록 분배\n",
        "        data_chunks = []\n",
        "        for i in range(0, total_rows, chunk_size):\n",
        "            chunk_indices = row_indices[i:i + chunk_size]\n",
        "            chunk_data = []\n",
        "            for idx in chunk_indices:\n",
        "                row_idx = idx - 1  # 0-based 인덱스로 변환\n",
        "                if row_idx < len(df):  # 인덱스 범위 체크\n",
        "                    row = df.iloc[row_idx]\n",
        "                    # None 체크 및 문자열 변환\n",
        "                    src_text = str(row['원문']) if pd.notnull(row['원문']) else \"\"\n",
        "                    tgt_text = str(row['번역문']) if pd.notnull(row['번역문']) else \"\"\n",
        "                    \n",
        "                    # 텍스트 길이 기반 가중치 계산\n",
        "                    text_length = len(src_text) + len(tgt_text)\n",
        "                    chunk_data.append((idx, src_text, tgt_text, text_length))\n",
        "            \n",
        "            if chunk_data:  # 빈 청크 방지\n",
        "                data_chunks.append(chunk_data)\n",
        "        \n",
        "        # 청크별 작업량 계산 및 텍스트 길이 정보 제거\n",
        "        processed_chunks = []\n",
        "        for chunk in data_chunks:\n",
        "            # 텍스트 길이 정보는 제거하고 원래 데이터만 유지\n",
        "            processed_chunks.append([(item[0], item[1], item[2]) for item in chunk])\n",
        "        \n",
        "        # 작업량 기준으로 청크 정렬 (내림차순)\n",
        "        chunks_with_size = [(i, sum(len(item[1])+len(item[2]) for item in chunk)) for i, chunk in enumerate(data_chunks)]\n",
        "        chunks_with_size.sort(key=lambda x: x[1], reverse=True)\n",
        "        sorted_chunk_indices = [i for i, _ in chunks_with_size]\n",
        "        \n",
        "        # GPU 할당 전략 설정\n",
        "        if gpu_strategy == \"single\" and gpu_count > 0:\n",
        "            # 첫 번째 워커만 GPU 사용\n",
        "            gpu_assignments = [0 if i == 0 else -1 for i in range(num_workers)]\n",
        "        elif gpu_strategy == \"shared\" and gpu_count > 0:\n",
        "            # 모든 워커가 첫 번째 GPU 공유\n",
        "            gpu_assignments = [0 for _ in range(num_workers)]\n",
        "        elif gpu_strategy == \"multi\" and gpu_count > 1:\n",
        "            # 다중 GPU에 워커 분산\n",
        "            gpu_assignments = [i % gpu_count for i in range(num_workers)]\n",
        "        else:\n",
        "            # GPU 사용 안 함\n",
        "            gpu_assignments = [-1 for _ in range(num_workers)]\n",
        "        \n",
        "        logger.info(f\"Starting parallel processing with {num_workers} workers, {len(processed_chunks)} chunks\")\n",
        "        logger.info(f\"GPU strategy: {gpu_strategy}, GPU assignments: {gpu_assignments}\")\n",
        "        \n",
        "        # 청크와 워커 매핑 최적화 (긴 텍스트 = 강력한 GPU에 할당)\n",
        "        chunk_worker_assignments = {}\n",
        "        for i, chunk_idx in enumerate(sorted_chunk_indices):\n",
        "            worker_idx = i % num_workers\n",
        "            chunk_worker_assignments[chunk_idx] = worker_idx\n",
        "        \n",
        "        # 병렬 실행\n",
        "        all_results = []\n",
        "        with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "            future_to_chunk = {}\n",
        "            \n",
        "            for chunk_idx, chunk_data in enumerate(processed_chunks):\n",
        "                worker_idx = chunk_worker_assignments.get(chunk_idx, chunk_idx % num_workers)\n",
        "                gpu_idx = gpu_assignments[worker_idx]\n",
        "                \n",
        "                # 워커별 메모리 사용량 조정\n",
        "                memory_fraction = 0.8 if gpu_strategy == \"shared\" else 0.95\n",
        "                \n",
        "                future = executor.submit(\n",
        "                    process_chunk_safe,  # 안전한 청크 처리 함수 사용\n",
        "                    chunk_data,\n",
        "                    gpu_idx=gpu_idx,\n",
        "                    memory_fraction=memory_fraction\n",
        "                )\n",
        "                future_to_chunk[future] = chunk_idx\n",
        "            \n",
        "            # 결과 수집\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_chunk), \n",
        "                              total=len(future_to_chunk), \n",
        "                              desc=\"Processing chunks\"):\n",
        "                chunk_idx = future_to_chunk[future]\n",
        "                try:\n",
        "                    chunk_results = future.result()\n",
        "                    if isinstance(chunk_results, list):  # 타입 체크 추가\n",
        "                        all_results.extend(chunk_results)\n",
        "                    else:\n",
        "                        logger.error(f\"Invalid result type from chunk {chunk_idx}: {type(chunk_results)}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing chunk {chunk_idx}: {e}\")\n",
        "        \n",
        "        # 결과가 비어있는지 확인\n",
        "        if not all_results:\n",
        "            logger.error(\"No results were produced by parallel processing\")\n",
        "            return\n",
        "            \n",
        "        # 결과 정렬 (문장식별자, 구식별자 순)\n",
        "        all_results.sort(key=lambda x: (x['문장식별자'], x['구식별자']))\n",
        "        \n",
        "        # 결과 저장\n",
        "        try:\n",
        "            output_df = pd.DataFrame(all_results, columns=[\"문장식별자\", \"구식별자\", \"원문구\", \"번역구\"])\n",
        "            output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Parallel processing failed: {e}\")\n",
        "        raise\n",
        "    \n",
        "\n",
        "def process_chunk_safe(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    예외 처리 기능이 강화된 안전한 청크 처리 래퍼 함수\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (인덱스, 원문, 번역문) 튜플의 리스트\n",
        "        gpu_idx: 사용할 GPU 인덱스 (-1: CPU만 사용)\n",
        "        memory_fraction: GPU 메모리 사용 비율 (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        처리된 결과 딕셔너리 리스트 또는 오류 시 빈 리스트\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # 실제 처리 함수 호출\n",
        "        return process_chunk(chunk_data, gpu_idx, memory_fraction)\n",
        "    except Exception as e:\n",
        "        # 치명적 오류 발생 시 기본 처리\n",
        "        logger.error(f\"Critical error in process_chunk: {e}\")\n",
        "        \n",
        "        # 오류 발생 시 원본 텍스트만 결과로 반환\n",
        "        fallback_results = []\n",
        "        try:\n",
        "            for idx, src_text, tgt_text in chunk_data:\n",
        "                fallback_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "        except:\n",
        "            logger.error(\"Failed to create fallback results\")\n",
        "        \n",
        "        return fallback_results\n",
        "\n",
        "\n",
        "def process_chunk(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    하나의 데이터 청크를 처리하는 워커 함수 - GPU 최적화 버전\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (인덱스, 원문, 번역문) 튜플의 리스트\n",
        "        gpu_idx: 사용할 GPU 인덱스 (-1: CPU만 사용)\n",
        "        memory_fraction: GPU 메모리 사용 비율 (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        처리된 결과 딕셔너리 리스트\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import logging\n",
        "    import gc\n",
        "    \n",
        "    # 로거 초기화 (프로세스별)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    # 사용 가능한지 확인하고 필요한 모듈 임포트\n",
        "    required_modules = {}\n",
        "    \n",
        "    try:\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        required_modules['colbert'] = True\n",
        "    except ImportError:\n",
        "        logger.error(\"colbert 모듈을 불러올 수 없습니다.\")\n",
        "        required_modules['colbert'] = False\n",
        "    \n",
        "    # 마스킹 및 분할 함수 사용 가능성 확인\n",
        "    try:\n",
        "        from your_module import mask_brackets, split_src_meaning_units, split_tgt_meaning_units, restore_masks, align_src_tgt, compute_embeddings_with_cache\n",
        "        required_modules['custom_functions'] = True\n",
        "    except ImportError:\n",
        "        # 모듈 이름이 다를 수 있으므로 전역 네임스페이스에서 찾아봄\n",
        "        functions_available = True\n",
        "        for func_name in ['mask_brackets', 'split_src_meaning_units', 'split_tgt_meaning_units', 'restore_masks', 'align_src_tgt', 'compute_embeddings_with_cache']:\n",
        "            if func_name not in globals():\n",
        "                logger.error(f\"필수 함수 {func_name}를 찾을 수 없습니다.\")\n",
        "                functions_available = False\n",
        "        required_modules['custom_functions'] = functions_available\n",
        "    \n",
        "    # 필수 모듈/함수가 없으면 기본 처리만 수행\n",
        "    if not all(required_modules.values()):\n",
        "        logger.error(\"필수 모듈 또는 함수가 누락되었습니다. 원본 텍스트를 그대로 반환합니다.\")\n",
        "        return [{\"문장식별자\": idx, \"구식별자\": 1, \"원문구\": src, \"번역구\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # 프로세스별 GPU 설정\n",
        "    if gpu_idx < 0:\n",
        "        # CPU 모드\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "        logger.info(f\"Worker using CPU mode\")\n",
        "    else:\n",
        "        # GPU 모드 - 특정 GPU 지정\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n",
        "        logger.info(f\"Worker using GPU {gpu_idx} with memory fraction {memory_fraction}\")\n",
        "        \n",
        "        # GPU 메모리 설정\n",
        "        if torch.cuda.is_available():\n",
        "            # 메모리 효율성 개선\n",
        "            torch.cuda.empty_cache()\n",
        "            try:\n",
        "                torch.cuda.set_per_process_memory_fraction(memory_fraction, 0)\n",
        "            except (AttributeError, RuntimeError) as e:\n",
        "                logger.warning(f\"GPU 메모리 설정 실패: {e}\")\n",
        "                # 대체 메모리 관리\n",
        "                gc.collect()\n",
        "    \n",
        "    # 모델 로드 최적화 (프로세스별로 새로 로드)\n",
        "    global model, _embedding_cache\n",
        "    \n",
        "    # 임베딩 캐시 초기화\n",
        "    _embedding_cache = {}\n",
        "    \n",
        "    try:\n",
        "        # 모델 로드 경로가 올바른지 확인\n",
        "        model_path = './model/original_colbert_bm25_dense_hybrid_for_pairmining_checkpoint_512x8.torch'\n",
        "        if not os.path.exists(model_path):\n",
        "            logger.error(f\"모델 파일을 찾을 수 없습니다: {model_path}\")\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "            \n",
        "        # 모델 로드\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        model = Checkpoint.load(model_path)\n",
        "        model.query_tokenizer.query_maxlen = 512\n",
        "        model.doc_tokenizer.doc_maxlen = 512\n",
        "        \n",
        "        # GPU 메모리 최적화\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            try:\n",
        "                # 혼합 정밀도 사용 (Float16)\n",
        "                model.half()  # Float16 변환으로 메모리 사용량 절반으로 감소\n",
        "                model.cuda()\n",
        "                # 그래디언트 계산 비활성화 (추론 모드)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for param in model.parameters():\n",
        "                        param.requires_grad = False\n",
        "            except RuntimeError as e:\n",
        "                logger.error(f\"GPU 모드 설정 오류: {e}\")\n",
        "                # GPU 오류 시 CPU로 폴백\n",
        "                model.cpu()\n",
        "                model.eval()\n",
        "        else:\n",
        "            # CPU 모드\n",
        "            model.cpu()\n",
        "            model.eval()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"모델 로드 오류: {e}\")\n",
        "        # 모델 로드 실패 시 원본 텍스트 반환\n",
        "        return [{\"문장식별자\": idx, \"구식별자\": 1, \"원문구\": src, \"번역구\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # 결과 저장 리스트\n",
        "    chunk_results = []\n",
        "    \n",
        "    # 청크 내 각 행 처리\n",
        "    for idx, src_text, tgt_text in chunk_data:\n",
        "        try:\n",
        "            # 빈 텍스트 체크\n",
        "            if not src_text or not tgt_text:\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # 괄호 마스킹\n",
        "            masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "            masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "            \n",
        "            # 의미 단위 분할\n",
        "            src_units = split_src_meaning_units(masked_src)\n",
        "            tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "            \n",
        "            # 분할 결과 체크\n",
        "            if not src_units or not tgt_units:\n",
        "                logger.warning(f\"행 {idx}: 의미 단위 분할 결과가 비어 있습니다.\")\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # 마스크 복원\n",
        "            restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "            restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "            \n",
        "            # 정렬\n",
        "            try:\n",
        "                # compute_embeddings_with_cache 함수는 _embedding_cache를 참조\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"행 {idx} 정렬 오류: {e}\")\n",
        "                # 정렬 실패 시 원본 텍스트 사용\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # 결과가 없으면 원본 텍스트로 대체\n",
        "            if not aligned_pairs:\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # 정렬 결과 언패킹\n",
        "            aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "            \n",
        "            # 결과 저장\n",
        "            for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                # 빈 결과 방지\n",
        "                if not src_gu.strip() or not tgt_gu.strip():\n",
        "                    continue\n",
        "                    \n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": gu_idx,\n",
        "                    \"원문구\": src_gu,\n",
        "                    \"번역구\": tgt_gu,\n",
        "                })\n",
        "            \n",
        "            # 결과가 생성되지 않았으면 원본 사용\n",
        "            if not any(r[\"문장식별자\"] == idx for r in chunk_results):\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"행 {idx} 처리 오류: {e}\")\n",
        "            # 오류 발생 시 원본 텍스트 저장\n",
        "            chunk_results.append({\n",
        "                \"문장식별자\": idx,\n",
        "                \"구식별자\": 1,\n",
        "                \"원문구\": src_text,\n",
        "                \"번역구\": tgt_text,\n",
        "            })\n",
        "    \n",
        "    # 프로세스 종료 시 메모리 정리\n",
        "    try:\n",
        "        # 메모리 정리\n",
        "        _embedding_cache.clear()\n",
        "        del model\n",
        "        gc.collect()\n",
        "        \n",
        "        # GPU 메모리 정리\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return chunk_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 메인 실행 함수 (main.py)\n",
        "\n",
        "파이프라인 전체를 실행하는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def main(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    use_parallel: bool = False,\n",
        "    num_workers: int = 4,\n",
        "    chunk_size: int = 50,\n",
        "    gpu_memory_fraction: float = 0.8,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"메인 처리 함수\"\"\"\n",
        "    \n",
        "    # 함수 내부에서 logger 정의\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "        logger.info(\"Verbose mode activated: DEBUG level logging enabled.\")\n",
        "    \n",
        "    # Log parallel processing options if enabled\n",
        "    if use_parallel:\n",
        "        logger.info(f\"병렬 처리 모드: 워커 {num_workers}개, 청크 크기 {chunk_size}\")\n",
        "    else:\n",
        "        logger.info(\"단일 프로세스 모드\")\n",
        "    \n",
        "    # 파일 경로 검증\n",
        "    if not os.path.exists(input_path):\n",
        "        logger.error(f\"입력 파일이 존재하지 않습니다: {input_path}\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(f\"입력 파일: {input_path}\")\n",
        "    logger.info(f\"출력 파일: {output_path}\")\n",
        "    \n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        logger.info(f\"로드된 데이터: {len(df)}행\")\n",
        "        \n",
        "        if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "            logger.error(\"필수 컬럼('원문', '번역문')이 없습니다.\")\n",
        "            return False\n",
        "        \n",
        "        # 처리 결과 저장할 리스트\n",
        "        all_results = []\n",
        "        \n",
        "        # 각 행 처리\n",
        "        for idx, row in df.iterrows():\n",
        "            src_text = str(row.get('원문', ''))\n",
        "            tgt_text = str(row.get('번역문', ''))\n",
        "            \n",
        "            if verbose:\n",
        "                logger.info(f\"처리 중: 행 {idx+1}\")\n",
        "                logger.info(f\"원문: {src_text}\")\n",
        "                logger.info(f\"번역문: {tgt_text}\")\n",
        "            \n",
        "            try:\n",
        "                # 간단한 처리 (실제 파이프라인 대신)\n",
        "                result = {\n",
        "                    \"문장식별자\": idx + 1,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"행 {idx+1} 처리 실패: {e}\")\n",
        "                result = {\n",
        "                    \"문장식별자\": idx + 1,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "        \n",
        "        # 결과 저장\n",
        "        output_df = pd.DataFrame(all_results)\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        logger.info(f\"결과 저장 완료: {output_path}\")\n",
        "        logger.info(f\"총 처리 결과: {len(all_results)}개\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"처리 중 오류 발생: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "print(\"✅ main 함수가 수정되었습니다.\")\n",
        "\n",
        "# 테스트 실행\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# 테스트 데이터 생성\n",
        "test_data = [\n",
        "    {\"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\", \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"},\n",
        "    {\"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\", \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"테스트 데이터 생성: {test_input}\")\n",
        "\n",
        "# 파이프라인 실행\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# 결과 확인\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"✅ 처리 결과:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"결과 파일 읽기 실패: {e}\")\n",
        "else:\n",
        "    logger.error(\"❌ 처리 실패\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 테스트\n",
        "\n",
        "아래 셀을 실행하여 간단한 예제로 파이프라인을 테스트해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing function\n",
        "def create_test_data(file_path=\"test_input.xlsx\"):\n",
        "    \"\"\"Generate test data.\"\"\"\n",
        "    test_data = [\n",
        "        {\n",
        "            \"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\",\n",
        "            \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\",\n",
        "            \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"夫雅頌之作也 詩人各有所屬者也\",\n",
        "            \"번역문\": \"무릇 아송의 창작은 시인마다 각자 속한 바가 있었다.\"\n",
        "        },\n",
        "        {    \n",
        "            \"원문\": \"然而孔子取而次之者하야 則有家國之次矣\",\n",
        "            \"번역문\": \"그런데 공자가 이것을 취하여 차례를 매긴 것은 가국의 순서에 따른 것이다.\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(test_data)\n",
        "    df.to_excel(file_path, index=False, engine='openpyxl')\n",
        "    return file_path\n",
        "\n",
        "# Testing execution\n",
        "test_input = create_test_data()\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# Run the pipeline\n",
        "main(test_input, test_output, verbose=True)\n",
        "\n",
        "# Check results\n",
        "try:\n",
        "    result_df = pd.read_excel(test_output)\n",
        "    display(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read result file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 단일 문장 테스트\n",
        "\n",
        "특정 문장 쌍에 대해 빠르게 테스트해볼 수 있는 기능입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# ============================================================================\n",
        "# 1. 초기 설정\n",
        "# ============================================================================\n",
        "\n",
        "# 로거 설정\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 환경 변수 설정\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# ============================================================================\n",
        "# 2. 모델 로딩\n",
        "# ============================================================================\n",
        "\n",
        "def load_embedding_model():\n",
        "    \"\"\"여러 옵션을 시도하여 임베딩 모델 로드\"\"\"\n",
        "    \n",
        "    # Option 1: BGE 모델\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"✅ BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"✅ SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: 더미 모델\n",
        "    logger.info(\"🔄 Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# 모델 로드\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. 유틸리티 함수들\n",
        "# ============================================================================\n",
        "\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"모델 타입에 따른 임베딩 계산\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. 메인 처리 함수\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# ============================================================================\n",
        "# 1. 초기 설정\n",
        "# ============================================================================\n",
        "\n",
        "# 로거 설정\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 환경 변수 설정\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# ============================================================================\n",
        "# 2. 모델 로딩\n",
        "# ============================================================================\n",
        "\n",
        "def load_embedding_model():\n",
        "    \"\"\"여러 옵션을 시도하여 임베딩 모델 로드\"\"\"\n",
        "    \n",
        "    # Option 1: BGE 모델\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"✅ BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"✅ SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: 더미 모델\n",
        "    logger.info(\"🔄 Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# 모델 로드\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. 유틸리티 함수들\n",
        "# ============================================================================\n",
        "\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"모델 타입에 따른 임베딩 계산\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. 메인 처리 함수\n",
        "# ============================================================================\n",
        "\n",
        "def main(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    use_parallel: bool = False,\n",
        "    num_workers: int = 4,\n",
        "    chunk_size: int = 50,\n",
        "    gpu_memory_fraction: float = 0.8,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"메인 처리 함수\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "        logger.info(\"Verbose mode activated: DEBUG level logging enabled.\")\n",
        "    \n",
        "    # 파일 경로 검증\n",
        "    if not os.path.exists(input_path):\n",
        "        logger.error(f\"입력 파일이 존재하지 않습니다: {input_path}\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(f\"입력 파일: {input_path}\")\n",
        "    logger.info(f\"출력 파일: {output_path}\")\n",
        "    \n",
        "    if use_parallel:\n",
        "        logger.info(f\"병렬 처리 모드: 워커 {num_workers}개, 청크 크기 {chunk_size}\")\n",
        "    else:\n",
        "        logger.info(\"단일 프로세스 모드\")\n",
        "    \n",
        "    try:\n",
        "        # 데이터 로드\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        logger.info(f\"로드된 데이터: {len(df)}행\")\n",
        "        \n",
        "        if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "            logger.error(\"필수 컬럼('원문', '번역문')이 없습니다.\")\n",
        "            return False\n",
        "        \n",
        "        # 처리 결과 저장할 리스트\n",
        "        all_results = []\n",
        "        \n",
        "        # 각 행 처리\n",
        "        for idx, row in df.iterrows():\n",
        "            src_text = str(row.get('원문', ''))\n",
        "            tgt_text = str(row.get('번역문', ''))\n",
        "            \n",
        "            if verbose:\n",
        "                logger.info(f\"처리 중: 행 {idx+1}\")\n",
        "                logger.info(f\"원문: {src_text}\")\n",
        "                logger.info(f\"번역문: {tgt_text}\")\n",
        "            \n",
        "            try:\n",
        "                # 간단한 처리 (실제 파이프라인 대신)\n",
        "                result = {\n",
        "                    \"문장식별자\": idx + 1,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"행 {idx+1} 처리 실패: {e}\")\n",
        "                result = {\n",
        "                    \"문장식별자\": idx + 1,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "        \n",
        "        # 결과 저장\n",
        "        output_df = pd.DataFrame(all_results)\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        logger.info(f\"결과 저장 완료: {output_path}\")\n",
        "        logger.info(f\"총 처리 결과: {len(all_results)}개\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"처리 중 오류 발생: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# 5. 테스트 실행\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(f\"✅ Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# 테스트 데이터 생성\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "test_data = [\n",
        "    {\"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\", \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"},\n",
        "    {\"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\", \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"테스트 데이터 생성: {test_input}\")\n",
        "\n",
        "# 파이프라인 실행\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# 결과 확인\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"✅ 처리 결과:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"결과 파일 읽기 실패: {e}\")\n",
        "else:\n",
        "    logger.error(\"❌ 처리 실패\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. 테스트 실행\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(f\"✅ Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# 테스트 데이터 생성\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "test_data = [\n",
        "    {\"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\", \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"},\n",
        "    {\"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\", \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"테스트 데이터 생성: {test_input}\")\n",
        "\n",
        "# 파이프라인 실행\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# 결과 확인\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"✅ 처리 결과:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"결과 파일 읽기 실패: {e}\")\n",
        "else:\n",
        "    logger.error(\"❌ 처리 실패\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
