"""PA ì „ìš© ì •ë ¬ê¸° - SAì˜ Vice Versa ë°©ì‹ (ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥)"""

import sys
import os
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd
from tqdm import tqdm
from difflib import SequenceMatcher
import hashlib
import logging

# ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¡œì»¬ ëª¨ë“ˆ import (ê¸°ì¡´ê³¼ ë™ì¼)
from sentence_splitter import split_target_sentences_advanced, split_source_by_whitespace_and_align

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    torch = None
    TORCH_AVAILABLE = False

class IntegrityManager:
    """í…ìŠ¤íŠ¸ ë¬´ê²°ì„± ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.original_checksums = {}
        self.processing_log = []
        self.integrity_errors = []
    
    def calculate_checksum(self, text: str, label: str = "") -> str:
        """í…ìŠ¤íŠ¸ì˜ ì²´í¬ì„¬ ê³„ì‚°"""
        if not isinstance(text, str):
            text = str(text)
        # ê³µë°±ê³¼ ê°œí–‰ ì •ê·œí™” í›„ ì²´í¬ì„¬ ê³„ì‚°
        normalized = text.replace('\n', ' ').replace('\t', ' ')
        normalized = ' '.join(normalized.split())  # ì—°ì† ê³µë°± ì œê±°
        
        checksum = hashlib.md5(normalized.encode('utf-8')).hexdigest()
        self.processing_log.append(f"{label}: {checksum}")
        return checksum
    
    def store_original(self, text: str, identifier: str):
        """ì›ë³¸ í…ìŠ¤íŠ¸ ì²´í¬ì„¬ ì €ì¥"""
        checksum = self.calculate_checksum(text, f"ORIGINAL_{identifier}")
        self.original_checksums[identifier] = {
            'checksum': checksum,
            'text': text,
            'length': len(text.replace(' ', ''))
        }
    
    def verify_integrity(self, processed_text: str, identifier: str) -> Tuple[bool, str]:
        """ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì˜ ë¬´ê²°ì„± ê²€ì¦"""
        if identifier not in self.original_checksums:
            return False, f"ì›ë³¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {identifier}"
        
        original_info = self.original_checksums[identifier]
        processed_checksum = self.calculate_checksum(processed_text, f"PROCESSED_{identifier}")
        
        # ì²´í¬ì„¬ ë¹„êµ
        if original_info['checksum'] == processed_checksum:
            return True, "ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ"
        
        # ê¸¸ì´ ë¹„êµ (ëŒ€ì•ˆ ê²€ì¦)
        processed_length = len(processed_text.replace(' ', ''))
        length_diff = abs(original_info['length'] - processed_length)
        
        if length_diff == 0:
            return True, "ê¸¸ì´ ê¸°ë°˜ ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ"
        
        error_msg = f"ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨ - ê¸¸ì´ ì°¨ì´: {length_diff}ì"
        self.integrity_errors.append({
            'identifier': identifier,
            'original_checksum': original_info['checksum'],
            'processed_checksum': processed_checksum,
            'original_length': original_info['length'],
            'processed_length': processed_length,
            'length_diff': length_diff
        })
        
        return False, error_msg
    
    def restore_integrity(self, processed_units: List[str], identifier: str) -> List[str]:
        """ë¬´ê²°ì„±ì´ í›¼ì†ëœ ê²½ìš° ë³µì› ì‹œë„"""
        if identifier not in self.original_checksums:
            logger.error(f"ë³µì› ë¶ˆê°€: ì›ë³¸ ë°ì´í„° ì—†ìŒ - {identifier}")
            return processed_units
        
        original_text = self.original_checksums[identifier]['text']
        processed_combined = ''.join(processed_units).replace(' ', '')
        original_clean = original_text.replace(' ', '').replace('\n', '').replace('\t', '')
        
        if processed_combined == original_clean:
            return processed_units
        
        logger.warning(f"ë¬´ê²°ì„± ë³µì› ì‹œë„: {identifier}")
        
        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ì°¨ì´ì  ë¶„ì„ ë° ë³µì›
        sm = SequenceMatcher(None, processed_combined, original_clean)
        opcodes = sm.get_opcodes()
        
        restored_units = processed_units[:]
        total_insert_length = 0
        total_delete_length = 0
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                # ëˆ„ë½ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
                missing_text = original_clean[j1:j2]
                if restored_units:
                    restored_units[-1] += missing_text
                else:
                    restored_units.append(missing_text)
                total_insert_length += len(missing_text)
                logger.info(f"ëˆ„ë½ í…ìŠ¤íŠ¸ ë³µì›: '{missing_text}'")
                
            elif tag == 'delete':
                # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ì œê±°
                excess_text = processed_combined[i1:i2]
                for k, unit in enumerate(restored_units):
                    if excess_text in unit:
                        restored_units[k] = unit.replace(excess_text, '', 1)
                        total_delete_length += len(excess_text)
                        logger.info(f"ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°: '{excess_text}'")
                        break
        
        logger.info(f"ë¬´ê²°ì„± ë³µì› ì™„ë£Œ: ì¶”ê°€ {total_insert_length}ì, ì œê±° {total_delete_length}ì")
        return restored_units

# ì „ì—­ ë¬´ê²°ì„± ê´€ë¦¬ì
integrity_manager = IntegrityManager()

def safe_text_split(text: str, max_length: int = 150, method: str = "punctuation") -> List[str]:
    """ë¬´ê²°ì„± ë³´ì¥ í…ìŠ¤íŠ¸ ë¶„í• """
    if not text or not text.strip():
        return []
    
    # ì›ë³¸ ì €ì¥
    text_id = f"split_{id(text)}"
    integrity_manager.store_original(text, text_id)
    
    try:
        # ê¸°ì¡´ ë¶„í•  ë°©ì‹ ì ìš©
        if method == "spacy_tokenizer":
            # spaCy + í† í¬ë‚˜ì´ì € ë°©ì‹
            sentences = split_target_sentences_spacy_tokenizer(text, max_length)
        else:
            # ê¸°ì¡´ ë°©ì‹
            sentences = split_target_sentences_advanced(text, max_length, splitter=method)
        
        if not sentences:
            sentences = [text]
        
        # ë¬´ê²°ì„± ê²€ì¦
        combined_result = ''.join(sentences)
        is_valid, message = integrity_manager.verify_integrity(combined_result, text_id)
        
        if not is_valid:
            logger.warning(f"ë¶„í•  ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {message}")
            # ë³µì› ì‹œë„
            sentences = integrity_manager.restore_integrity(sentences, text_id)
            
            # ì¬ê²€ì¦
            combined_result = ''.join(sentences)
            is_valid, message = integrity_manager.verify_integrity(combined_result, text_id)
            
            if not is_valid:
                logger.error(f"ë¬´ê²°ì„± ë³µì› ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {message}")
                return [text]  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        return sentences
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
        return [text]  # ì˜¤ë¥˜ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

def safe_source_split(tgt_sentences: List[str], src_text: str, tokenizer_func=None, nlp=None) -> List[str]:
    """ë¬´ê²°ì„± ë³´ì¥ ì›ë¬¸ ë¶„í• """
    if not tgt_sentences or not src_text.strip():
        return []
    
    # ì›ë³¸ ì €ì¥
    src_id = f"src_split_{id(src_text)}"
    integrity_manager.store_original(src_text, src_id)
    
    try:
        # spaCy + í† í¬ë‚˜ì´ì € ë°©ì‹ ë˜ëŠ” ê¸°ë³¸ ë°©ì‹
        if nlp and tokenizer_func:
            src_chunks = split_src_by_tgt_units_spacy_tokenizer(tgt_sentences, src_text, tokenizer_func, nlp)
        else:
            src_chunks = split_src_by_tgt_units_vice_versa(tgt_sentences, src_text, None, tokenizer_func)
        
        if not src_chunks:
            src_chunks = [src_text]
        
        # ë¬´ê²°ì„± ê²€ì¦
        combined_result = ''.join(src_chunks)
        is_valid, message = integrity_manager.verify_integrity(combined_result, src_id)
        
        if not is_valid:
            logger.warning(f"ì›ë¬¸ ë¶„í•  ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {message}")
            # ë³µì› ì‹œë„
            src_chunks = integrity_manager.restore_integrity(src_chunks, src_id)
            
            # ì¬ê²€ì¦
            combined_result = ''.join(src_chunks)
            is_valid, message = integrity_manager.verify_integrity(combined_result, src_id)
            
            if not is_valid:
                logger.error(f"ì›ë¬¸ ë¬´ê²°ì„± ë³µì› ì‹¤íŒ¨, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {message}")
                # ê¸°ë³¸ ë¶„í• ë¡œ í´ë°±
                src_chunks = split_source_by_whitespace_and_align(src_text, len(tgt_sentences))
        
        # ê²°ê³¼ ê°œìˆ˜ ë³´ì •
        while len(src_chunks) < len(tgt_sentences):
            src_chunks.append('')
        
        return src_chunks[:len(tgt_sentences)]
        
    except Exception as e:
        logger.error(f"ì›ë¬¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ë¶„í• 
        return split_source_by_whitespace_and_align(src_text, len(tgt_sentences))

def verify_paragraph_integrity(src_paragraph: str, tgt_paragraph: str, alignments: List[Dict]) -> bool:
    """ë¬¸ë‹¨ ë‹¨ìœ„ ë¬´ê²°ì„± ê²€ì¦"""
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ ê²°í•©
    original_src = src_paragraph.replace(' ', '').replace('\n', '').replace('\t', '')
    original_tgt = tgt_paragraph.replace(' ', '').replace('\n', '').replace('\t', '')
    
    # ì •ë ¬ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    aligned_src = ''.join([align.get('ì›ë¬¸', '') for align in alignments]).replace(' ', '')
    aligned_tgt = ''.join([align.get('ë²ˆì—­ë¬¸', '') for align in alignments]).replace(' ', '')
    
    # ë¬´ê²°ì„± ê²€ì¦
    src_integrity = (original_src == aligned_src)
    tgt_integrity = (original_tgt == aligned_tgt)
    
    if not src_integrity:
        logger.error(f"ì›ë¬¸ ë¬´ê²°ì„± ì‹¤íŒ¨ - ì›ë³¸: {len(original_src)}ì, ê²°ê³¼: {len(aligned_src)}ì")
        logger.error(f"ì›ë³¸: {original_src[:100]}...")
        logger.error(f"ê²°ê³¼: {aligned_src[:100]}...")
    
    if not tgt_integrity:
        logger.error(f"ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ì‹¤íŒ¨ - ì›ë³¸: {len(original_tgt)}ì, ê²°ê³¼: {len(aligned_tgt)}ì")
        logger.error(f"ì›ë³¸: {original_tgt[:100]}...")
        logger.error(f"ê²°ê³¼: {aligned_tgt[:100]}...")
    
    return src_integrity and tgt_integrity

def restore_paragraph_integrity(src_paragraph: str, tgt_paragraph: str, alignments: List[Dict]) -> List[Dict]:
    """ë¬¸ë‹¨ ë¬´ê²°ì„± ë³µì›"""
    
    # í˜„ì¬ ì •ë ¬ ê²°ê³¼ ë¶„ì„
    aligned_src = ''.join([align.get('ì›ë¬¸', '') for align in alignments]).replace(' ', '')
    aligned_tgt = ''.join([align.get('ë²ˆì—­ë¬¸', '') for align in alignments]).replace(' ', '')
    
    original_src = src_paragraph.replace(' ', '').replace('\n', '').replace('\t', '')
    original_tgt = tgt_paragraph.replace(' ', '').replace('\n', '').replace('\t', '')
    
    restored_alignments = alignments[:]
    
    # ì›ë¬¸ ë³µì›
    if original_src != aligned_src:
        logger.info("ì›ë¬¸ ë¬´ê²°ì„± ë³µì› ì‹œì‘...")
        sm = SequenceMatcher(None, aligned_src, original_src)
        opcodes = sm.get_opcodes()
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                # ëˆ„ë½ëœ ì›ë¬¸ ì¶”ê°€
                missing_text = original_src[j1:j2]
                if restored_alignments:
                    restored_alignments[-1]['ì›ë¬¸'] += missing_text
                else:
                    restored_alignments.append({
                        'ì›ë¬¸': missing_text,
                        'ë²ˆì—­ë¬¸': '',
                        'similarity': 0.0,
                        'split_method': 'integrity_restore',
                        'align_method': 'src_missing_restore'
                    })
                logger.info(f"ëˆ„ë½ ì›ë¬¸ ë³µì›: '{missing_text}'")
                
            elif tag == 'delete':
                # ì¤‘ë³µëœ ì›ë¬¸ ì œê±°
                excess_text = aligned_src[i1:i2]
                for align in restored_alignments:
                    if excess_text in align.get('ì›ë¬¸', ''):
                        align['ì›ë¬¸'] = align['ì›ë¬¸'].replace(excess_text, '', 1)
                        logger.info(f"ì¤‘ë³µ ì›ë¬¸ ì œê±°: '{excess_text}'")
                        break
    
    # ë²ˆì—­ë¬¸ ë³µì›
    aligned_tgt_after_src_restore = ''.join([align.get('ë²ˆì—­ë¬¸', '') for align in restored_alignments]).replace(' ', '')
    
    if original_tgt != aligned_tgt_after_src_restore:
        logger.info("ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ë³µì› ì‹œì‘...")
        sm = SequenceMatcher(None, aligned_tgt_after_src_restore, original_tgt)
        opcodes = sm.get_opcodes()
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                # ëˆ„ë½ëœ ë²ˆì—­ë¬¸ ì¶”ê°€
                missing_text = original_tgt[j1:j2]
                if restored_alignments:
                    restored_alignments[-1]['ë²ˆì—­ë¬¸'] += missing_text
                else:
                    restored_alignments.append({
                        'ì›ë¬¸': '',
                        'ë²ˆì—­ë¬¸': missing_text,
                        'similarity': 0.0,
                        'split_method': 'integrity_restore',
                        'align_method': 'tgt_missing_restore'
                    })
                logger.info(f"ëˆ„ë½ ë²ˆì—­ë¬¸ ë³µì›: '{missing_text}'")
                
            elif tag == 'delete':
                # ì¤‘ë³µëœ ë²ˆì—­ë¬¸ ì œê±°
                excess_text = aligned_tgt_after_src_restore[i1:i2]
                for align in restored_alignments:
                    if excess_text in align.get('ë²ˆì—­ë¬¸', ''):
                        align['ë²ˆì—­ë¬¸'] = align['ë²ˆì—­ë¬¸'].replace(excess_text, '', 1)
                        logger.info(f"ì¤‘ë³µ ë²ˆì—­ë¬¸ ì œê±°: '{excess_text}'")
                        break
    
    return restored_alignments

# ===== ê¸°ì¡´ í•¨ìˆ˜ë“¤ì— ë¬´ê²°ì„± ë³´ì¥ ì ìš© =====

def get_spacy_nlp():
    """spaCy ëª¨ë¸ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    try:
        import spacy
        try:
            nlp = spacy.load("ko_core_news_sm")
            print("âœ… spaCy í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return nlp
        except OSError:
            try:
                nlp = spacy.load("en_core_web_sm")
                print("âš ï¸ í•œêµ­ì–´ ëª¨ë¸ ì—†ìŒ, ì˜ì–´ ëª¨ë¸ ì‚¬ìš©")
                return nlp
            except OSError:
                print("âŒ spaCy ëª¨ë¸ ì—†ìŒ")
                return None
    except ImportError:
        print("âŒ spaCy ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return None

def split_target_sentences_spacy_tokenizer(
    text: str, 
    max_length: int = 150,
    tokenizer_func=None,
    nlp=None
) -> List[str]:
    """spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ë¬¸ì¥ ë¶„í•  (ë¬´ê²°ì„± ë³´ì¥)"""
    if not text.strip():
        return []
    
    # ë¬´ê²°ì„± ê´€ë¦¬ ì ìš©
    text_id = f"spacy_tok_{id(text)}"
    integrity_manager.store_original(text, text_id)
    
    sentences = []
    
    # 1ë‹¨ê³„: spaCyë¡œ ë¬¸ì¥ ê²½ê³„ ê°ì§€
    if nlp:
        try:
            doc = nlp(text)
            spacy_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
            
            if spacy_sentences:
                print(f"ğŸ” spaCy ë¶„í• : {len(spacy_sentences)}ê°œ ë¬¸ì¥")
                sentences = spacy_sentences
            else:
                sentences = [text]
        except Exception as e:
            print(f"âš ï¸ spaCy ë¶„í•  ì‹¤íŒ¨: {e}")
            sentences = [text]
    else:
        sentences = split_target_sentences_advanced(text, max_length, splitter="punctuation")
    
    # 2ë‹¨ê³„: í† í¬ë‚˜ì´ì €ë¡œ ê¸´ ë¬¸ì¥ ì„¸ë¶„í™”
    if tokenizer_func and sentences:
        refined_sentences = []
        
        for sentence in sentences:
            if len(sentence) > max_length:
                refined_parts = split_long_sentence_with_tokenizer(sentence, max_length, tokenizer_func)
                refined_sentences.extend(refined_parts)
            else:
                refined_sentences.append(sentence)
        
        print(f"ğŸ”§ í† í¬ë‚˜ì´ì € ì¡°ì •: {len(sentences)} â†’ {len(refined_sentences)}ê°œ ë¬¸ì¥")
        sentences = refined_sentences
    
    # ë¬´ê²°ì„± ê²€ì¦ ë° ë³µì›
    combined_result = ''.join(sentences)
    is_valid, message = integrity_manager.verify_integrity(combined_result, text_id)
    
    if not is_valid:
        logger.warning(f"spaCy+í† í¬ë‚˜ì´ì € ë¶„í•  ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
        sentences = integrity_manager.restore_integrity(sentences, text_id)
    
    return sentences if sentences else [text]

def split_long_sentence_with_tokenizer(sentence: str, max_length: int, tokenizer_func) -> List[str]:
    """í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ì¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  (ë¬´ê²°ì„± ë³´ì¥)"""
    
    # ì›ë³¸ ì €ì¥
    sent_id = f"long_sent_{id(sentence)}"
    integrity_manager.store_original(sentence, sent_id)
    
    try:
        tokens = tokenizer_func(sentence)
        if not tokens:
            return [sentence]
        
        parts = []
        current_part = []
        current_length = 0
        
        for token in tokens:
            token_length = len(token)
            
            if current_length + token_length > max_length and current_part:
                parts.append(''.join(current_part))
                current_part = [token]
                current_length = token_length
            else:
                current_part.append(token)
                current_length += token_length
        
        if current_part:
            parts.append(''.join(current_part))
        
        # ë¬´ê²°ì„± ê²€ì¦
        combined_result = ''.join(parts)
        is_valid, message = integrity_manager.verify_integrity(combined_result, sent_id)
        
        if not is_valid:
            logger.warning(f"ê¸´ ë¬¸ì¥ ë¶„í•  ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
            parts = integrity_manager.restore_integrity(parts, sent_id)
        
        return parts if parts else [sentence]
        
    except Exception as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¶„í•  ì‹¤íŒ¨: {e}")
        return [sentence]

# ===== ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ë¬´ê²°ì„± ë³´ì¥ ì ìš©) =====

def get_tokenizer_function(tokenizer_name: str = "jieba"):
    """í† í¬ë‚˜ì´ì € í•¨ìˆ˜ ë°˜í™˜ - SA ì¬ì‚¬ìš©"""
    try:
        if tokenizer_name == "jieba":
            sys.path.insert(0, str(project_root / 'sa' / 'sa_tokenizers'))
            from jieba_mecab import tokenize_chinese_text
            print("âœ… jieba í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            return tokenize_chinese_text
        elif tokenizer_name == "mecab":
            sys.path.insert(0, str(project_root / 'sa' / 'sa_tokenizers'))
            from jieba_mecab import tokenize_korean_text
            print("âœ… mecab í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            return tokenize_korean_text
        else:
            print(f"âš ï¸ ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {tokenizer_name}")
            return lambda text: list(text)
    except ImportError as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")
        return lambda text: list(text)

def get_embedder_function(embedder_name: str, device: str = "cpu", openai_model: str = None, openai_api_key: str = None):
    """ì„ë² ë” í•¨ìˆ˜ ë°˜í™˜ - ê¸°ì¡´ê³¼ ë™ì¼"""
    
    if device == "cuda":
        if not TORCH_AVAILABLE or not torch.cuda.is_available():
            print("âš ï¸ CUDA ë¯¸ì§€ì›: CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            device = "cpu"
    
    if embedder_name == 'bge':
        try:
            sys.path.insert(0, str(project_root / 'common' / 'embedders'))
            from bge import get_embed_func
            embed_func = get_embed_func(device_id=0 if device == "cuda" else None)
            if embed_func is None:
                print("âŒ BGE ì„ë² ë” ì´ˆê¸°í™” ì‹¤íŒ¨")
                return None
            print("âœ… BGE ì„ë² ë” ì´ˆê¸°í™” ì„±ê³µ")
            return embed_func
        except ImportError as e:
            print(f"âŒ BGE ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
            
    elif embedder_name == 'openai':
        try:
            if openai_api_key:
                os.environ["OPENAI_API_KEY"] = openai_api_key
            
            sys.path.insert(0, str(project_root / 'common' / 'embedders'))
            from openai import compute_embeddings_with_cache
            
            def embed_func(texts):
                return compute_embeddings_with_cache(
                    texts, 
                    model=openai_model if openai_model else "text-embedding-3-large"
                )
            print("âœ… OpenAI ì„ë² ë” ì´ˆê¸°í™” ì„±ê³µ")
            return embed_func
        except ImportError as e:
            print(f"âŒ OpenAI ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}")
        return None

def split_src_by_tgt_units_spacy_tokenizer(
    tgt_sentences: List[str], 
    src_text: str, 
    tokenizer_func=None,
    nlp=None
) -> List[str]:
    """spaCy + í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•œ Vice Versa ì›ë¬¸ ë¶„í•  (ë¬´ê²°ì„± ë³´ì¥)"""
    return safe_source_split(tgt_sentences, src_text, tokenizer_func, nlp)

def split_src_by_tgt_units_vice_versa(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    tokenizer_func=None,
    similarity_threshold: float = 0.3
) -> List[str]:
    """SAì˜ Vice Versa: ë²ˆì—­ë¬¸ ë¬¸ì¥ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì›ë¬¸ì„ ë¶„í•  (ë¬´ê²°ì„± ë³´ì¥)"""
    return safe_source_split(tgt_sentences, src_text, tokenizer_func, None)

def compute_similarity_simple(text1: str, text2: str) -> float:
    """ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜ ìœ ì‚¬ë„"""
    if not text1.strip() or not text2.strip():
        return 0.0
    
    len1, len2 = len(text1), len(text2)
    if len1 == 0 or len2 == 0:
        return 0.0
    
    ratio = min(len1, len2) / max(len1, len2)
    return 0.5 + (ratio * 0.5)

def improved_align_paragraphs(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """ê¸°ì¡´ ìˆœì°¨ì  1:1 ì •ë ¬ (ë¬´ê²°ì„± ë³´ì¥)"""
    if not tgt_sentences:
        return []
    
    # ì›ë¬¸ì„ ë²ˆì—­ë¬¸ ê°œìˆ˜ì— ë§ì¶° ìˆœì°¨ì ìœ¼ë¡œ ë¶„í• 
    aligned_src_chunks = split_source_by_whitespace_and_align(src_text, len(tgt_sentences))
    
    alignments = []
    for i in range(len(tgt_sentences)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[i] if i < len(aligned_src_chunks) else '',
            'ë²ˆì—­ë¬¸': tgt_sentences[i],
            'similarity': 1.0,
            'split_method': 'punctuation',
            'align_method': 'sequential'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'punctuation',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments

def improved_align_paragraphs_spacy_tokenizer(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    tokenizer_func=None,
    nlp=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬ (ë¬´ê²°ì„± ë³´ì¥)"""
    if not tgt_sentences:
        return []
    
    # spaCy + í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•œ ì›ë¬¸ ë¶„í• 
    aligned_src_chunks = safe_source_split(tgt_sentences, src_text, tokenizer_func, nlp)
    
    alignments = []
    for i in range(len(tgt_sentences)):
        src_chunk = aligned_src_chunks[i] if i < len(aligned_src_chunks) else ''
        tgt_sentence = tgt_sentences[i]
        
        similarity = compute_similarity_simple(src_chunk, tgt_sentence)
        
        alignments.append({
            'ì›ë¬¸': src_chunk,
            'ë²ˆì—­ë¬¸': tgt_sentence,
            'similarity': similarity,
            'split_method': 'spacy_tokenizer_fusion',
            'align_method': 'spacy_tokenizer_based_split'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'spacy_tokenizer_fusion',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments

def process_paragraph_alignment(
    src_paragraph: str, 
    tgt_paragraph: str, 
    embedder_name: str = 'bge',
    tokenizer_name: str = 'jieba',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8,
    use_spacy_tokenizer: bool = False
):
    """PA ì²˜ë¦¬: ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥"""
    
    print(f"ğŸ”„ PA ì²˜ë¦¬ ì‹œì‘ (ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥)")
    
    # ì›ë³¸ ë¬¸ë‹¨ ì €ì¥
    para_id = f"paragraph_{id(src_paragraph)}_{id(tgt_paragraph)}"
    integrity_manager.store_original(src_paragraph, f"{para_id}_src")
    integrity_manager.store_original(tgt_paragraph, f"{para_id}_tgt")
    
    try:
        # 1. ê¸°ì¡´ ìˆœì°¨ì  ì •ë ¬ (punctuation)
        tgt_sentences_seq = safe_text_split(tgt_paragraph, max_length, "punctuation")
        alignments_seq = improved_align_paragraphs(tgt_sentences_seq, src_paragraph)
        
        # 2. ê¸°ì¡´ ì˜ë¯¸ì  ì •ë ¬ (spacy)
        tgt_sentences_sem = safe_text_split(tgt_paragraph, max_length, "spacy")
        embed_func = get_embedder_function(embedder_name, device=device)
        alignments_sem = improved_align_paragraphs(tgt_sentences_sem, src_paragraph, embed_func, similarity_threshold)
        
        # 3. ê¸°ì¡´ Vice Versa í† í¬ë‚˜ì´ì € ì •ë ¬
        tokenizer_func = get_tokenizer_function(tokenizer_name)
        tgt_sentences_tok = safe_text_split(tgt_paragraph, max_length, "punctuation")
        aligned_src_chunks = safe_source_split(tgt_sentences_tok, src_paragraph, tokenizer_func, None)
        
        alignments_tok = []
        for i, (src_chunk, tgt_sentence) in enumerate(zip(aligned_src_chunks, tgt_sentences_tok)):
            similarity = compute_similarity_simple(src_chunk, tgt_sentence)
            alignments_tok.append({
                'ì›ë¬¸': src_chunk,
                'ë²ˆì—­ë¬¸': tgt_sentence,
                'similarity': similarity,
                'split_method': 'vice_versa_tokenized',
                'align_method': 'tgt_based_src_split'
            })
        
        # 4. ìƒˆë¡œìš´ spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬
        alignments_spacy_tok = []
        if use_spacy_tokenizer:
            nlp = get_spacy_nlp()
            tgt_sentences_spacy_tok = safe_text_split(tgt_paragraph, max_length, "spacy_tokenizer")
            alignments_spacy_tok = improved_align_paragraphs_spacy_tokenizer(
                tgt_sentences_spacy_tok, src_paragraph, embed_func, tokenizer_func, nlp, similarity_threshold
            )
        
        # ìµœì  ë°©ì‹ ì„ íƒ ë° ê²°ê³¼ ìƒì„±
        all_alignments = [alignments_seq, alignments_sem, alignments_tok, alignments_spacy_tok]
        max_len = max(len(alignments) for alignments in all_alignments if alignments)
        
        results = []
        for i in range(max_len):
            seq = alignments_seq[i] if i < len(alignments_seq) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'punctuation','align_method':'sequential'}
            sem = alignments_sem[i] if i < len(alignments_sem) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'spacy','align_method':'semantic'}
            tok = alignments_tok[i] if i < len(alignments_tok) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'vice_versa_tokenized','align_method':'tgt_based_src_split'}
            spacy_tok = alignments_spacy_tok[i] if i < len(alignments_spacy_tok) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'spacy_tokenizer_fusion','align_method':'spacy_tokenizer_based_split'}
            
            if use_spacy_tokenizer and alignments_spacy_tok:
                weighted_sim = seq['similarity']*0.2 + sem['similarity']*0.3 + tok['similarity']*0.2 + spacy_tok['similarity']*0.3
                
                if weighted_sim >= quality_threshold:
                    result = {
                        'ì›ë¬¸': spacy_tok['ì›ë¬¸'] if spacy_tok['ì›ë¬¸'] else (tok['ì›ë¬¸'] if tok['ì›ë¬¸'] else (sem['ì›ë¬¸'] if sem['ì›ë¬¸'] else seq['ì›ë¬¸'])),
                        'ë²ˆì—­ë¬¸': spacy_tok['ë²ˆì—­ë¬¸'] if spacy_tok['ë²ˆì—­ë¬¸'] else (tok['ë²ˆì—­ë¬¸'] if tok['ë²ˆì—­ë¬¸'] else (sem['ë²ˆì—­ë¬¸'] if sem['ë²ˆì—­ë¬¸'] else seq['ë²ˆì—­ë¬¸'])),
                        'similarity': weighted_sim,
                        'split_method': f"seq+sem+tok+spacy_tok",
                        'align_method': 'hybrid_with_spacy_tokenizer'
                    }
                else:
                    result = spacy_tok.copy()
                    result['align_method'] = 'spacy_tokenizer_fusion_only'
            else:
                weighted_sim = seq['similarity']*0.3 + sem['similarity']*0.4 + tok['similarity']*0.3
                
                if weighted_sim >= quality_threshold:
                    result = {
                        'ì›ë¬¸': tok['ì›ë¬¸'] if tok['ì›ë¬¸'] else (sem['ì›ë¬¸'] if sem['ì›ë¬¸'] else seq['ì›ë¬¸']),
                        'ë²ˆì—­ë¬¸': tok['ë²ˆì—­ë¬¸'] if tok['ë²ˆì—­ë¬¸'] else (sem['ë²ˆì—­ë¬¸'] if sem['ë²ˆì—­ë¬¸'] else seq['ë²ˆì—­ë¬¸']),
                        'similarity': weighted_sim,
                        'split_method': f"seq+sem+tok",
                        'align_method': 'hybrid_with_tokenizer'
                    }
                else:
                    result = tok.copy()
                    result['align_method'] = 'tokenizer_vice_versa_only'
            
            results.append(result)
        
        # ìµœì¢… ë¬´ê²°ì„± ê²€ì¦
        if not verify_paragraph_integrity(src_paragraph, tgt_paragraph, results):
            logger.warning("ë¬¸ë‹¨ ë¬´ê²°ì„± ì‹¤íŒ¨, ë³µì› ì‹œë„")
            results = restore_paragraph_integrity(src_paragraph, tgt_paragraph, results)
            
            # ì¬ê²€ì¦
            if not verify_paragraph_integrity(src_paragraph, tgt_paragraph, results):
                logger.error("ë¬´ê²°ì„± ë³µì› ì‹¤íŒ¨")
        
        return results
        
    except Exception as e:
        logger.error(f"ë¬¸ë‹¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ì‹œ ì•ˆì „í•œ ê¸°ë³¸ ì²˜ë¦¬
        return [{
            'ì›ë¬¸': src_paragraph,
            'ë²ˆì—­ë¬¸': tgt_paragraph,
            'similarity': 1.0,
            'split_method': 'error_fallback',
            'align_method': 'error_fallback'
        }]

def process_paragraph_file(
    input_file: str, 
    output_file: str, 
    embedder_name: str = 'bge',
    tokenizer_name: str = 'jieba',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8,
    use_spacy_tokenizer: bool = False,
    verbose: bool = False,
    **kwargs
):
    """íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ - ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥"""
    print(f"ğŸ“‚ PA íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥): {input_file}")
    if use_spacy_tokenizer:
        print(f"ğŸ”— ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € + spaCy í† í¬ë‚˜ì´ì € ìœµí•©")
    else:
        print(f"ğŸ”„ ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € í†µí•©")
    print(f"âš™ï¸  í† í¬ë‚˜ì´ì €: {tokenizer_name}")
    print(f"âš™ï¸  ì„ë² ë”: {embedder_name}")
    print(f"ğŸ”—  spaCy ìœµí•©: {use_spacy_tokenizer}")
    print(f"ğŸ”’  ë¬´ê²°ì„± ë³´ì¥: ON")
    
    try:
        df = pd.read_excel(input_file)
        print(f"ğŸ“„ {len(df)}ê°œ ë¬¸ë‹¨ ë¡œë“œë¨")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None
    
    # ì›ë³¸ ë°ì´í„° ì „ì²´ ì²´í¬ì„¬ ì €ì¥
    original_src_all = ''.join([str(row.get('ì›ë¬¸', '')) for _, row in df.iterrows()])
    original_tgt_all = ''.join([str(row.get('ë²ˆì—­ë¬¸', '')) for _, row in df.iterrows()])
    
    file_id = f"file_{id(input_file)}"
    integrity_manager.store_original(original_src_all, f"{file_id}_src_all")
    integrity_manager.store_original(original_tgt_all, f"{file_id}_tgt_all")
    
    all_results = []
    total = len(df)
    processed_count = 0
    error_count = 0
    
    for idx, row in tqdm(df.iterrows(), total=total, desc="ğŸ“Š ë¬¸ë‹¨ ì²˜ë¦¬ (ë¬´ê²°ì„± ë³´ì¥)"):
        src_paragraph = str(row.get('ì›ë¬¸', ''))
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', ''))
        
        if src_paragraph.strip() and tgt_paragraph.strip():
            try:
                alignments = process_paragraph_alignment(
                    src_paragraph,
                    tgt_paragraph,
                    embedder_name=embedder_name,
                    tokenizer_name=tokenizer_name,
                    max_length=max_length,
                    similarity_threshold=similarity_threshold,
                    device=device,
                    quality_threshold=quality_threshold,
                    use_spacy_tokenizer=use_spacy_tokenizer
                )
                
                # ë¬¸ë‹¨ì‹ë³„ì ë¶€ì—¬
                for a in alignments:
                    a['ë¬¸ë‹¨ì‹ë³„ì'] = idx + 1
                
                all_results.extend(alignments)
                processed_count += 1
                
            except Exception as e:
                print(f"âŒ ë¬¸ë‹¨ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                error_count += 1
                if verbose:
                    import traceback
                    traceback.print_exc()
                
                # ì˜¤ë¥˜ì‹œ ì•ˆì „í•œ í´ë°±
                all_results.append({
                    'ë¬¸ë‹¨ì‹ë³„ì': idx + 1,
                    'ì›ë¬¸': src_paragraph,
                    'ë²ˆì—­ë¬¸': tgt_paragraph,
                    'similarity': 1.0,
                    'split_method': 'error_fallback',
                    'align_method': 'error_fallback'
                })
    
    if not all_results:
        print("âŒ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    result_df = pd.DataFrame(all_results)
    final_columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
    result_df = result_df[final_columns]

    # === ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ë° ë³´ì™„ ===
    print("ğŸ”’ ìµœì¢… ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
    
    output_src_all = ''.join(result_df['ì›ë¬¸'].fillna(''))
    output_tgt_all = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna(''))
    
    # ì›ë¬¸ ë¬´ê²°ì„± ê²€ì¦
    src_valid, src_msg = integrity_manager.verify_integrity(output_src_all, f"{file_id}_src_all")
    if not src_valid:
        print(f'âš ï¸ ì›ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: {src_msg}')
        print('ğŸ”§ ì›ë¬¸ ë³µì› ì‹œë„ ì¤‘...')
        
        sm = SequenceMatcher(None, output_src_all.replace(' ', ''), original_src_all.replace(' ', ''))
        opcodes = sm.get_opcodes()
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ì›ë¬¸'] += original_src_all[j1:j2]
                else:
                    new_row = pd.DataFrame([{
                        'ë¬¸ë‹¨ì‹ë³„ì': df.shape[0] + 1,
                        'ì›ë¬¸': original_src_all[j1:j2],
                        'ë²ˆì—­ë¬¸': '',
                        'similarity': 1.0,
                        'split_method': 'integrity_restore',
                        'align_method': 'src_missing_patch'
                    }])
                    result_df = pd.concat([result_df, new_row], ignore_index=True)
                print(f"âœ… ëˆ„ë½ ì›ë¬¸ ë³µì›: '{original_src_all[j1:j2][:50]}...'")
                
            elif tag == 'delete':
                excess_text = output_src_all[i1:i2]
                for idx in result_df.index:
                    if excess_text in str(result_df.at[idx, 'ì›ë¬¸']):
                        result_df.at[idx, 'ì›ë¬¸'] = str(result_df.at[idx, 'ì›ë¬¸']).replace(excess_text, '', 1)
                        print(f"âœ… ì¤‘ë³µ ì›ë¬¸ ì œê±°: '{excess_text[:50]}...'")
                        break
    
    # ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ê²€ì¦
    output_tgt_all_after_src = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna(''))
    tgt_valid, tgt_msg = integrity_manager.verify_integrity(output_tgt_all_after_src, f"{file_id}_tgt_all")
    
    if not tgt_valid:
        print(f'âš ï¸ ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: {tgt_msg}')
        print('ğŸ”§ ë²ˆì—­ë¬¸ ë³µì› ì‹œë„ ì¤‘...')
        
        sm = SequenceMatcher(None, output_tgt_all_after_src.replace(' ', ''), original_tgt_all.replace(' ', ''))
        opcodes = sm.get_opcodes()
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸'] += original_tgt_all[j1:j2]
                else:
                    new_row = pd.DataFrame([{
                        'ë¬¸ë‹¨ì‹ë³„ì': df.shape[0] + 1,
                        'ì›ë¬¸': '',
                        'ë²ˆì—­ë¬¸': original_tgt_all[j1:j2],
                        'similarity': 1.0,
                        'split_method': 'integrity_restore',
                        'align_method': 'tgt_missing_patch'
                    }])
                    result_df = pd.concat([result_df, new_row], ignore_index=True)
                print(f"âœ… ëˆ„ë½ ë²ˆì—­ë¬¸ ë³µì›: '{original_tgt_all[j1:j2][:50]}...'")
                
            elif tag == 'delete':
                excess_text = output_tgt_all_after_src[i1:i2]
                for idx in result_df.index:
                    if excess_text in str(result_df.at[idx, 'ë²ˆì—­ë¬¸']):
                        result_df.at[idx, 'ë²ˆì—­ë¬¸'] = str(result_df.at[idx, 'ë²ˆì—­ë¬¸']).replace(excess_text, '', 1)
                        print(f"âœ… ì¤‘ë³µ ë²ˆì—­ë¬¸ ì œê±°: '{excess_text[:50]}...'")
                        break
    
    # ìµœì¢… ì €ì¥
    result_df = result_df[final_columns]
    result_df.to_excel(output_file, index=False)
    
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë¬¸ì¥ ìŒ ìƒì„±")
    print(f"âœ… ì²˜ë¦¬ ì„±ê³µ: {processed_count}ê°œ ë¬¸ë‹¨")
    print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {error_count}ê°œ ë¬¸ë‹¨")
    
    # ë¬´ê²°ì„± í†µê³„
    final_src_check = ''.join(result_df['ì›ë¬¸'].fillna(''))
    final_tgt_check = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna(''))
    
    final_src_valid, _ = integrity_manager.verify_integrity(final_src_check, f"{file_id}_src_all")
    final_tgt_valid, _ = integrity_manager.verify_integrity(final_tgt_check, f"{file_id}_tgt_all")
    
    print(f"ğŸ”’ ìµœì¢… ë¬´ê²°ì„± ìƒíƒœ:")
    print(f"   ì›ë¬¸: {'âœ… ì™„ë²½' if final_src_valid else 'âŒ ë¶ˆì™„ì „'}")
    print(f"   ë²ˆì—­ë¬¸: {'âœ… ì™„ë²½' if final_tgt_valid else 'âŒ ë¶ˆì™„ì „'}")
    
    if use_spacy_tokenizer:
        print(f"ğŸ”— spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ë°©ì‹ ì™„ë£Œ")
    else:
        print(f"ğŸ”„ Vice Versa í† í¬ë‚˜ì´ì € ë°©ì‹ ì™„ë£Œ")
    
    return result_df