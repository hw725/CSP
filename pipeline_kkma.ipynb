{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 원문-번역문 의미 기반 구 단위 매칭 파이프라인\n",
        "\n",
        "이 노트북은 한문 원문과 번역문 간의 구 단위 정렬을 위한 파이프라인을 구현합니다. 다음과 같은 주요 기능을 포함합니다:\n",
        "\n",
        "1. **텍스트 토크나이징**: 원문과 번역문을 의미 단위로 분할\n",
        "2. **임베딩 계산**: 각 의미 단위에 대한 벡터 표현 생성\n",
        "3. **구 단위 정렬**: 동적 프로그래밍 기반 정렬 알고리즘\n",
        "4. **파일 입출력**: Excel 파일 처리\n",
        "\n",
        "이 파이프라인은 원문-번역문 쌍을 입력으로 받아 구 단위로 정렬된 결과를 출력합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 패키지 설치 및 임포트\n",
        "\n",
        "아래 셀을 실행하여 필요한 패키지를 설치하고 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "%pip install regex pandas numpy tqdm torch sentence-transformers openpyxl FlagEmbedding konlpy tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import torch\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any, Optional, Callable\n",
        "from tqdm.notebook import tqdm\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from soynlp.tokenizer import LTokenizer  # Using soynlp tokenizer\n",
        "import sentencepiece as spm\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "\n",
        "\n",
        "# Initialize soynlp tokenizer\n",
        "tokenizer = LTokenizer()\n",
        "\n",
        "# Load BGE model\n",
        "model = BGEM3FlagModel(\n",
        "    'BAAI/bge-m3',\n",
        "    use_fp16=True\n",
        ")\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 임베딩 모듈 (embedder.py)\n",
        "\n",
        "텍스트 구에 대한 임베딩을 계산하고 캐시를 관리하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding module\n",
        "_embedding_cache = {}\n",
        "\n",
        "# compute_embeddings_with_cache 함수 교체\n",
        "def compute_embeddings_with_cache(\n",
        "    texts: List[str],\n",
        "    batch_size: int = 20,\n",
        "    show_batch_progress: bool = False\n",
        ") -> np.ndarray:\n",
        "    \"\"\"향상된 임베딩 캐싱 기능\"\"\"\n",
        "    global _embedding_cache\n",
        "    \n",
        "    result_list: List[Optional[np.ndarray]] = [None] * len(texts)\n",
        "    to_embed: List[str] = []\n",
        "    indices_to_embed: List[int] = []\n",
        "\n",
        "    # 캐시 확인\n",
        "    for i, txt in enumerate(texts):\n",
        "        if txt in _embedding_cache:\n",
        "            result_list[i] = _embedding_cache[txt]\n",
        "        else:\n",
        "            to_embed.append(txt)\n",
        "            indices_to_embed.append(i)\n",
        "\n",
        "    # 새 임베딩 계산 필요시\n",
        "    if to_embed:\n",
        "        embeddings = []\n",
        "        it = range(0, len(to_embed), batch_size)\n",
        "        if show_batch_progress:\n",
        "            it = tqdm(it, desc=\"Embedding batches\", ncols=80)\n",
        "        \n",
        "        # GPU 메모리 정리\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        for start in it:\n",
        "            batch = to_embed[start:start + batch_size]\n",
        "            try:\n",
        "                output = model.encode(\n",
        "                    batch,\n",
        "                    return_dense=True,\n",
        "                    return_sparse=False,  # 필요한 출력만 계산\n",
        "                    return_colbert_vecs=False  # 필요한 출력만 계산\n",
        "                )\n",
        "                dense = output['dense_vecs']\n",
        "            except RuntimeError as e:\n",
        "                # OOM 오류 시 배치 크기 줄여서 재시도\n",
        "                if \"out of memory\" in str(e) and batch_size > 1:\n",
        "                    reduced_batch = batch_size // 2\n",
        "                    logger.warning(f\"메모리 부족, 배치 크기 축소: {batch_size} -> {reduced_batch}\")\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    \n",
        "                    # 재귀적으로 더 작은 배치로 처리\n",
        "                    return compute_embeddings_with_cache(\n",
        "                        texts, \n",
        "                        batch_size=reduced_batch,\n",
        "                        show_batch_progress=show_batch_progress\n",
        "                    )\n",
        "                else:\n",
        "                    raise e\n",
        "                    \n",
        "            embeddings.extend(dense)\n",
        "\n",
        "        # 캐시 업데이트\n",
        "        for i, (txt, emb) in enumerate(zip(to_embed, embeddings)):\n",
        "            _embedding_cache[txt] = emb\n",
        "            result_list[indices_to_embed[i]] = emb\n",
        "\n",
        "    return np.array(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 구두점 처리 모듈 (punctuation.py)\n",
        "\n",
        "괄호 처리 및 마스킹 기능을 제공하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Punctuation handling module\n",
        "MASK_TEMPLATE = '[MASK{}]'\n",
        "\n",
        "HALF_WIDTH_BRACKETS = [\n",
        "    ('(', ')'),\n",
        "    ('[', ']'),\n",
        "]\n",
        "FULL_WIDTH_BRACKETS = [\n",
        "    ('（', '）'),\n",
        "    ('［', '］'),\n",
        "]\n",
        "TRANS_BRACKETS = [\n",
        "    ('<', '>'),\n",
        "    ('《', '》'),\n",
        "    ('〈', '〉'),\n",
        "    ('「', '」'),\n",
        "    ('『', '』'),\n",
        "    ('〔', '〕'),\n",
        "    ('【', '】'),\n",
        "    ('〖', '〗'),\n",
        "    ('〘', '〙'),\n",
        "    ('〚', '〛'),\n",
        "]\n",
        "\n",
        "ALL_BRACKETS = HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS + TRANS_BRACKETS\n",
        "\n",
        "def mask_brackets(text: str, text_type: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Mask content within brackets according to rules.\"\"\"\n",
        "    assert text_type in {'source', 'target'}, \"text_type must be 'source' or 'target'\"\n",
        "\n",
        "    masks: List[str] = []\n",
        "    mask_id = [0]\n",
        "\n",
        "    def safe_sub(pattern, repl, s):\n",
        "        def safe_replacer(m):\n",
        "            if '[MASK' in m.group(0):\n",
        "                return m.group(0)\n",
        "            return repl(m)\n",
        "        return pattern.sub(safe_replacer, s)\n",
        "\n",
        "    patterns: List[Tuple[re.Pattern, bool]] = []\n",
        "\n",
        "    if text_type == 'source':\n",
        "        for left, right in HALF_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "    elif text_type == 'target':\n",
        "        for left, right in HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in TRANS_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "\n",
        "    def mask_content(s: str, pattern: re.Pattern, content_mask: bool) -> str:\n",
        "        def replacer(match: re.Match) -> str:\n",
        "            token = MASK_TEMPLATE.format(mask_id[0])\n",
        "            masks.append(match.group())\n",
        "            mask_id[0] += 1\n",
        "            return token\n",
        "        return safe_sub(pattern, replacer, s)\n",
        "\n",
        "    for pattern, content_mask in patterns:\n",
        "        if content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "    for pattern, content_mask in patterns:\n",
        "        if not content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "\n",
        "    return text, masks\n",
        "\n",
        "def restore_masks(text: str, masks: List[str]) -> str:\n",
        "    \"\"\"Restore masked tokens to their original content.\"\"\"\n",
        "    for i, original in enumerate(masks):\n",
        "        text = text.replace(MASK_TEMPLATE.format(i), original)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 토크나이저 모듈 (tokenizer.py)\n",
        "\n",
        "원문과 번역문을 의미 단위로 분할하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "# Kkma 형태소 분석기\n",
        "kkma = Kkma()\n",
        "\n",
        "# 미리 컴파일된 정규식\n",
        "hanja_re    = regex.compile(r'\\p{Han}+')\n",
        "hangul_re   = regex.compile(r'^\\p{Hangul}+$')\n",
        "combined_re = regex.compile(\n",
        "    r'(\\p{Han}+)+(?:\\p{Hangul}+)(?:은|는|이|가|을|를|에|에서|으로|로|와|과|도|만|으며|고|하고|의|때)?'\n",
        ")\n",
        "\n",
        "def split_src_meaning_units(text: str) -> list[str]:\n",
        "    text = text.replace('\\n', ' ').replace('：', '： ')\n",
        "    tokens = regex.findall(r'\\S+', text)\n",
        "    units: list[str] = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "        m = combined_re.match(tok)\n",
        "        if m:\n",
        "            units.append(m.group(0))\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if hanja_re.search(tok):\n",
        "            unit = tok\n",
        "            j = i + 1\n",
        "            while j < len(tokens) and hangul_re.match(tokens[j]):\n",
        "                unit += tokens[j]\n",
        "                j += 1\n",
        "            units.append(unit)\n",
        "            i = j\n",
        "            continue\n",
        "\n",
        "        if hangul_re.match(tok):\n",
        "            korean_tokens = kkma.morphs(tok)\n",
        "            units.extend(korean_tokens)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        units.append(tok)\n",
        "        i += 1\n",
        "\n",
        "    return units\n",
        "\n",
        "def split_inside_chunk(chunk: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    조사, 어미, 그리고 '：' 기준으로 의미 단위 분할\n",
        "    원형 보존, 공백 삽입 없이 분리\n",
        "    \"\"\"\n",
        "    delimiters = ['을', '를', '이', '가', '은', '는', '에', '에서', '로', '으로',\n",
        "                  '와', '과', '고', '며', '하고', '때', '의', '도', '만', '：']\n",
        "    \n",
        "    # lookbehind 패턴 생성\n",
        "    pattern = '|'.join([f'(?<={re.escape(d)})' for d in delimiters])\n",
        "    try:\n",
        "        parts = re.split(pattern, chunk)\n",
        "        return [p.strip() for p in parts if p.strip()]\n",
        "    except:\n",
        "        return [p for p in chunk.split() if p.strip()]\n",
        "\n",
        "def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:\n",
        "    hanja_chars = regex.findall(r'\\p{Han}+', src_unit)\n",
        "    if not hanja_chars:\n",
        "        return 0\n",
        "    last = hanja_chars[-1]\n",
        "    idx = remaining_tgt.rfind(last)\n",
        "    if idx == -1:\n",
        "        return len(remaining_tgt)\n",
        "    end = idx + len(last)\n",
        "    next_space = remaining_tgt.find(' ', end)\n",
        "    return next_space + 1 if next_space != -1 else len(remaining_tgt)\n",
        "\n",
        "def find_target_span_end_semantic(\n",
        "    src_unit: str,\n",
        "    remaining_tgt: str,\n",
        "    embed_func=compute_embeddings_with_cache,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50,\n",
        "    similarity_threshold: float = 0.4\n",
        ") -> int:\n",
        "    \"\"\"최적화된 타겟 스팬 탐색 함수\"\"\"\n",
        "    # 예외 처리 강화\n",
        "    if not src_unit or not remaining_tgt:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        # 1) 원문 임베딩 (단일 계산)\n",
        "        src_emb = embed_func([src_unit])[0]\n",
        "        \n",
        "        # 2) 번역문 토큰 분리 및 누적 길이 계산\n",
        "        tgt_tokens = remaining_tgt.split()\n",
        "        if not tgt_tokens:\n",
        "            return 0\n",
        "            \n",
        "        upper = min(len(tgt_tokens), max_tokens)\n",
        "        cumulative_lengths = [0]\n",
        "        current_length = 0\n",
        "        \n",
        "        for tok in tgt_tokens:\n",
        "            current_length += len(tok) + 1  # 토큰 + 공백\n",
        "            cumulative_lengths.append(current_length)\n",
        "            \n",
        "        # 3) 후보 세그먼트 생성 (메모리 효율적 방식)\n",
        "        candidates = []\n",
        "        candidate_indices = []\n",
        "        \n",
        "        # 적은 수의 후보만 생성하여 효율성 향상\n",
        "        step_size = 1 if upper <= 10 else 2  # 토큰 10개 이하면 모든 후보 검사, 이상이면 2칸씩\n",
        "        \n",
        "        for end_i in range(min_tokens-1, upper, step_size):\n",
        "            cand = \" \".join(tgt_tokens[:end_i+1])\n",
        "            candidates.append(cand)\n",
        "            candidate_indices.append(end_i)\n",
        "            \n",
        "        # 4) 배치 임베딩 (한 번에 계산)\n",
        "        cand_embs = embed_func(candidates)\n",
        "        \n",
        "        # 5) 최적 매칭 탐색 (코사인 유사도 + 길이 패널티)\n",
        "        best_score = -1.0\n",
        "        best_end_idx = cumulative_lengths[-1]  # 기본값은 전체 길이\n",
        "        \n",
        "        for i, emb in enumerate(cand_embs):\n",
        "            # 코사인 유사도 계산\n",
        "            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)\n",
        "            \n",
        "            # 길이 패널티 (너무 짧은 매칭 방지)\n",
        "            end_i = candidate_indices[i]\n",
        "            length_ratio = (end_i + 1) / len(tgt_tokens)\n",
        "            length_penalty = min(1.0, length_ratio * 2)  # 최대 1.0\n",
        "            \n",
        "            adjusted_score = score * length_penalty\n",
        "            \n",
        "            # 임계값 확인 및 최대값 갱신\n",
        "            if adjusted_score > best_score and score >= similarity_threshold:\n",
        "                best_score = adjusted_score\n",
        "                best_end_idx = cumulative_lengths[end_i + 1]\n",
        "                \n",
        "        return best_end_idx\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"의미 매칭 오류, 단순 매칭으로 대체: {e}\")\n",
        "        return find_target_span_end_simple(src_unit, remaining_tgt)\n",
        "\n",
        "\n",
        "def split_tgt_by_src_units(src_units: list[str], tgt_text: str) -> list[str]:\n",
        "    results = []\n",
        "    cursor = 0\n",
        "    total = len(tgt_text)\n",
        "    for src_u in src_units:\n",
        "        remaining = tgt_text[cursor:]\n",
        "        end_len = find_target_span_end_simple(src_u, remaining)\n",
        "        chunk = tgt_text[cursor:cursor+end_len]\n",
        "        results.extend(split_inside_chunk(chunk))\n",
        "        cursor += end_len\n",
        "    if cursor < total:\n",
        "        results.extend(split_inside_chunk(tgt_text[cursor:]))\n",
        "    return results\n",
        "\n",
        "def split_tgt_by_src_units_semantic(src_units, tgt_text, embed_func=compute_embeddings_with_cache, min_tokens=1):\n",
        "    tgt_tokens = tgt_text.split()\n",
        "    N, T = len(src_units), len(tgt_tokens)\n",
        "    if N == 0 or T == 0:\n",
        "        return []\n",
        "\n",
        "    dp = np.full((N+1, T+1), -np.inf)\n",
        "    back = np.zeros((N+1, T+1), dtype=int)\n",
        "    dp[0, 0] = 0.0\n",
        "\n",
        "    # 원문 임베딩 계산\n",
        "    src_embs = embed_func(src_units)\n",
        "\n",
        "    # DP 테이블 채우기 (j 루프 범위 주의!)\n",
        "    for i in range(1, N+1):\n",
        "        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):\n",
        "            for k in range((i-1)*min_tokens, j-min_tokens+1):\n",
        "                span = \" \".join(tgt_tokens[k:j])\n",
        "                tgt_emb = embed_func([span])[0]\n",
        "                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))\n",
        "                score = dp[i-1, k] + sim\n",
        "                if score > dp[i, j]:\n",
        "                    dp[i, j] = score\n",
        "                    back[i, j] = k\n",
        "\n",
        "    # Traceback\n",
        "    cuts = [T]\n",
        "    curr = T\n",
        "    for i in range(N, 0, -1):\n",
        "        prev = int(back[i, curr])\n",
        "        cuts.append(prev)\n",
        "        curr = prev\n",
        "    cuts = cuts[::-1]\n",
        "    assert cuts[0] == 0 and cuts[-1] == T and len(cuts) == N + 1\n",
        "\n",
        "    # Build actual spans\n",
        "    tgt_spans = []\n",
        "    for i in range(N):\n",
        "        span = \" \".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()\n",
        "        tgt_spans.append(span)\n",
        "    return tgt_spans\n",
        "\n",
        "def split_tgt_meaning_units(\n",
        "    src_text: str,\n",
        "    tgt_text: str,\n",
        "    use_semantic: bool = True,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50\n",
        ") -> list[str]:\n",
        "    src_units = split_src_meaning_units(src_text)\n",
        "\n",
        "    if use_semantic:\n",
        "        return split_tgt_by_src_units_semantic(\n",
        "            src_units,\n",
        "            tgt_text,\n",
        "            embed_func=compute_embeddings_with_cache,\n",
        "            min_tokens=min_tokens\n",
        "        )\n",
        "    else:\n",
        "        return split_tgt_by_src_units(src_units, tgt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 정렬 모듈 (aligner.py)\n",
        "\n",
        "원문과 번역문 구 간의 정렬을 위한 알고리즘입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aligner module\n",
        "def cosine_similarity(vec1: Any, vec2: Any) -> float:\n",
        "    \"\"\"Calculate cosine similarity (handling zero vectors).\"\"\"\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
        "\n",
        "def align_src_tgt(src_units, tgt_units, embed_func=compute_embeddings_with_cache):\n",
        "    \"\"\"Align source and target units.\"\"\"\n",
        "    logger.info(f\"Source units: {len(src_units)} items, Target units: {len(tgt_units)} items\")\n",
        "\n",
        "    if len(src_units) != len(tgt_units):\n",
        "        try:\n",
        "            flatten_tgt = \" \".join(tgt_units)\n",
        "            new_tgt_units = split_tgt_by_src_units_semantic(src_units, flatten_tgt, embed_func, min_tokens=1)\n",
        "            if len(new_tgt_units) == len(src_units):\n",
        "                logger.info(\"Semantic re-alignment successful\")\n",
        "                return list(zip(src_units, new_tgt_units))\n",
        "            else:\n",
        "                logger.warning(f\"Length mismatch after re-alignment: Source={len(src_units)}, Target={len(new_tgt_units)}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during semantic re-alignment: {e}\")\n",
        "\n",
        "        if len(src_units) > len(tgt_units):\n",
        "            tgt_units.extend([\"\"] * (len(src_units) - len(tgt_units)))\n",
        "        else:\n",
        "            src_units.extend([\"\"] * (len(tgt_units) - len(src_units)))\n",
        "\n",
        "    return list(zip(src_units, tgt_units))\n",
        "\n",
        "def calculate_alignment_matrix(src_embs, tgt_embs, batch_size=512):\n",
        "    \"\"\"Optimized function for calculating large similarity matrices.\"\"\"\n",
        "    src_len, tgt_len = len(src_embs), len(tgt_embs)\n",
        "    similarity_matrix = np.zeros((src_len, tgt_len))\n",
        "\n",
        "    for i in range(0, src_len, batch_size):\n",
        "        batch_src = src_embs[i:i + batch_size]\n",
        "        for j in range(0, tgt_len, batch_size):\n",
        "            batch_tgt = tgt_embs[j:j + batch_size]\n",
        "            batch_src_norm = np.linalg.norm(batch_src, axis=1, keepdims=True)\n",
        "            batch_tgt_norm = np.linalg.norm(batch_tgt, axis=1, keepdims=True)\n",
        "\n",
        "            dots = np.matmul(batch_src, batch_tgt.T)\n",
        "            norms = np.matmul(batch_src_norm, batch_tgt_norm.T)\n",
        "            batch_sim = dots / (norms + 1e-8)\n",
        "\n",
        "            similarity_matrix[i:i + batch_size, j:j + batch_size] = batch_sim\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. I/O 모듈 (io_manager.py)\n",
        "\n",
        "Excel 파일을 읽고 처리한 후 결과를 저장하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I/O module\n",
        "def process_file(input_path: str, output_path: str, batch_size: int = 128, verbose: bool = False) -> None:\n",
        "    \"\"\"청크 단위로 최적화된 파일 처리 함수\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "        return\n",
        "\n",
        "    if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "        logger.error(\"[IO] Missing '원문' or '번역문' columns.\")\n",
        "        return\n",
        "\n",
        "    outputs: List[Dict[str, Any]] = []\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    # 처리할 행을 작은 청크로 분할\n",
        "    chunk_size = min(50, total_rows)  # 최대 50행씩 처리\n",
        "    \n",
        "    # 임베딩 캐시 초기화 - 메모리 관리\n",
        "    global _embedding_cache\n",
        "    \n",
        "    for chunk_start in tqdm(range(0, total_rows, chunk_size), desc=\"Processing chunks\"):\n",
        "        chunk_end = min(chunk_start + chunk_size, total_rows)\n",
        "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
        "        \n",
        "        # 청크별 임베딩 캐시 관리 (메모리 효율성)\n",
        "        if len(_embedding_cache) > 10000:  # 캐시가 너무 크면\n",
        "            _embedding_cache = {}  # 초기화\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # 청크 내 행 처리\n",
        "        for idx, row in enumerate(chunk_df.itertuples(index=False), start=chunk_start+1):\n",
        "            src_text = str(getattr(row, '원문', '') or '')\n",
        "            tgt_text = str(getattr(row, '번역문', '') or '')\n",
        "            if verbose:\n",
        "                print(f\"\\n[========= ROW {idx} =========]\")\n",
        "                print(\"Source (input):\", src_text)\n",
        "                print(\"Target (input):\", tgt_text)\n",
        "\n",
        "            try:\n",
        "                masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "                masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "                src_units = split_src_meaning_units(masked_src)\n",
        "                tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "\n",
        "                restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "                restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "                aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Alignment result: (source to target comparison)\")\n",
        "                    for src_gu, tgt_gu in zip(aligned_src_units, aligned_tgt_units):\n",
        "                        print(f\"SRC: {src_gu} | TGT: {tgt_gu}\")\n",
        "\n",
        "                for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                    outputs.append({\n",
        "                        \"문장식별자\": idx,\n",
        "                        \"구식별자\": gu_idx,\n",
        "                        \"원문구\": src_gu,\n",
        "                        \"번역구\": tgt_gu,\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"[IO] Failed to process row {idx}: {e}\")\n",
        "                outputs.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "\n",
        "    try:\n",
        "        output_df = pd.DataFrame(outputs, columns=[\"문장식별자\", \"구식별자\", \"원문구\", \"번역구\"])\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        if verbose:\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to save results: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 메인 실행 함수 (main.py)\n",
        "\n",
        "파이프라인 전체를 실행하는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main execution function\n",
        "def main(input_file, output_file, verbose=False):\n",
        "    \"\"\"Execute the pipeline.\"\"\"\n",
        "    if not os.path.isfile(input_file):\n",
        "        logger.error(f\"Input file does not exist: {input_file}\")\n",
        "        return\n",
        "    if not input_file.lower().endswith(('.xls', '.xlsx')):\n",
        "        logger.error(\"Input file must have .xls/.xlsx extension.\")\n",
        "        return\n",
        "    if not output_file.lower().endswith('.xlsx'):\n",
        "        logger.error(\"Output file must have .xlsx extension.\")\n",
        "        return\n",
        "\n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "        logger.info(\"Verbose mode activated: INFO level logging enabled.\")\n",
        "\n",
        "    try:\n",
        "        process_file(\n",
        "            input_path=input_file,\n",
        "            output_path=output_file,\n",
        "            batch_size=128,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        logger.info(f\"Processing completed: {output_file}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical error occurred during pipeline execution: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 테스트\n",
        "\n",
        "아래 셀을 실행하여 간단한 예제로 파이프라인을 테스트해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing function\n",
        "def create_test_data(file_path=\"test_input.xlsx\"):\n",
        "    \"\"\"Generate test data.\"\"\"\n",
        "    test_data = [\n",
        "        {\n",
        "            \"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\",\n",
        "            \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\",\n",
        "            \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"夫雅頌之作也 詩人各有所屬者也\",\n",
        "            \"번역문\": \"무릇 아송의 창작은 시인마다 각자 속한 바가 있었다.\"\n",
        "        },\n",
        "        {    \n",
        "            \"원문\": \"然而孔子取而次之者하야 則有家國之次矣\",\n",
        "            \"번역문\": \"그런데 공자가 이것을 취하여 차례를 매긴 것은 가국의 순서에 따른 것이다.\"        },\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(test_data)\n",
        "    df.to_excel(file_path, index=False, engine='openpyxl')\n",
        "    return file_path\n",
        "\n",
        "# Testing execution\n",
        "test_input = create_test_data()\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# Run the pipeline\n",
        "main(test_input, test_output, verbose=True)\n",
        "\n",
        "# Check results\n",
        "try:\n",
        "    result_df = pd.read_excel(test_output)\n",
        "    display(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read result file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 단일 문장 테스트\n",
        "\n",
        "특정 문장 쌍에 대해 빠르게 테스트해볼 수 있는 기능입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single sentence test function\n",
        "def test_single_alignment(src_text, tgt_text):\n",
        "    \"\"\"Test alignment for a single sentence pair.\"\"\"\n",
        "    print(\"=== Input Sentences ===\")\n",
        "    print(f\"Source: {src_text}\")\n",
        "    print(f\"Target: {tgt_text}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        # Preprocessing\n",
        "        masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "        masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "        # Split into units\n",
        "        src_units = split_src_meaning_units(masked_src)\n",
        "        print(\"=== Source Units ===\")\n",
        "        for i, unit in enumerate(src_units):\n",
        "            print(f\"[{i+1}] {unit}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        tgt_units = split_tgt_meaning_units(\n",
        "            masked_src,\n",
        "            masked_tgt,\n",
        "            use_semantic=True,\n",
        "            min_tokens=1,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "        print(\"=== Target Units ===\")\n",
        "        for i, unit in enumerate(tgt_units):\n",
        "            print(f\"[{i+1}] {unit}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Alignment\n",
        "        aligned_pairs = align_src_tgt(src_units, tgt_units, compute_embeddings_with_cache)\n",
        "        aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "        aligned_src_units = [restore_masks(unit, src_masks) for unit in aligned_src_units]\n",
        "        aligned_tgt_units = [restore_masks(unit, tgt_masks) for unit in aligned_tgt_units]\n",
        "\n",
        "        print(\"=== Alignment Results ===\")\n",
        "        for i, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), 1):\n",
        "            print(f\"[{i}] Source: {src_gu}\")\n",
        "            print(f\"    Target: {tgt_gu}\")\n",
        "            print()\n",
        "\n",
        "        return aligned_src_units, aligned_tgt_units\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "# Single sentence test execution\n",
        "src_example = \"作詁訓傳時에 移其篇第하고 因改之耳라\"\n",
        "tgt_example = \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"\n",
        "\n",
        "test_single_alignment(src_example, tgt_example)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
