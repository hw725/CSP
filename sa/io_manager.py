"""ê°„ì†Œí™”ëœ ë³‘ë ¬ ì²˜ë¦¬ - ë¶ˆí•„ìš”í•œ ì½”ë“œ ì œê±°"""

import pandas as pd
from tqdm import tqdm
import logging
from pathlib import Path
import multiprocessing as mp
from typing import Dict, Any, List
import sys
import os

# ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir))

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì´ˆê¸°í™”)
worker_embed_func = None
worker_modules = {}

def init_worker(embedder_name='bge'):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” - ê°„ì†Œí™”"""
    global worker_embed_func, worker_modules
    
    try:
        # ì„ë² ë” ì´ˆê¸°í™”
        if embedder_name == 'openai':
            from common.embedders.openai import compute_embeddings_with_cache
            worker_embed_func = compute_embeddings_with_cache
        else:  # bge ê¸°ë³¸ê°’
            from common.embedders.bge import get_embed_func
            worker_embed_func = get_embed_func()
        
        # í•„ìš”í•œ ëª¨ë“ˆë“¤ import
        from sa_tokenizers.jieba_mecab import (
            split_src_meaning_units, 
            split_tgt_by_src_units_semantic
        )
        from punctuation import mask_brackets, restore_brackets
        
        worker_modules = {
            'split_src_meaning_units': split_src_meaning_units,
            'split_tgt_by_src_units_semantic': split_tgt_by_src_units_semantic,
            'mask_brackets': mask_brackets,
            'restore_brackets': restore_brackets
        }
        
    except Exception as e:
        print(f"ì›Œì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        worker_embed_func = None
        worker_modules = {}

def process_batch_sentences(sentence_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ ë‹¨ìœ„ ë¬¸ì¥ ì²˜ë¦¬ - ê°„ì†Œí™”"""
    global worker_embed_func, worker_modules
    
    if worker_embed_func is None or not worker_modules:
        return []
    
    results = []
    
    for sentence_data in sentence_batch:
        try:
            sentence_id = sentence_data['sentence_id']
            src_text = sentence_data['src_text']
            tgt_text = sentence_data['tgt_text']
            
            # ëª¨ë“ˆë“¤ ê°€ì ¸ì˜¤ê¸°
            mask_brackets = worker_modules['mask_brackets']
            restore_brackets = worker_modules['restore_brackets']
            split_src_meaning_units = worker_modules['split_src_meaning_units']
            split_tgt_by_src_units_semantic = worker_modules['split_tgt_by_src_units_semantic']
            
            # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            masked_src, src_masks = mask_brackets(src_text, 'source')
            masked_tgt, tgt_masks = mask_brackets(tgt_text, 'target')
            
            src_units = split_src_meaning_units(masked_src)
            tgt_units = split_tgt_by_src_units_semantic(
                src_units, masked_tgt, worker_embed_func, min_tokens=1
            )
            
            # ê²°ê³¼ ìƒì„±
            for phrase_idx, (src_unit, tgt_unit) in enumerate(zip(src_units, tgt_units), 1):
                restored_src = restore_brackets(src_unit, src_masks)
                restored_tgt = restore_brackets(tgt_unit, tgt_masks)
                
                results.append({
                    'ë¬¸ì¥ì‹ë³„ì': sentence_id,
                    'êµ¬ì‹ë³„ì': phrase_idx,
                    'ì›ë¬¸': restored_src,
                    'ë²ˆì—­ë¬¸': restored_tgt
                })
            
        except Exception as e:
            logger.error(f"ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    return results

def process_file(
    input_path: str, 
    output_path: str, 
    parallel: bool = False, 
    workers: int = 4, 
    embedder_name: str = 'bge'
) -> pd.DataFrame:
    """íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ - ê°„ì†Œí™”"""
    
    # ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {input_path}")
    
    try:
        df = pd.read_excel(input_path) if input_path.endswith('.xlsx') else pd.read_csv(input_path)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['ë¬¸ì¥ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        error_msg = f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}"
        print(f"âŒ {error_msg}")
        raise ValueError(error_msg)
    
    # ë°ì´í„° ì¤€ë¹„
    sentence_data_list = []
    for idx, row in df.iterrows():
        src_text = str(row['ì›ë¬¸']).strip()
        tgt_text = str(row['ë²ˆì—­ë¬¸']).strip()
        
        if src_text and tgt_text:
            sentence_data_list.append({
                'sentence_id': row['ë¬¸ì¥ì‹ë³„ì'],
                'src_text': src_text,
                'tgt_text': tgt_text
            })
    
    total_sentences = len(sentence_data_list)
    print(f"ğŸ“Š ì²˜ë¦¬í•  ë¬¸ì¥: {total_sentences}ê°œ")
    
    if total_sentences == 0:
        raise ValueError("ì²˜ë¦¬í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    results = []
    
    if parallel and total_sentences > workers:
        print(f"ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ({workers}ê°œ í”„ë¡œì„¸ìŠ¤)")
        
        # ë¬¸ì¥ë“¤ì„ ë°°ì¹˜ë¡œ ë¶„í• 
        batch_size_per_worker = max(1, total_sentences // workers)
        sentence_batches = []
        
        for i in range(0, total_sentences, batch_size_per_worker):
            batch = sentence_data_list[i:i + batch_size_per_worker]
            sentence_batches.append(batch)
        
        print(f"ğŸ“‹ ë°°ì¹˜ ë¶„í• : {len(sentence_batches)}ê°œ ë°°ì¹˜")
        print(f"ğŸ”§ {embedder_name.upper()} ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # í”„ë¡œì„¸ìŠ¤ í’€ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        with mp.Pool(processes=workers, initializer=init_worker, initargs=(embedder_name,)) as pool:
            try:
                print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¬¸ì¥ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                
                # ê²°ê³¼ ìˆ˜ì§‘
                with tqdm(total=total_sentences, desc="ë¬¸ì¥ ì²˜ë¦¬", unit="ë¬¸ì¥") as pbar:
                    for i, batch in enumerate(sentence_batches):
                        try:
                            batch_results = pool.apply(process_batch_sentences, (batch,))
                            results.extend(batch_results)
                            
                            pbar.update(len(batch))
                            pbar.set_postfix({"êµ¬": len(results), "ë°°ì¹˜": f"{i+1}/{len(sentence_batches)}"})
                            
                        except Exception as e:
                            pbar.update(len(batch))
                            logger.error(f"ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        
            except KeyboardInterrupt:
                print("ì‚¬ìš©ì ì¤‘ë‹¨")
                pool.terminate()
                pool.join()
                raise
                
    else:
        print(f"ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘")
        print(f"ğŸ”§ {embedder_name.upper()} ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ìˆœì°¨ ì²˜ë¦¬
        if embedder_name == 'openai':
            from common.embedders.openai import compute_embeddings_with_cache
            embed_func = compute_embeddings_with_cache
        else:  # bge ê¸°ë³¸ê°’
            from common.embedders.bge import get_embed_func
            embed_func = get_embed_func()
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¬¸ì¥ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        from sa_tokenizers.jieba_mecab import (
            split_src_meaning_units, 
            split_tgt_by_src_units_semantic
        )
        from punctuation import mask_brackets, restore_brackets
        
        for sentence_data in tqdm(sentence_data_list, desc="ë¬¸ì¥ ì²˜ë¦¬"):
            try:
                sentence_id = sentence_data['sentence_id']
                src_text = sentence_data['src_text']
                tgt_text = sentence_data['tgt_text']
                
                # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                masked_src, src_masks = mask_brackets(src_text, 'source')
                masked_tgt, tgt_masks = mask_brackets(tgt_text, 'target')
                
                src_units = split_src_meaning_units(masked_src)
                tgt_units = split_tgt_by_src_units_semantic(
                    src_units, masked_tgt, embed_func, min_tokens=1
                )
                
                # ê²°ê³¼ ìƒì„±
                for phrase_idx, (src_unit, tgt_unit) in enumerate(zip(src_units, tgt_units), 1):
                    restored_src = restore_brackets(src_unit, src_masks)
                    restored_tgt = restore_brackets(tgt_unit, tgt_masks)
                    
                    results.append({
                        'ë¬¸ì¥ì‹ë³„ì': sentence_id,
                        'êµ¬ì‹ë³„ì': phrase_idx,
                        'ì›ë¬¸': restored_src,
                        'ë²ˆì—­ë¬¸': restored_tgt
                    })
                
            except Exception as e:
                logger.error(f"ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ìˆœì°¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
    
    # ê²°ê³¼ ì €ì¥
    if not results:
        raise ValueError("ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_path}")
    result_df = pd.DataFrame(results)
    
    try:
        result_df.to_excel(output_path, index=False)
        
        print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼: {total_sentences}ê°œ ë¬¸ì¥ â†’ {len(results)}ê°œ êµ¬")
        print(f"ğŸ“ ì¶œë ¥: {output_path}")
        
        return result_df
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise