{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 1: 의존성 설치 및 환경 확인\n",
    "%pip install numpy sentence-transformers torch scikit-learn pandas\n",
    "\n",
    "# VEC 문장 분할 및 의미 기반 원문 정렬 (최신 요구사항 반영, \\p{Han} 사용)\n",
    "import unicodedata\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. SBERT 모델 및 토크나이저 로딩\n",
    "_model = None\n",
    "_tokenizer = None\n",
    "def get_model_and_tokenizer():\n",
    "    global _model, _tokenizer\n",
    "    if _model is None:\n",
    "        model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "        _model = SentenceTransformer(model_name)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        _model = _model.to(device)\n",
    "        _tokenizer = _model.tokenizer\n",
    "    return _model, _tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a0bda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BGEEmbedder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizerFast\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentencepiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBGEEmbedder\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 파일 경로 설정\u001b[39;00m\n\u001b[0;32m     10\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/junto/Downloads/head-repo/SP/split_p/input_p.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'BGEEmbedder'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Environment Setup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "import sentencepiece as spm\n",
    "import FlagEmbedding\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_path = \"C:/Users/junto/Downloads/head-repo/SP/split_p/input_p.xlsx\"\n",
    "output_path = \"C:/Users/junto/Downloads/head-repo/SP/split_p/output_p.xlsx\"\n",
    "\n",
    "# (Optional) 환경 점검\n",
    "print(\"NumPy\", np.__version__)\n",
    "print(\"Pandas\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b53b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Tokenizer Initialization\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm.model')  # SentencePiece 모델 파일 경로\n",
    "\n",
    "def tokenize(text, method='bert'):\n",
    "    if method == 'bert':\n",
    "        return bert_tokenizer.tokenize(text)\n",
    "    elif method == 'spm':\n",
    "        return sp.encode(text, out_type=str)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tokenization method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Source & Target Texts from Excel\n",
    "# input_p.xlsx에 'src' 및 'tgt' 컬럼이 있다고 가정\n",
    "df_input = pd.read_excel(input_path)\n",
    "src_full = df_input['원문'].astype(str).str.cat(sep=' ')\n",
    "tgt_full = df_input['번역문'].astype(str).str_cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac92266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Sentence Splitting (기존 기준 유지)\n",
    "def split_sentences(text):\n",
    "    sents = re.split(r'(?<=[。？！.!?])\\s+', text)\n",
    "    parts = []\n",
    "    for s in sents:\n",
    "        if len(s) > 150:\n",
    "            parts.extend([s[i:i+150] for i in range(0, len(s), 150)])\n",
    "        else:\n",
    "            parts.append(s)\n",
    "    merged, buffer = [], ''\n",
    "    for seg in parts:\n",
    "        han_count = len(re.findall(r'[\\u4E00-\\u9FFF]', seg))\n",
    "        if han_count <= 3:\n",
    "            buffer += seg\n",
    "        else:\n",
    "            if buffer:\n",
    "                if merged:\n",
    "                    merged[-1] += buffer\n",
    "                buffer = ''\n",
    "            merged.append(seg)\n",
    "    if buffer and merged:\n",
    "        merged[-1] += buffer\n",
    "    return [m.strip() for m in merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate Target Units (Tokenized)\n",
    "tgt_sents = split_sentences(tgt_full)\n",
    "tgt_units = [' '.join(tokenize(s, method='bert')) for s in tgt_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28907d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Initial Source Chunking\n",
    "def chunk_src(src_text, max_chars=200):\n",
    "    sents = split_sentences(src_text)\n",
    "    chunks, buf = [], ''\n",
    "    for s in sents:\n",
    "        if len(buf) + len(s) > max_chars:\n",
    "            chunks.append(buf.strip())\n",
    "            buf = s\n",
    "        else:\n",
    "            buf += s\n",
    "    if buf:\n",
    "        chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "src_chunks = chunk_src(src_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Embedding with BGE-M3\n",
    "embedder = BGEEmbedder(model_name='bge-m3')\n",
    "src_embs = embedder.embed_sentences(src_chunks)\n",
    "tgt_embs = embedder.embed_sentences(tgt_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Reverse Alignment (Greedy Matching)\n",
    "assignments = []\n",
    "for i, t_emb in enumerate(tgt_embs):\n",
    "    sims = np.inner([t_emb], src_embs)[0]\n",
    "    best_j = sims.argmax()\n",
    "    assignments.append((i, best_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Split & Collate Results from assignments\n",
    "from tokenizer import split_src_meaning_units  # pipeline-bge 모듈 활용\n",
    "records = []\n",
    "for tgt_i, src_j in assignments:\n",
    "    segments = split_src_meaning_units(src_chunks[src_j])\n",
    "    records.append({\n",
    "        'tgt_id': tgt_i,\n",
    "        'tgt_text': tgt_sents[tgt_i],\n",
    "        'src_segments': segments\n",
    "    })\n",
    "df_output = pd.DataFrame(records)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Output to Excel\n",
    "df_output.to_excel(output_path, index=False)\n",
    "print(f\"Output saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
