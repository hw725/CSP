{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e515df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python310-custom\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: sentence-transformers in c:\\python310-custom\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: torch in c:\\python310-custom\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python310-custom\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: pandas in c:\\python310-custom\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (0.32.5)\n",
      "Requirement already satisfied: Pillow in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\python310-custom\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\python310-custom\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python310-custom\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\python310-custom\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\python310-custom\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python310-custom\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python310-custom\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python310-custom\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310-custom\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310-custom\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python310-custom\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310-custom\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python310-custom\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\python310-custom\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310-custom\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python310-custom\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310-custom\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310-custom\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310-custom\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 셀 1: 의존성 설치 및 환경 확인\n",
    "%pip install numpy sentence-transformers torch scikit-learn pandas\n",
    "\n",
    "# VEC 문장 분할 및 의미 기반 원문 정렬 (최신 요구사항 반영, \\p{Han} 사용)\n",
    "import unicodedata\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8da11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. SBERT 모델 및 토크나이저 로딩\n",
    "_model = None\n",
    "_tokenizer = None\n",
    "def get_model_and_tokenizer():\n",
    "    global _model, _tokenizer\n",
    "    if _model is None:\n",
    "        model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
    "        _model = SentenceTransformer(model_name)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        _model = _model.to(device)\n",
    "        _tokenizer = _model.tokenizer\n",
    "    return _model, _tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28907d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 기반 원문-번역문 의미 단위 분할 함수 정의\n",
    "\n",
    "def split_original_sentences(text):\n",
    "    \"\"\"\n",
    "    원문을 문장 단위로 분할 (중국어, 한국어, 영어 문장부호)\n",
    "    너무 짧은 구는 제거 (한자 3자 이하)\n",
    "    \"\"\"\n",
    "    parts = re.split(r'(?<=[。？！.!?])\\s*', text)\n",
    "    # 한자 3자 이상만 유지\n",
    "    return [p.strip() for p in parts if len(re.findall(r'\\p{Han}', p)) > 3]\n",
    "\n",
    "def align_and_group(orig_segments, trans_sentences, model):\n",
    "    \"\"\"\n",
    "    원문 세그먼트와 번역문 문장 간 의미 기반 유사도 정렬 및 묶기\n",
    "    \"\"\"\n",
    "    if not orig_segments or not trans_sentences:\n",
    "        return [''.join(orig_segments)]\n",
    "\n",
    "    emb_orig = model.encode(orig_segments, convert_to_tensor=True)\n",
    "    emb_trans = model.encode(trans_sentences, convert_to_tensor=True)\n",
    "    sim_matrix = util.cos_sim(emb_trans, emb_orig)\n",
    "    assignments = [int(row.argmax()) for row in sim_matrix]\n",
    "\n",
    "    grouped = []\n",
    "    if assignments:\n",
    "        current = [orig_segments[assignments[0]]]\n",
    "        last_idx = assignments[0]\n",
    "        for o_idx in assignments[1:]:\n",
    "            if o_idx == last_idx or o_idx == last_idx + 1:\n",
    "                current.append(orig_segments[o_idx])\n",
    "            else:\n",
    "                grouped.append(''.join(current))\n",
    "                current = [orig_segments[o_idx]]\n",
    "            last_idx = o_idx\n",
    "        grouped.append(''.join(current))\n",
    "    else:\n",
    "        grouped = [''.join(orig_segments)]\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# -- 사용 예시 --\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# for pid, orig_text in paragraphs_dict.items():\n",
    "#     segments = split_original_sentences(orig_text)\n",
    "#     trans_sentences = translations_dict[pid]\n",
    "#     result = align_and_group(segments, trans_sentences, model)\n",
    "#     final_split = ' | '.join(result)\n",
    "#     print(pid, final_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb1a0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP 기반 1:N 매핑 지원\n",
    "\n",
    "def align_by_similarity(src_units, tgt_units, model, sim_threshold=0.3):\n",
    "    src_vecs = model.encode(src_units, batch_size=32, convert_to_numpy=True,\n",
    "                            normalize_embeddings=True)\n",
    "    tgt_vecs = model.encode(tgt_units, batch_size=32, convert_to_numpy=True,\n",
    "                            normalize_embeddings=True)\n",
    "    sim = cosine_similarity(src_vecs, tgt_vecs)\n",
    "    n, m = sim.shape\n",
    "\n",
    "    # DP 테이블 초기화\n",
    "    dp = np.full((n + 1, m + 1), -np.inf)\n",
    "    bt = np.zeros((n + 1, m + 1), dtype=int)\n",
    "    dp[0, 0] = 0\n",
    "\n",
    "    # 점수 계산 함수\n",
    "    def score(i, j):\n",
    "        return sim[i, j] if sim[i, j] >= sim_threshold else -1.0\n",
    "\n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            if i < n and j < m:\n",
    "                s = score(i, j)\n",
    "                if dp[i, j] + s > dp[i + 1, j + 1]:\n",
    "                    dp[i + 1, j + 1] = dp[i, j] + s\n",
    "                    bt[i + 1, j + 1] = 1  # 대각선 (정렬)\n",
    "\n",
    "            if i < n and dp[i, j] > dp[i + 1, j]:\n",
    "                dp[i + 1, j] = dp[i, j]\n",
    "                bt[i + 1, j] = 2  # src는 대응 없음\n",
    "\n",
    "            if j < m and dp[i, j] > dp[i, j + 1]:\n",
    "                dp[i, j + 1] = dp[i, j]\n",
    "                bt[i, j + 1] = 3  # tgt는 대응 없음\n",
    "\n",
    "    # 역추적\n",
    "    i, j = n, m\n",
    "    pairs = []\n",
    "    while i > 0 or j > 0:\n",
    "        move = bt[i, j]\n",
    "        if move == 1:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            pairs.append((i, j))\n",
    "        elif move == 2:\n",
    "            i -= 1\n",
    "            pairs.append((i, None))\n",
    "        elif move == 3:\n",
    "            j -= 1\n",
    "            pairs.append((None, j))\n",
    "        else:\n",
    "            break\n",
    "    pairs.reverse()\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed495a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_for_tgt: DP 매핑 결과로 1:N 및 누락 src 포함\n",
    "\n",
    "def split_src_for_tgt(src_text, tgt_sents):\n",
    "    tgt_concat = ' '.join(tgt_sents)\n",
    "    norm_src = unicodedata.normalize('NFKC', src_text)\n",
    "    norm_src = re.sub(r'\\s+', ' ', norm_src.strip())\n",
    "    norm_tgt = unicodedata.normalize('NFKC', tgt_concat)\n",
    "    norm_tgt = re.sub(r'\\s+', ' ', norm_tgt.strip())\n",
    "\n",
    "    src_chunks = [chunk for chunk in re.split(r'(?<=[.!?。！？])\\s+', norm_src) if chunk.strip()]\n",
    "    flat_units = src_chunks\n",
    "    flat_boundaries = []\n",
    "    idx = 0\n",
    "    for unit in flat_units:\n",
    "        start = norm_src.find(unit, idx)\n",
    "        end = start + len(unit)\n",
    "        flat_boundaries.append((start, end))\n",
    "        idx = end\n",
    "\n",
    "    model, _ = get_model_and_tokenizer()\n",
    "    pairs = align_by_similarity(flat_units, tgt_sents, model)\n",
    "\n",
    "    grouped_idx = {}\n",
    "    unmatched_src = []\n",
    "    for src_i, tgt_j in pairs:\n",
    "        # None이 아닌 값만 grouped_idx에 추가\n",
    "        if tgt_j is not None and src_i is not None:\n",
    "            grouped_idx.setdefault(tgt_j, []).append(src_i)\n",
    "        elif src_i is not None:\n",
    "            unmatched_src.append(src_i)\n",
    "\n",
    "    aligned = []\n",
    "    for j in range(len(tgt_sents)):\n",
    "        # None이 아닌 인덱스만 사용\n",
    "        idxs = sorted(i for i in grouped_idx.get(j, []) if i is not None)\n",
    "        if idxs:\n",
    "            st = flat_boundaries[idxs[0]][0]\n",
    "            en = flat_boundaries[idxs[-1]][1]\n",
    "            merged_src = norm_src[st:en]\n",
    "        else:\n",
    "            merged_src = ''\n",
    "        aligned.append((merged_src, tgt_sents[j]))\n",
    "\n",
    "    for src_i in unmatched_src:\n",
    "        if src_i is not None:\n",
    "            st, en = flat_boundaries[src_i]\n",
    "            aligned.append((norm_src[st:en], ''))\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0cb3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 처리\n",
    "\n",
    "def process_text(src_text, tgt_text, para_id):\n",
    "    # split_translated_sentences 대신 직접 분할\n",
    "    tgt_chunks = [chunk for chunk in re.split(r'(?<=[.!?。！？])\\s+', tgt_text) if chunk.strip()]\n",
    "    aligned = split_src_for_tgt(src_text, tgt_chunks)\n",
    "    return [{'문단식별자': para_id, '원문': o, '번역문': t} for o, t in aligned]\n",
    "\n",
    "def main(input_path, output_path):\n",
    "    df = pd.read_excel(input_path)\n",
    "    rows = []\n",
    "    # 컬럼명: '문단식별자', '원문', '번역문'으로 고정\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row['문단식별자']\n",
    "        src = row['원문']\n",
    "        tgt = row['번역문']\n",
    "        rows.extend(process_text(src, tgt, pid))\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    print(f\"✅ Completed and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f4480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed and saved to C:/Users/junto/Downloads/head-repo/private725/PC2024/split_root/output_p.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 실행 예시\n",
    "input_path = \"C:/Users/junto/Downloads/head-repo/private725/PC2024/split_root/input_p.xlsx\"\n",
    "output_path = \"C:/Users/junto/Downloads/head-repo/private725/PC2024/split_root/output_p.xlsx\"\n",
    "main(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
