"""SA: ê³µë°± ê¸°ì¤€ ë¶„í•  + ë¶„ì„ ë„êµ¬ í™œìš© (ìµœì í™” ë²„ì „)"""

import logging
import pandas as pd
from typing import List, Callable, Dict

# ğŸ”§ í•„ìˆ˜ import ì¶”ê°€
try:
    import numpy as np
except ImportError:
    # ğŸ”§ verbose ëª¨ë“œì—ì„œë§Œ ì¶œë ¥ (loggerê°€ ì•„ì§ ì„¤ì •ë˜ê¸° ì „ì´ë¯€ë¡œ ë‚˜ì¤‘ì— ì²˜ë¦¬)
    np = None

try:
    import jieba
    jieba.setLogLevel(logging.WARNING)
except ImportError:
    jieba = None

try:
    import MeCab
except ImportError:
    MeCab = None

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50

# MeCab ì´ˆê¸°í™” (ì•ˆì „í•˜ê²Œ)
mecab = None
try:
    if MeCab:
        mecabrc_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/mecabrc'
        dicdir_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir'
        userdic_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/user.dic'
        mecab = MeCab.Tagger(f'-r {mecabrc_path} -d {dicdir_path} -u {userdic_path}')
        # ğŸ”§ verbose ëª¨ë“œì—ì„œë§Œ ì¶œë ¥
        if logger.isEnabledFor(logging.DEBUG):
            print("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
        logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    if logger.isEnabledFor(logging.DEBUG):
        print(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(text: str, **kwargs) -> List[str]:
    """SA ì›ë¬¸ ë¶„í• : ë¬´ì¡°ê±´ ê³µë°± ë‹¨ìœ„ (ë¶„ì„ì€ ë‚´ë¶€ì ìœ¼ë¡œë§Œ í™œìš©)"""
    
    if not text or not text.strip():
        return []
    
    # ğŸ¯ SA í•µì‹¬: ë¬´ì¡°ê±´ ê³µë°± ë‹¨ìœ„ë¡œ ë¶„í• 
    words = text.split()
    
    # ğŸ“Š ë‚´ë¶€ ë¶„ì„ (ë¶„í• ì—ëŠ” ì˜í–¥ ì•ˆ ì¤Œ, ë¡œê¹…ìš©) - ì•ˆì „í•˜ê²Œ
    try:
        if jieba and logger.isEnabledFor(logging.DEBUG):
            jieba_analysis = list(jieba.cut(text))
            logger.debug(f"jieba ë¶„ì„ (ì°¸ê³ ìš©): {jieba_analysis}")
    except:
        pass
    
    try:
        if mecab and logger.isEnabledFor(logging.DEBUG):
            mecab_analysis = _analyze_with_mecab(text)
            logger.debug(f"MeCab ë¶„ì„ (ì°¸ê³ ìš©): {mecab_analysis[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ
    except:
        pass
    
    logger.debug(f"SA ì›ë¬¸ ê³µë°± ë¶„í• : {len(words)}ê°œ ì–´ì ˆ - {words}")
    return words

def split_tgt_meaning_units_sequential(
    src_text: str, 
    tgt_text: str, 
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None,
    **kwargs
) -> List[str]:
    """SA ë²ˆì—­ë¬¸ ë¶„í• : ì›ë¬¸ ê³µë°± ë‹¨ìœ„ì˜ ì˜ë¯¸ì— ë§ì¶° ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• """
    
    if not tgt_text or not tgt_text.strip():
        return []
    
    # 1. ì›ë¬¸ ê³µë°± ë‹¨ìœ„ ì¶”ì¶œ
    src_units = split_src_meaning_units(src_text)
    target_count = len(src_units)
    
    logger.debug(f"ì›ë¬¸ {target_count}ê°œ ì–´ì ˆì˜ ì˜ë¯¸ì— ë§ì¶° ë²ˆì—­ë¬¸ ë¶„í• ")
    logger.debug(f"ì›ë¬¸ ë‹¨ìœ„ë“¤: {src_units}")
    
    # 2. ì „ê° ì½œë¡  ì˜ˆì™¸ ì²˜ë¦¬ (í•˜ë“œ ê²½ê³„)
    if 'ï¼š' in tgt_text and target_count >= 2:
        colon_result = _handle_colon_split(tgt_text, target_count)
        if colon_result:
            logger.debug(f"ì „ê° ì½œë¡  ë¶„í•  ì ìš©: {colon_result}")
            return colon_result
    
    # 3. ë²ˆì—­ë¬¸ì„ ì›ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ì— ë§ì¶° ì§€ëŠ¥ì  ë¶„í• 
    if target_count == 1:
        return [tgt_text]
    
    # 4. ğŸ¯ ì˜ë¯¸ ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í•  (í•µì‹¬ ë¡œì§)
    try:
        tgt_units = _split_tgt_by_src_meanings(
            src_units, 
            tgt_text, 
            embed_func
        )
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì‹¤íŒ¨, ê· ë“± ë¶„í•  ì ìš©: {e}")
        # í´ë°±: ê· ë“± ë¶„í• 
        tgt_words = tgt_text.split()
        tgt_units = _distribute_words_evenly(tgt_words, target_count)
    
    logger.debug(f"ì˜ë¯¸ ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í•  ì™„ë£Œ: {tgt_units}")
    return tgt_units

def process_single_row(row: pd.Series, row_id: str = None, **kwargs) -> List[Dict]:
    """SA ë‹¨ì¼ í–‰ ì²˜ë¦¬: ë¬´ê²°ì„± ë³´ì¥ + ë¶„ì„ ë„êµ¬ í™œìš©"""
    
    try:
        src_text = str(row.get('ì›ë¬¸', ''))
        tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))
        
        if not src_text.strip() or not tgt_text.strip():
            return []
        
        # ğŸ”§ ì„ë² ë” í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ì™„ì „íˆ ì•ˆì „í•œ ë°©ì‹)
        embed_func = None
        embedder_name = kwargs.get('embedder_name', 'bge')
        
        if embedder_name and embedder_name != 'none':
            try:
                # ğŸ”§ ì•ˆì „í•œ ì„ë² ë” ë¡œë“œ ì‹œë„
                if embedder_name == 'bge':
                    try:
                        from common.embedders.bge import get_embed_func
                        embed_func = get_embed_func()
                        logger.debug(f"BGE ì„ë² ë” ë¡œë“œ ì„±ê³µ")
                    except:
                        try:
                            from common.embedders import get_embed_func
                            embed_func = get_embed_func()
                            logger.debug(f"ì¼ë°˜ ì„ë² ë” ë¡œë“œ ì„±ê³µ")
                        except:
                            logger.warning(f"BGE ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨")
                
                elif embedder_name == 'openai':
                    try:
                        from common.embedders.openai import get_embedder
                        embed_func = get_embedder()
                        logger.debug(f"OpenAI ì„ë² ë” ë¡œë“œ ì„±ê³µ")
                    except:
                        try:
                            from common.embedders import get_embedder
                            embed_func = get_embedder('openai')
                            logger.debug(f"OpenAI ì„ë² ë” ë¡œë“œ ì„±ê³µ (ëŒ€ì•ˆ)")
                        except:
                            logger.warning(f"OpenAI ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨")
                
            except Exception as e:
                logger.warning(f"ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ğŸ¯ SA í•µì‹¬ ì²˜ë¦¬
        # 1. ì›ë¬¸ = ë¬´ì¡°ê±´ ê³µë°± ë‹¨ìœ„
        src_units = split_src_meaning_units(src_text)
        
        # 2. ë²ˆì—­ë¬¸ = ì›ë¬¸ì— ë§ì¶° ì˜ë¯¸ì  ë¶„í•  (ë¶„ì„ ë„êµ¬ í™œìš©)
        tgt_units = split_tgt_meaning_units_sequential(
            src_text, 
            tgt_text, 
            embed_func=embed_func,
            **kwargs
        )
        
        # 3. ê°œìˆ˜ ì¼ì¹˜ ë³´ì¥
        max_units = max(len(src_units), len(tgt_units))
        
        while len(src_units) < max_units:
            src_units.append('')
        while len(tgt_units) < max_units:
            tgt_units.append('')
        
        # 4. ê²°ê³¼ ìƒì„± - êµ¬ì‹ë³„ì í¬í•¨í•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
        results = []
        
        # row_idì—ì„œ ë¬¸ì¥ì‹ë³„ì ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        try:
            if row_id and '_' in row_id:
                # file_14bfb2de_chunk_0_row_1 -> 1 ì¶”ì¶œ ì‹œë„
                parts = row_id.split('_')
                # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ìˆ«ìì¸ì§€ í™•ì¸
                if parts[-1].isdigit():
                    sentence_id = int(parts[-1]) + 1  # 0-basedë¥¼ 1-basedë¡œ ë³€í™˜
                else:
                    sentence_id = getattr(row, 'name', 0) + 1
            else:
                sentence_id = getattr(row, 'name', 0) + 1
        except (ValueError, AttributeError):
            sentence_id = getattr(row, 'name', 0) + 1
        
        for i in range(max_units):
            src_unit = src_units[i]
            tgt_unit = tgt_units[i]
            
            if not src_unit.strip() and not tgt_unit.strip():
                continue
            
            result = {
                'ë¬¸ì¥ì‹ë³„ì': sentence_id,  # ğŸ”§ ì•ˆì „í•œ ì •ìˆ˜ ì¶”ì¶œ
                'êµ¬ì‹ë³„ì': i + 1,  # ğŸ”§ êµ¬ì‹ë³„ì ì»¬ëŸ¼ ì¶”ê°€
                'ì›ë¬¸': src_unit,
                'ë²ˆì—­ë¬¸': tgt_unit
            }
            results.append(result)
        
        logger.debug(f"SA í–‰ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ë‹¨ìœ„")
        return results
        
    except Exception as e:
        logger.error(f"SA í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì˜¤ë¥˜ ì‹œì—ë„ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ì•ˆì „í•˜ê²Œ)
        try:
            if row_id and '_' in row_id:
                parts = row_id.split('_')
                if parts[-1].isdigit():
                    sentence_id = int(parts[-1]) + 1
                else:
                    sentence_id = getattr(row, 'name', 0) + 1
            else:
                sentence_id = getattr(row, 'name', 0) + 1
        except (ValueError, AttributeError):
            sentence_id = getattr(row, 'name', 0) + 1
        
        return [{
            'ë¬¸ì¥ì‹ë³„ì': sentence_id,  # ğŸ”§ ì•ˆì „í•œ ì •ìˆ˜ ì¶”ì¶œ
            'êµ¬ì‹ë³„ì': 1,  # ğŸ”§ êµ¬ì‹ë³„ì ì»¬ëŸ¼ ì¶”ê°€
            'ì›ë¬¸': str(row.get('ì›ë¬¸', '')),
            'ë²ˆì—­ë¬¸': str(row.get('ë²ˆì—­ë¬¸', '')),
        }]

def tokenize_text(text):
    """í…ìŠ¤íŠ¸ í† í°í™”"""
    if not text or not text.strip():
        return []
    return text.split()

# ğŸ”„ í˜¸í™˜ì„± í•¨ìˆ˜ë“¤ (ì•ˆì „í•œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
def split_tgt_by_src_units_semantic(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable = None, 
    **kwargs
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  - ì˜ë¯¸ ê¸°ë°˜ (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    
    logger.debug(f"í˜¸í™˜ì„± í•¨ìˆ˜ split_tgt_by_src_units_semantic í˜¸ì¶œë¨")
    
    try:
        # ìƒˆ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        src_text = ' '.join(src_units) if src_units else ''
        return split_tgt_meaning_units_sequential(
            src_text, 
            tgt_text, 
            embed_func=embed_func,
            **kwargs
        )
    except Exception as e:
        logger.error(f"í˜¸í™˜ì„± í•¨ìˆ˜ ì˜¤ë¥˜: {e}")
        # í´ë°±: ê· ë“± ë¶„í• 
        if not tgt_text:
            return [''] * len(src_units)
        words = tgt_text.split()
        return _distribute_words_evenly(words, len(src_units))

def split_tgt_by_src_units(src_units: List[str], tgt_text: str, **kwargs) -> List[str]:
    """í˜¸í™˜ì„± í•¨ìˆ˜ - ì´ì „ ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±"""
    logger.debug("í˜¸í™˜ì„± í•¨ìˆ˜ split_tgt_by_src_units í˜¸ì¶œë¨")
    return split_tgt_by_src_units_semantic(src_units, tgt_text, **kwargs)

def split_tgt_meaning_units(
    tgt_text: str, 
    src_text: str = '', 
    **kwargs
) -> List[str]:
    """ë²ˆì—­ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (í˜¸í™˜ì„± í•¨ìˆ˜)"""
    logger.debug("í˜¸í™˜ì„± í•¨ìˆ˜ split_tgt_meaning_units í˜¸ì¶œë¨")
    return split_tgt_meaning_units_sequential(src_text, tgt_text, **kwargs)

# ===== ë‚´ë¶€ í•¨ìˆ˜ë“¤ (ì•ˆì „í•˜ê²Œ êµ¬í˜„) =====

def _split_tgt_by_src_meanings(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable = None
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì˜ ì˜ë¯¸ì— ë§ì¶° ë²ˆì—­ë¬¸ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í•  - í†µí•© ë¶„ì„"""
    
    target_count = len(src_units)
    tgt_text = tgt_text.strip()
    tgt_words = tgt_text.split()
    
    if len(tgt_words) <= target_count:
        # ë²ˆì—­ë¬¸ ì–´ì ˆì´ ì ìœ¼ë©´ íŒ¨ë”©
        return tgt_words + [''] * (target_count - len(tgt_words))
    
    logger.debug(f"í†µí•© ë¶„ì„ìœ¼ë¡œ {len(tgt_words)}ê°œ ì–´ì ˆì„ {target_count}ê°œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• ")
    
    try:
        # ğŸ¯ ëª¨ë“  ë„êµ¬ë¥¼ ë™ì‹œì— ì‚¬ìš©í•œ ì¢…í•© ë¶„ì„
        analysis_results = _comprehensive_analysis(
            src_units, tgt_words, embed_func
        )
        
        # ğŸ“Š ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì  ê²½ê³„ ê²°ì •
        optimal_boundaries = _determine_optimal_boundaries(
            analysis_results, target_count - 1, len(tgt_words)
        )
        
        # âœ‚ï¸ ê²°ì •ëœ ê²½ê³„ë¡œ ë²ˆì—­ë¬¸ ë¶„í• 
        final_units = _split_by_boundaries(tgt_words, optimal_boundaries)
        
        # ğŸ”§ ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ì¡°ì •
        if len(final_units) != target_count:
            final_units = _adjust_to_target_count(final_units, target_count)
        
        logger.debug(f"í†µí•© ë¶„ì„ ë¶„í•  ê²°ê³¼: {final_units}")
        return final_units
        
    except Exception as e:
        logger.warning(f"ê³ ê¸‰ ë¶„í•  ì‹¤íŒ¨, ê· ë“± ë¶„í•  ì ìš©: {e}")
        return _distribute_words_evenly(tgt_words, target_count)

def _comprehensive_analysis(
    src_units: List[str], 
    tgt_words: List[str], 
    embed_func: Callable = None
) -> Dict:
    """ì„ë² ë” + jieba + MeCab í†µí•© ë¶„ì„ (ì•ˆì „ ë²„ì „)"""
    
    analysis = {
        'semantic_scores': [],      # ì„ë² ë” ì˜ë¯¸ ìœ ì‚¬ë„
        'grammar_boundaries': [],   # MeCab ë¬¸ë²• ê²½ê³„
        'jieba_boundaries': [],     # jieba ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„
        'combined_scores': []       # ì¢…í•© ì ìˆ˜
    }
    
    # 1ï¸âƒ£ ì„ë² ë” ì˜ë¯¸ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
    if embed_func and np:
        try:
            analysis['semantic_scores'] = _calculate_semantic_alignment(
                src_units, tgt_words, embed_func
            )
            logger.debug(f"ì„ë² ë” ë¶„ì„ ì™„ë£Œ: {len(analysis['semantic_scores'])}ê°œ ì ìˆ˜")
        except Exception as e:
            logger.warning(f"ì„ë² ë” ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis['semantic_scores'] = [0.5] * max(0, len(tgt_words) - 1)
    else:
        analysis['semantic_scores'] = [0.5] * max(0, len(tgt_words) - 1)
    
    # 2ï¸âƒ£ MeCab ë¬¸ë²• ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
    if mecab:
        try:
            analysis['grammar_boundaries'] = _analyze_grammar_boundaries(tgt_words)
            logger.debug(f"MeCab ë¶„ì„ ì™„ë£Œ: {len(analysis['grammar_boundaries'])}ê°œ ê²½ê³„")
        except Exception as e:
            logger.warning(f"MeCab ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis['grammar_boundaries'] = [0.3] * max(0, len(tgt_words) - 1)
    else:
        analysis['grammar_boundaries'] = [0.3] * max(0, len(tgt_words) - 1)
    
    # 3ï¸âƒ£ jieba ì˜ë¯¸ ë‹¨ìœ„ ë¶„ì„ (ì•ˆì „í•˜ê²Œ)
    if jieba:
        try:
            analysis['jieba_boundaries'] = _analyze_jieba_boundaries(tgt_words)
            logger.debug(f"jieba ë¶„ì„ ì™„ë£Œ: {len(analysis['jieba_boundaries'])}ê°œ ê²½ê³„")
        except Exception as e:
            logger.warning(f"jieba ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis['jieba_boundaries'] = [0.4] * max(0, len(tgt_words) - 1)
    else:
        analysis['jieba_boundaries'] = [0.4] * max(0, len(tgt_words) - 1)
    
    # 4ï¸âƒ£ ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì•ˆì „í•˜ê²Œ)
    try:
        analysis['combined_scores'] = _calculate_combined_scores(
            analysis['semantic_scores'],
            analysis['grammar_boundaries'],
            analysis['jieba_boundaries']
        )
    except Exception as e:
        logger.warning(f"ì¢…í•© ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        analysis['combined_scores'] = [0.4] * max(0, len(tgt_words) - 1)
    
    logger.debug(f"í†µí•© ë¶„ì„ ì™„ë£Œ: ì¢…í•© ì ìˆ˜ {len(analysis['combined_scores'])}ê°œ")
    return analysis

def _calculate_semantic_alignment(
    src_units: List[str], 
    tgt_words: List[str], 
    embed_func: Callable
) -> List[float]:
    """ì„ë² ë”ë¡œ ê° ë²ˆì—­ë¬¸ ì–´ì ˆì´ ì–´ëŠ ì›ë¬¸ ë‹¨ìœ„ì™€ ê°€ì¥ ìœ ì‚¬í•œì§€ ë¶„ì„ (ì•ˆì „ ë²„ì „)"""
    
    if not np:
        return [0.5] * max(0, len(tgt_words) - 1)
    
    try:
        # ì›ë¬¸ ë‹¨ìœ„ë“¤ì˜ ì„ë² ë”©
        src_embeddings = []
        for src_unit in src_units:
            try:
                emb = embed_func([src_unit])
                src_embeddings.append(emb[0] if emb and len(emb) > 0 else np.zeros(768))
            except:
                src_embeddings.append(np.zeros(768))
        
        # ë²ˆì—­ë¬¸ ì–´ì ˆë“¤ì˜ ì„ë² ë”©ê³¼ ì›ë¬¸ê³¼ì˜ ìœ ì‚¬ë„
        word_alignments = []
        
        for word in tgt_words:
            try:
                word_emb = embed_func([word])
                word_emb = word_emb[0] if word_emb and len(word_emb) > 0 else np.zeros(768)
                
                # ê° ì›ë¬¸ ë‹¨ìœ„ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = []
                for src_emb in src_embeddings:
                    sim = np.dot(word_emb, src_emb) / (
                        np.linalg.norm(word_emb) * np.linalg.norm(src_emb) + 1e-8
                    )
                    similarities.append(sim)
                
                # ê°€ì¥ ìœ ì‚¬í•œ ì›ë¬¸ ë‹¨ìœ„ì˜ ì¸ë±ìŠ¤ì™€ ìœ ì‚¬ë„
                best_match_idx = np.argmax(similarities)
                best_similarity = similarities[best_match_idx]
                
                word_alignments.append({
                    'word': word,
                    'best_src_idx': best_match_idx,
                    'similarity': best_similarity,
                    'all_similarities': similarities
                })
                
            except:
                word_alignments.append({
                    'word': word,
                    'best_src_idx': 0,
                    'similarity': 0.5,
                    'all_similarities': [0.5] * len(src_units)
                })
        
        # ê²½ê³„ ì ìˆ˜ ê³„ì‚°: ì¸ì ‘í•œ ì–´ì ˆë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ì›ë¬¸ ë‹¨ìœ„ì— ë§¤ì¹­ë˜ë©´ ë†’ì€ ì ìˆ˜
        boundary_scores = []
        
        for i in range(len(tgt_words) - 1):
            curr_alignment = word_alignments[i]
            next_alignment = word_alignments[i + 1]
            
            if curr_alignment['best_src_idx'] != next_alignment['best_src_idx']:
                # ì„œë¡œ ë‹¤ë¥¸ ì›ë¬¸ ë‹¨ìœ„ì— ë§¤ì¹­ë˜ë©´ ê²½ê³„ ê°€ëŠ¥ì„± ë†’ìŒ
                score = (curr_alignment['similarity'] + next_alignment['similarity']) / 2
                boundary_scores.append(max(0.1, min(0.9, score)))
            else:
                # ê°™ì€ ì›ë¬¸ ë‹¨ìœ„ì— ë§¤ì¹­ë˜ë©´ ê²½ê³„ ê°€ëŠ¥ì„± ë‚®ìŒ
                boundary_scores.append(0.1)
        
        return boundary_scores
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ì •ë ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return [0.5] * max(0, len(tgt_words) - 1)

def _analyze_grammar_boundaries(tgt_words: List[str]) -> List[float]:
    """MeCabìœ¼ë¡œ ë¬¸ë²•ì  ê²½ê³„ ê°•ë„ ë¶„ì„ (ì•ˆì „ ë²„ì „)"""
    
    boundary_scores = []
    
    for i in range(len(tgt_words) - 1):
        try:
            curr_word = tgt_words[i]
            next_word = tgt_words[i + 1]
            
            # í˜„ì¬ ì–´ì ˆì˜ ë§ˆì§€ë§‰ í˜•íƒœì†Œ ë¶„ì„
            curr_result = mecab.parse(curr_word)
            curr_pos = None
            
            for line in curr_result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        curr_pos = parts[1].split(',')[0]
                        break
            
            # ë‹¤ìŒ ì–´ì ˆì˜ ì²« í˜•íƒœì†Œ ë¶„ì„
            next_result = mecab.parse(next_word)
            next_pos = None
            
            for line in next_result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        next_pos = parts[1].split(',')[0]
                        break
            
            # ê²½ê³„ ê°•ë„ ê³„ì‚°
            score = _calculate_grammar_boundary_strength(curr_pos, next_pos, curr_word)
            boundary_scores.append(score)
            
        except:
            boundary_scores.append(0.3)  # ê¸°ë³¸ê°’
    
    return boundary_scores

def _calculate_grammar_boundary_strength(curr_pos: str, next_pos: str, curr_word: str) -> float:
    """ë¬¸ë²•ì  ê²½ê³„ ê°•ë„ ê³„ì‚° (ì•ˆì „ ë²„ì „)"""
    
    try:
        # ê°•í•œ ê²½ê³„ (ì ˆ ê²½ê³„, ë¬¸ì¥ ê²½ê³„)
        if curr_pos in ['EF', 'EC']:  # ì¢…ê²°ì–´ë¯¸, ì—°ê²°ì–´ë¯¸
            return 0.9
        
        if curr_pos in ['JX', 'JC']:  # ë³´ì¡°ì‚¬, ì ‘ì†ì¡°ì‚¬
            return 0.8
        
        # ì¤‘ê°„ ê°•ë„ ê²½ê³„ (í’ˆì‚¬ ë³€í™”)
        if curr_pos and next_pos and curr_pos != next_pos:
            # ëª…ì‚¬ -> ë™ì‚¬, ë™ì‚¬ -> ëª…ì‚¬ ë“±ì˜ í’ˆì‚¬ ë³€í™”
            if (curr_pos.startswith('N') and next_pos.startswith('V')) or \
               (curr_pos.startswith('V') and next_pos.startswith('N')):
                return 0.6
            
            # ê¸°íƒ€ í’ˆì‚¬ ë³€í™”
            return 0.4
        
        # ì•½í•œ ê²½ê³„ ë˜ëŠ” ê²½ê³„ ì—†ìŒ
        return 0.2
        
    except:
        return 0.3

def _analyze_jieba_boundaries(tgt_words: List[str]) -> List[float]:
    """jiebaë¡œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ë¶„ì„ (ì•ˆì „ ë²„ì „)"""
    
    try:
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ jiebaë¡œ ë¶„í• 
        full_text = ' '.join(tgt_words)
        jieba_units = list(jieba.cut(full_text))
        jieba_units = [unit.strip() for unit in jieba_units if unit.strip()]
        
        # jieba ë¶„í•  ê²°ê³¼ë¥¼ ì–´ì ˆ ë‹¨ìœ„ë¡œ ë§¤í•‘
        boundary_scores = [0.4] * max(0, len(tgt_words) - 1)
        
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: jieba ë‹¨ìœ„ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        jieba_ratio = len(jieba_units) / len(tgt_words) if tgt_words else 1
        
        if jieba_ratio > 1.5:  # jiebaê°€ ë” ì„¸ë¶„í™”í–ˆìŒ
            for i in range(len(boundary_scores)):
                boundary_scores[i] = min(0.8, boundary_scores[i] + 0.2)
        elif jieba_ratio < 0.7:  # jiebaê°€ ëœ ì„¸ë¶„í™”í–ˆìŒ
            for i in range(len(boundary_scores)):
                boundary_scores[i] = max(0.1, boundary_scores[i] - 0.2)
        
        return boundary_scores
        
    except Exception as e:
        logger.warning(f"jieba ê²½ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return [0.4] * max(0, len(tgt_words) - 1)

def _calculate_combined_scores(
    semantic_scores: List[float],
    grammar_scores: List[float], 
    jieba_scores: List[float]
) -> List[float]:
    """ì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê²°í•© (ì•ˆì „ ë²„ì „)"""
    
    try:
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        semantic_weight = 0.5    # ì„ë² ë” ì˜ë¯¸ ë¶„ì„ 50%
        grammar_weight = 0.3     # MeCab ë¬¸ë²• ë¶„ì„ 30%
        jieba_weight = 0.2       # jieba ì˜ë¯¸ ë‹¨ìœ„ 20%
        
        combined = []
        max_len = max(len(semantic_scores), len(grammar_scores), len(jieba_scores))
        
        for i in range(max_len):
            semantic = semantic_scores[i] if i < len(semantic_scores) else 0.5
            grammar = grammar_scores[i] if i < len(grammar_scores) else 0.3
            jieba = jieba_scores[i] if i < len(jieba_scores) else 0.4
            
            combined_score = (
                semantic * semantic_weight + 
                grammar * grammar_weight + 
                jieba * jieba_weight
            )
            
            # ì ìˆ˜ ë²”ìœ„ ì œí•œ
            combined_score = max(0.0, min(1.0, combined_score))
            combined.append(combined_score)
        
        return combined
        
    except Exception as e:
        logger.warning(f"ì¢…í•© ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return [0.4] * max(0, len(semantic_scores))

def _determine_optimal_boundaries(
    analysis_results: Dict, 
    needed_boundaries: int, 
    total_words: int
) -> List[int]:
    """í†µí•© ë¶„ì„ ê²°ê³¼ë¡œ ìµœì  ê²½ê³„ ìœ„ì¹˜ ê²°ì • (ì•ˆì „ ë²„ì „)"""
    
    try:
        combined_scores = analysis_results.get('combined_scores', [])
        
        if needed_boundaries <= 0:
            return []
        
        if not combined_scores or len(combined_scores) < needed_boundaries:
            # ì ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ê· ë“± ë¶„í• 
            if total_words <= needed_boundaries + 1:
                return list(range(1, min(needed_boundaries + 1, total_words)))
            step = total_words // (needed_boundaries + 1)
            return [i * step for i in range(1, needed_boundaries + 1) if i * step < total_words]
        
        # ì ìˆ˜ê°€ ë†’ì€ ìœ„ì¹˜ë“¤ì„ ê²½ê³„ë¡œ ì„ íƒ
        scored_positions = [(score, i + 1) for i, score in enumerate(combined_scores) if i + 1 < total_words]
        scored_positions.sort(reverse=True)  # ë†’ì€ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        
        # ìƒìœ„ ì ìˆ˜ì˜ ìœ„ì¹˜ë“¤ ì„ íƒ (ë„ˆë¬´ ê°€ê¹Œìš´ ê²½ê³„ ì œì™¸)
        selected_boundaries = []
        min_distance = max(1, total_words // (needed_boundaries + 2))  # ìµœì†Œ ê±°ë¦¬
        
        for score, position in scored_positions:
            if len(selected_boundaries) >= needed_boundaries:
                break
            
            # ê¸°ì¡´ ì„ íƒëœ ê²½ê³„ë“¤ê³¼ ìµœì†Œ ê±°ë¦¬ í™•ì¸
            too_close = False
            for existing_pos in selected_boundaries:
                if abs(position - existing_pos) < min_distance:
                    too_close = True
                    break
            
            if not too_close and 0 < position < total_words:
                selected_boundaries.append(position)
        
        # ë¶€ì¡±í•˜ë©´ ê· ë“± ë¶„í• ë¡œ ë³´ì™„
        while len(selected_boundaries) < needed_boundaries:
            # ê°€ì¥ í° ê°„ê²© ì°¾ê¸°
            gaps = []
            all_positions = sorted(selected_boundaries + [0, total_words])
            
            for i in range(len(all_positions) - 1):
                gap_size = all_positions[i + 1] - all_positions[i]
                gap_mid = all_positions[i] + gap_size // 2
                if gap_size > 2 and gap_mid not in selected_boundaries and 0 < gap_mid < total_words:
                    gaps.append((gap_size, gap_mid))
            
            if gaps:
                gaps.sort(reverse=True)
                new_pos = gaps[0][1]
                selected_boundaries.append(new_pos)
            else:
                break
        
        return sorted(selected_boundaries[:needed_boundaries])
        
    except Exception as e:
        logger.warning(f"ê²½ê³„ ê²°ì • ì‹¤íŒ¨, ê· ë“± ë¶„í•  ì ìš©: {e}")
        if total_words <= needed_boundaries + 1:
            return list(range(1, min(needed_boundaries + 1, total_words)))
        step = total_words // (needed_boundaries + 1)
        return [i * step for i in range(1, needed_boundaries + 1) if i * step < total_words]

def _split_by_boundaries(words: List[str], boundaries: List[int]) -> List[str]:
    """ê²°ì •ëœ ê²½ê³„ë¡œ ì–´ì ˆë“¤ì„ ë¶„í•  (ì•ˆì „ ë²„ì „)"""
    
    try:
        if not boundaries or not words:
            return [' '.join(words)] if words else ['']
        
        result = []
        start = 0
        
        for boundary in sorted(boundaries):
            if boundary > start and boundary <= len(words):
                segment = ' '.join(words[start:boundary])
                result.append(segment if segment else '')
                start = boundary
        
        # ë§ˆì§€ë§‰ ë¶€ë¶„
        if start < len(words):
            segment = ' '.join(words[start:])
            result.append(segment if segment else '')
        
        return result
        
    except Exception as e:
        logger.warning(f"ê²½ê³„ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [' '.join(words)] if words else ['']

def _adjust_to_target_count(units: List[str], target_count: int) -> List[str]:
    """ê²°ê³¼ë¥¼ ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ì¡°ì • (ì•ˆì „ ë²„ì „)"""
    
    try:
        if not units:
            return [''] * target_count
        
        if len(units) == target_count:
            return units
        elif len(units) < target_count:
            # ë¶€ì¡±í•˜ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ íŒ¨ë”©
            return units + [''] * (target_count - len(units))
        else:
            # ì´ˆê³¼í•˜ë©´ ë§ˆì§€ë§‰ ë‹¨ìœ„ë“¤ì„ ë³‘í•©
            if target_count <= 0:
                return [' '.join(units)]
            result = units[:target_count-1]
            merged_last = ' '.join(units[target_count-1:])
            result.append(merged_last)
            return result
            
    except Exception as e:
        logger.warning(f"ê°œìˆ˜ ì¡°ì • ì‹¤íŒ¨: {e}")
        return [''] * target_count

def _analyze_with_mecab(text: str) -> List[tuple]:
    """MeCab ë¶„ì„ (ë‚´ë¶€ìš©, ì•ˆì „ ë²„ì „)"""
    
    if not mecab or not text:
        return []
    
    try:
        result = mecab.parse(text)
        analysis = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    pos = features[0] if features else 'UNKNOWN'
                    analysis.append((surface, pos))
        return analysis
    except:
        return []

def _handle_colon_split(text: str, target_count: int) -> List[str]:
    """ì „ê° ì½œë¡  ì²˜ë¦¬ (ì•ˆì „ ë²„ì „)"""
    
    try:
        if 'ï¼š' not in text:
            return None
            
        parts = text.split('ï¼š')
        if len(parts) != 2:
            return None
        
        left_part = parts[0].strip()
        right_part = parts[1].strip()
        
        if not left_part and not right_part:
            return None
            
        left_part = left_part + 'ï¼š'
        
        if target_count == 2:
            return [left_part, right_part]
        elif target_count > 2:
            # ì˜¤ë¥¸ìª½ì„ ì¶”ê°€ ë¶„í• 
            right_words = right_part.split()
            remaining_count = target_count - 1
            
            if len(right_words) <= remaining_count:
                result = [left_part] + right_words
                result.extend([''] * (target_count - len(result)))
                return result
            else:
                # ì˜¤ë¥¸ìª½ì„ ê· ë“± ë¶„í• 
                right_splits = _distribute_words_evenly(right_words, remaining_count)
                return [left_part] + right_splits
        
        return None
        
    except Exception as e:
        logger.warning(f"ì½œë¡  ë¶„í•  ì‹¤íŒ¨: {e}")
        return None

def _distribute_words_evenly(words: List[str], target_count: int) -> List[str]:
    """ì–´ì ˆì„ ê· ë“± ë¶„ë°° (ì•ˆì „ ë²„ì „)"""
    
    try:
        if not words:
            return [''] * target_count
            
        if target_count <= 0:
            return [' '.join(words)]
        
        if target_count >= len(words):
            return words + [''] * (target_count - len(words))
        
        words_per_unit = len(words) // target_count
        remainder = len(words) % target_count
        
        result = []
        start_idx = 0
        
        for i in range(target_count):
            current_size = words_per_unit + (1 if i < remainder else 0)
            end_idx = start_idx + current_size
            
            if end_idx > len(words):
                end_idx = len(words)
            
            if start_idx < end_idx:
                result.append(' '.join(words[start_idx:end_idx]))
            else:
                result.append('')
            
            start_idx = end_idx
        
        return result
        
    except Exception as e:
        logger.warning(f"ê· ë“± ë¶„ë°° ì‹¤íŒ¨: {e}")
        return [''] * target_count

def _calculate_simple_similarity(src_text: str, tgt_text: str) -> float:
    """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì•ˆì „ ë²„ì „)"""
    
    try:
        if not src_text or not tgt_text:
            return 0.0
            
        src_text = str(src_text).strip()
        tgt_text = str(tgt_text).strip()
        
        if not src_text or not tgt_text:
            return 0.0
        
        src_tokens = set(src_text.split())
        tgt_tokens = set(tgt_text.split())
        
        if not src_tokens or not tgt_tokens:
            return 0.0
        
        intersection = len(src_tokens & tgt_tokens)
        union = len(src_tokens | tgt_tokens)
        similarity = intersection / union if union > 0 else 0.0
        
        return round(max(0.0, min(1.0, similarity)), 3)
        
    except Exception as e:
        logger.warning(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0