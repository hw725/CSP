"""PA ì „ìš© ì •ë ¬ê¸° - SAì˜ Vice Versa ë°©ì‹ (ê¸°ì¡´ ë¶„í•  ë°©ì‹ ìœ ì§€)"""

import sys
import os
import numpy as np
from pathlib import Path
from typing import List, Dict
import pandas as pd
from tqdm import tqdm
from difflib import SequenceMatcher

# ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(current_dir))

# ë¡œì»¬ ëª¨ë“ˆ import (ê¸°ì¡´ê³¼ ë™ì¼)
from sentence_splitter import split_target_sentences_advanced, split_source_by_whitespace_and_align

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    torch = None
    TORCH_AVAILABLE = False

# ===== ìƒˆë¡œìš´ spaCy ì§€ì› í•¨ìˆ˜ë“¤ ì¶”ê°€ =====
def get_spacy_nlp():
    """spaCy ëª¨ë¸ ì•ˆì „í•˜ê²Œ ë¡œë“œ"""
    try:
        import spacy
        # í•œêµ­ì–´ ëª¨ë¸ ì‹œë„
        try:
            nlp = spacy.load("ko_core_news_sm")
            print("âœ… spaCy í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            return nlp
        except OSError:
            # ì˜ì–´ ëª¨ë¸ í´ë°±
            try:
                nlp = spacy.load("en_core_web_sm")
                print("âš ï¸ í•œêµ­ì–´ ëª¨ë¸ ì—†ìŒ, ì˜ì–´ ëª¨ë¸ ì‚¬ìš©")
                return nlp
            except OSError:
                print("âŒ spaCy ëª¨ë¸ ì—†ìŒ")
                return None
    except ImportError:
        print("âŒ spaCy ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return None

def split_target_sentences_spacy_tokenizer(
    text: str, 
    max_length: int = 150,
    tokenizer_func=None,
    nlp=None
) -> List[str]:
    """
    spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ë¬¸ì¥ ë¶„í• 
    ê¸°ì¡´ split_target_sentences_advancedì˜ ëŒ€ì²´ í•¨ìˆ˜
    """
    if not text.strip():
        return []
    
    sentences = []
    
    # 1ë‹¨ê³„: spaCyë¡œ ë¬¸ì¥ ê²½ê³„ ê°ì§€
    if nlp:
        try:
            doc = nlp(text)
            spacy_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
            
            if spacy_sentences:
                print(f"ğŸ” spaCy ë¶„í• : {len(spacy_sentences)}ê°œ ë¬¸ì¥")
                sentences = spacy_sentences
            else:
                # spaCy ë¶„í•  ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¶„í• 
                sentences = [text]
        except Exception as e:
            print(f"âš ï¸ spaCy ë¶„í•  ì‹¤íŒ¨: {e}")
            sentences = [text]
    else:
        # spaCy ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        sentences = split_target_sentences_advanced(text, max_length, splitter="punctuation")
    
    # 2ë‹¨ê³„: í† í¬ë‚˜ì´ì €ë¡œ ê¸´ ë¬¸ì¥ ì„¸ë¶„í™”
    if tokenizer_func and sentences:
        refined_sentences = []
        
        for sentence in sentences:
            if len(sentence) > max_length:
                # ê¸´ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì €ë¡œ ì„¸ë¶„í™”
                refined_parts = split_long_sentence_with_tokenizer(
                    sentence, max_length, tokenizer_func
                )
                refined_sentences.extend(refined_parts)
            else:
                refined_sentences.append(sentence)
        
        print(f"ğŸ”§ í† í¬ë‚˜ì´ì € ì¡°ì •: {len(sentences)} â†’ {len(refined_sentences)}ê°œ ë¬¸ì¥")
        sentences = refined_sentences
    
    return sentences if sentences else [text]

def split_long_sentence_with_tokenizer(
    sentence: str, 
    max_length: int, 
    tokenizer_func
) -> List[str]:
    """í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ì¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    try:
        tokens = tokenizer_func(sentence)
        if not tokens:
            return [sentence]
        
        parts = []
        current_part = []
        current_length = 0
        
        for token in tokens:
            token_length = len(token)
            
            # í˜„ì¬ íŒŒíŠ¸ê°€ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ìƒˆ íŒŒíŠ¸ ì‹œì‘
            if current_length + token_length > max_length and current_part:
                parts.append(''.join(current_part))
                current_part = [token]
                current_length = token_length
            else:
                current_part.append(token)
                current_length += token_length
        
        # ë§ˆì§€ë§‰ íŒŒíŠ¸ ì¶”ê°€
        if current_part:
            parts.append(''.join(current_part))
        
        return parts if parts else [sentence]
        
    except Exception as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¶„í•  ì‹¤íŒ¨: {e}")
        return [sentence]

def split_src_by_tgt_units_spacy_tokenizer(
    tgt_sentences: List[str], 
    src_text: str, 
    tokenizer_func=None,
    nlp=None
) -> List[str]:
    """
    spaCy + í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•œ Vice Versa ì›ë¬¸ ë¶„í• 
    ê¸°ì¡´ split_src_by_tgt_units_vice_versaì˜ ê°œì„  ë²„ì „
    """
    if not tgt_sentences or not src_text.strip():
        return []
    
    # spaCyë¡œ ì›ë¬¸ êµ¬ì¡° ë¶„ì„
    structure_info = analyze_source_structure_with_spacy(src_text, nlp)
    
    # í† í¬ë‚˜ì´ì €ë¡œ ì›ë¬¸ í† í°í™”
    if tokenizer_func:
        try:
            src_tokens = tokenizer_func(src_text)
            if not src_tokens:
                src_tokens = list(src_text)
        except Exception as e:
            print(f"âš ï¸ í† í¬ë‚˜ì´ì € ì‹¤íŒ¨: {e}")
            src_tokens = list(src_text)
    else:
        src_tokens = list(src_text)
    
    if not src_tokens:
        return ['' for _ in tgt_sentences]
    
    num_tgt = len(tgt_sentences)
    if num_tgt == 1:
        return [''.join(src_tokens)]
    
    # spaCy êµ¬ì¡° ì •ë³´ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë¶„í• 
    if structure_info['entities'] or structure_info['noun_chunks']:
        return smart_split_with_spacy_structure(src_tokens, tgt_sentences, structure_info)
    else:
        # ê¸°ë³¸ ê· ë“± ë¶„í• 
        return simple_equal_split_tokens(src_tokens, num_tgt)

def analyze_source_structure_with_spacy(src_text: str, nlp) -> Dict:
    """spaCyë¡œ ì›ë¬¸ êµ¬ì¡° ë¶„ì„"""
    structure_info = {
        'entities': [],
        'noun_chunks': [],
        'pos_patterns': [],
        'sentence_count': 1
    }
    
    if not nlp:
        return structure_info
    
    try:
        doc = nlp(src_text)
        
        # ê°œì²´ëª… ì¶”ì¶œ
        structure_info['entities'] = [(ent.text, ent.label_) for ent in doc.ents]
        
        # ëª…ì‚¬êµ¬ ì¶”ì¶œ
        structure_info['noun_chunks'] = [chunk.text for chunk in doc.noun_chunks]
        
        # í’ˆì‚¬ íŒ¨í„´ ì¶”ì¶œ
        structure_info['pos_patterns'] = [token.pos_ for token in doc]
        
        # ë¬¸ì¥ ìˆ˜
        structure_info['sentence_count'] = len(list(doc.sents))
        
        if structure_info['entities'] or structure_info['noun_chunks']:
            print(f"ğŸ“Š spaCy êµ¬ì¡° ë¶„ì„: ê°œì²´ëª… {len(structure_info['entities'])}ê°œ, ëª…ì‚¬êµ¬ {len(structure_info['noun_chunks'])}ê°œ")
        
    except Exception as e:
        print(f"âš ï¸ spaCy êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    return structure_info

def smart_split_with_spacy_structure(
    src_tokens: List[str], 
    tgt_sentences: List[str], 
    structure_info: Dict
) -> List[str]:
    """spaCy êµ¬ì¡° ì •ë³´ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë¶„í• """
    
    src_text = ''.join(src_tokens)
    num_tgt = len(tgt_sentences)
    
    # ê°œì²´ëª…ì´ë‚˜ ëª…ì‚¬êµ¬ ìœ„ì¹˜ë¥¼ ë¶„í•  ê²½ê³„ë¡œ í™œìš©
    split_points = []
    
    # ê°œì²´ëª… ë ìœ„ì¹˜ë“¤ì„ ë¶„í•  í›„ë³´ë¡œ ì¶”ê°€
    for entity_text, _ in structure_info['entities']:
        pos = src_text.find(entity_text)
        if pos != -1:
            split_points.append(pos + len(entity_text))
    
    # ëª…ì‚¬êµ¬ ë ìœ„ì¹˜ë“¤ì„ ë¶„í•  í›„ë³´ë¡œ ì¶”ê°€
    for chunk_text in structure_info['noun_chunks']:
        pos = src_text.find(chunk_text)
        if pos != -1:
            split_points.append(pos + len(chunk_text))
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    split_points = sorted(set(split_points))
    
    if len(split_points) >= num_tgt - 1:
        # ì¶©ë¶„í•œ ë¶„í• ì ì´ ìˆìœ¼ë©´ í™œìš©
        selected_points = split_points[:num_tgt-1]
        
        chunks = []
        start = 0
        for point in selected_points:
            chunks.append(src_text[start:point])
            start = point
        chunks.append(src_text[start:])  # ë§ˆì§€ë§‰ ì²­í¬
        
        print(f"ğŸ¯ spaCy êµ¬ì¡° ê¸°ë°˜ ë¶„í• : {len(chunks)}ê°œ ì²­í¬")
        return chunks
    else:
        # ë¶„í• ì ì´ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ë¶„í• 
        return simple_equal_split_tokens(src_tokens, num_tgt)

def simple_equal_split_tokens(src_tokens: List[str], num_chunks: int) -> List[str]:
    """ê¸°ë³¸ ê· ë“± ë¶„í•  (í† í° ê¸°ë°˜)"""
    tokens_per_chunk = len(src_tokens) // num_chunks
    remainder = len(src_tokens) % num_chunks
    
    chunks = []
    start_idx = 0
    
    for i in range(num_chunks):
        current_size = tokens_per_chunk + (1 if i < remainder else 0)
        end_idx = start_idx + current_size
        
        if end_idx > len(src_tokens):
            end_idx = len(src_tokens)
        
        if start_idx < len(src_tokens):
            chunk = ''.join(src_tokens[start_idx:end_idx])
            chunks.append(chunk)
        else:
            chunks.append('')
        
        start_idx = end_idx
    
    return chunks
# ===== ìƒˆë¡œìš´ spaCy ì§€ì› í•¨ìˆ˜ë“¤ ë =====

def get_tokenizer_function(tokenizer_name: str = "jieba"):
    """í† í¬ë‚˜ì´ì € í•¨ìˆ˜ ë°˜í™˜ - SA ì¬ì‚¬ìš©"""
    try:
        if tokenizer_name == "jieba":
            sys.path.insert(0, str(project_root / 'sa' / 'sa_tokenizers'))
            from jieba_mecab import tokenize_chinese_text
            print("âœ… jieba í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            return tokenize_chinese_text
        elif tokenizer_name == "mecab":
            sys.path.insert(0, str(project_root / 'sa' / 'sa_tokenizers'))
            from jieba_mecab import tokenize_korean_text
            print("âœ… mecab í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            return tokenize_korean_text
        else:
            print(f"âš ï¸ ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {tokenizer_name}")
            return lambda text: list(text)
    except ImportError as e:
        print(f"âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")
        return lambda text: list(text)

def get_embedder_function(embedder_name: str, device: str = "cpu", openai_model: str = None, openai_api_key: str = None):
    """ì„ë² ë” í•¨ìˆ˜ ë°˜í™˜ - ê¸°ì¡´ê³¼ ë™ì¼"""
    
    # ë””ë°”ì´ìŠ¤ í™•ì¸
    if device == "cuda":
        if not TORCH_AVAILABLE or not torch.cuda.is_available():
            print("âš ï¸ CUDA ë¯¸ì§€ì›: CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            device = "cpu"
    
    if embedder_name == 'bge':
        try:
            # common ëª¨ë“ˆì—ì„œ BGE ì„ë² ë” ê°€ì ¸ì˜¤ê¸°
            sys.path.insert(0, str(project_root / 'common' / 'embedders'))
            from bge import get_embed_func
            embed_func = get_embed_func(device_id=0 if device == "cuda" else None)
            if embed_func is None:
                print("âŒ BGE ì„ë² ë” ì´ˆê¸°í™” ì‹¤íŒ¨")
                return None
            print("âœ… BGE ì„ë² ë” ì´ˆê¸°í™” ì„±ê³µ")
            return embed_func
        except ImportError as e:
            print(f"âŒ BGE ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
            
    elif embedder_name == 'openai':
        try:
            # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            if openai_api_key:
                os.environ["OPENAI_API_KEY"] = openai_api_key
            
            # common ëª¨ë“ˆì—ì„œ OpenAI ì„ë² ë” ê°€ì ¸ì˜¤ê¸°
            sys.path.insert(0, str(project_root / 'common' / 'embedders'))
            from openai import compute_embeddings_with_cache
            
            def embed_func(texts):
                return compute_embeddings_with_cache(
                    texts, 
                    model=openai_model if openai_model else "text-embedding-3-large"
                )
            print("âœ… OpenAI ì„ë² ë” ì´ˆê¸°í™” ì„±ê³µ")
            return embed_func
        except ImportError as e:
            print(f"âŒ OpenAI ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}")
        return None

def split_src_by_tgt_units_vice_versa(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    tokenizer_func=None,
    similarity_threshold: float = 0.3
) -> List[str]:
    """
    SAì˜ Vice Versa: ë²ˆì—­ë¬¸ ë¬¸ì¥ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì›ë¬¸ì„ ë¶„í• 
    """
    if not tgt_sentences or not src_text.strip():
        return []
    
    # ì›ë¬¸ì„ í† í°ìœ¼ë¡œ ë¶„í• 
    if tokenizer_func:
        try:
            src_tokens = tokenizer_func(src_text)
            if not src_tokens:
                src_tokens = list(src_text)
        except Exception as e:
            print(f"âš ï¸ í† í¬ë‚˜ì´ì € ì‹¤íŒ¨: {e}")
            src_tokens = list(src_text)
    else:
        src_tokens = list(src_text)
    
    if not src_tokens:
        return ['' for _ in tgt_sentences]
    
    num_tgt_sentences = len(tgt_sentences)
    
    if num_tgt_sentences == 1:
        return [''.join(src_tokens)]
    
    # ê¸°ë³¸ ê· ë“± ë¶„í• 
    tokens_per_chunk = len(src_tokens) // num_tgt_sentences
    remainder = len(src_tokens) % num_tgt_sentences
    
    src_chunks = []
    start_idx = 0
    
    for i in range(num_tgt_sentences):
        current_size = tokens_per_chunk + (1 if i < remainder else 0)
        end_idx = start_idx + current_size
        
        if end_idx > len(src_tokens):
            end_idx = len(src_tokens)
        
        if start_idx < len(src_tokens):
            chunk_tokens = src_tokens[start_idx:end_idx]
            chunk_text = ''.join(chunk_tokens)
            src_chunks.append(chunk_text)
        else:
            src_chunks.append('')
        
        start_idx = end_idx
    
    # ì„ë² ë”ê°€ ìˆìœ¼ë©´ ì˜ë¯¸ì  ìµœì í™” ì‹œë„
    if embed_func:
        try:
            optimized_chunks = optimize_alignment_with_embedder(
                src_chunks, tgt_sentences, embed_func, similarity_threshold
            )
            src_chunks = optimized_chunks
        except Exception as e:
            print(f"âš ï¸ ì˜ë¯¸ì  ìµœì í™” ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ë³´ì •
    while len(src_chunks) < len(tgt_sentences):
        src_chunks.append('')
    
    return src_chunks[:len(tgt_sentences)]

def optimize_alignment_with_embedder(
    src_chunks: List[str], 
    tgt_sentences: List[str], 
    embed_func,
    similarity_threshold: float
) -> List[str]:
    """ì„ë² ë”ë¥¼ ì‚¬ìš©í•œ ì •ë ¬ ìµœì í™”"""
    optimized_chunks = []
    
    for i, (src_chunk, tgt_sentence) in enumerate(zip(src_chunks, tgt_sentences)):
        if not src_chunk.strip() or not tgt_sentence.strip():
            optimized_chunks.append(src_chunk)
            continue
        
        # í˜„ì¬ ë§¤ì¹­ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        current_similarity = compute_similarity(src_chunk, tgt_sentence, embed_func)
        
        best_chunk = src_chunk
        best_similarity = current_similarity
        
        # ì´ì „ ì²­í¬ì™€ í•©ì¹˜ê¸° ì‹œë„
        if i > 0 and optimized_chunks:
            extended_chunk = optimized_chunks[-1] + src_chunk
            extended_similarity = compute_similarity(extended_chunk, tgt_sentence, embed_func)
            
            if extended_similarity > best_similarity + 0.1:  # ì„ê³„ê°’
                # ì´ì „ ì²­í¬ë¥¼ ë¹„ìš°ê³  í˜„ì¬ ì²­í¬ë¥¼ í™•ì¥
                optimized_chunks[-1] = ''
                best_chunk = extended_chunk
                best_similarity = extended_similarity
        
        optimized_chunks.append(best_chunk)
    
    return optimized_chunks

def compute_similarity(text1: str, text2: str, embed_func) -> float:
    """ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        if not text1.strip() or not text2.strip():
            return 0.0
        
        embeddings = embed_func([text1, text2])
        if len(embeddings) != 2:
            return 0.0
        
        emb1 = np.array(embeddings[0])
        emb2 = np.array(embeddings[1])
        
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(np.dot(emb1, emb2) / (norm1 * norm2))
    except Exception as e:
        print(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return compute_similarity_simple(text1, text2)

def compute_similarity_simple(text1: str, text2: str) -> float:
    """ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜ ìœ ì‚¬ë„"""
    if not text1.strip() or not text2.strip():
        return 0.0
    
    len1, len2 = len(text1), len(text2)
    if len1 == 0 or len2 == 0:
        return 0.0
    
    ratio = min(len1, len2) / max(len1, len2)
    return 0.5 + (ratio * 0.5)

def improved_align_paragraphs_vice_versa(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    tokenizer_func=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """
    Vice Versa ë°©ì‹: ë²ˆì—­ë¬¸ ë¬¸ì¥ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì›ë¬¸ì„ ë¶„í• í•˜ì—¬ ì •ë ¬
    (ê¸°ì¡´ ìˆœì°¨ì  ì •ë ¬ê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
    """
    if not tgt_sentences:
        return []
    
    # Vice Versa: ë²ˆì—­ë¬¸ ë¬¸ì¥ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì›ë¬¸ì„ ë¶„í• 
    aligned_src_chunks = split_src_by_tgt_units_vice_versa(
        tgt_sentences, 
        src_text, 
        embed_func,
        tokenizer_func,
        similarity_threshold
    )
    
    alignments = []
    for i in range(len(tgt_sentences)):
        src_chunk = aligned_src_chunks[i] if i < len(aligned_src_chunks) else ''
        tgt_sentence = tgt_sentences[i]
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        if embed_func:
            try:
                similarity = compute_similarity(src_chunk, tgt_sentence, embed_func)
            except:
                similarity = compute_similarity_simple(src_chunk, tgt_sentence)
        else:
            similarity = compute_similarity_simple(src_chunk, tgt_sentence)
        
        alignments.append({
            'ì›ë¬¸': src_chunk,
            'ë²ˆì—­ë¬¸': tgt_sentence,
            'similarity': similarity,
            'split_method': 'vice_versa_tokenized',
            'align_method': 'tgt_based_src_split'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'vice_versa_tokenized',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments

# ===== ìƒˆë¡œìš´ spaCy+í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬ í•¨ìˆ˜ ì¶”ê°€ =====
def improved_align_paragraphs_spacy_tokenizer(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    tokenizer_func=None,
    nlp=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """
    spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬
    ìƒˆë¡œìš´ 4ë²ˆì§¸ ë°©ì‹
    """
    if not tgt_sentences:
        return []
    
    # spaCy + í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•œ ì›ë¬¸ ë¶„í• 
    aligned_src_chunks = split_src_by_tgt_units_spacy_tokenizer(
        tgt_sentences, 
        src_text, 
        tokenizer_func,
        nlp
    )
    
    alignments = []
    for i in range(len(tgt_sentences)):
        src_chunk = aligned_src_chunks[i] if i < len(aligned_src_chunks) else ''
        tgt_sentence = tgt_sentences[i]
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        if embed_func:
            try:
                similarity = compute_similarity(src_chunk, tgt_sentence, embed_func)
            except:
                similarity = compute_similarity_simple(src_chunk, tgt_sentence)
        else:
            similarity = compute_similarity_simple(src_chunk, tgt_sentence)
        
        alignments.append({
            'ì›ë¬¸': src_chunk,
            'ë²ˆì—­ë¬¸': tgt_sentence,
            'similarity': similarity,
            'split_method': 'spacy_tokenizer_fusion',
            'align_method': 'spacy_tokenizer_based_split'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'spacy_tokenizer_fusion',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments
# ===== ìƒˆë¡œìš´ spaCy+í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬ í•¨ìˆ˜ ë =====

def process_paragraph_alignment(
    src_paragraph: str, 
    tgt_paragraph: str, 
    embedder_name: str = 'bge',
    tokenizer_name: str = 'jieba',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8,
    use_spacy_tokenizer: bool = False  # ìƒˆë¡œìš´ ì˜µì…˜ ì¶”ê°€
):
    """
    PA ì²˜ë¦¬: ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € ì •ë ¬ + spaCy í† í¬ë‚˜ì´ì € ìœµí•© ë³‘í•©
    """
    print(f"ğŸ”„ PA ì²˜ë¦¬ ì‹œì‘ (ê¸°ì¡´ + Vice Versa + spaCy í† í¬ë‚˜ì´ì €)")
    
    # 1. ê¸°ì¡´ ìˆœì°¨ì  ì •ë ¬ (punctuation)
    tgt_sentences_seq = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="punctuation")
    alignments_seq = improved_align_paragraphs(
        tgt_sentences_seq, 
        src_paragraph
    )
    
    # 2. ê¸°ì¡´ ì˜ë¯¸ì  ì •ë ¬ (spacy)
    tgt_sentences_sem = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="spacy")
    embed_func = get_embedder_function(embedder_name, device=device)
    alignments_sem = improved_align_paragraphs(
        tgt_sentences_sem,
        src_paragraph,
        embed_func,
        similarity_threshold
    )
    
    # 3. ê¸°ì¡´ Vice Versa í† í¬ë‚˜ì´ì € ì •ë ¬
    tokenizer_func = get_tokenizer_function(tokenizer_name)
    tgt_sentences_tok = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="punctuation")
    alignments_tok = improved_align_paragraphs_vice_versa(
        tgt_sentences_tok,
        src_paragraph,
        embed_func,
        tokenizer_func,
        similarity_threshold
    )
    
    # 4. ìƒˆë¡œìš´ spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ì •ë ¬
    alignments_spacy_tok = []
    if use_spacy_tokenizer:
        nlp = get_spacy_nlp()
        # spaCy + í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ì¥ ë¶„í• 
        tgt_sentences_spacy_tok = split_target_sentences_spacy_tokenizer(
            tgt_paragraph, max_length, tokenizer_func, nlp
        )
        alignments_spacy_tok = improved_align_paragraphs_spacy_tokenizer(
            tgt_sentences_spacy_tok,
            src_paragraph,
            embed_func,
            tokenizer_func,
            nlp,
            similarity_threshold
        )
    
    # 4ê°€ì§€ ë°©ì‹ ì¤‘ ìµœì  ì„ íƒ
    all_alignments = [alignments_seq, alignments_sem, alignments_tok, alignments_spacy_tok]
    max_len = max(len(alignments) for alignments in all_alignments if alignments)
    
    results = []
    for i in range(max_len):
        seq = alignments_seq[i] if i < len(alignments_seq) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'punctuation','align_method':'sequential'}
        sem = alignments_sem[i] if i < len(alignments_sem) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'spacy','align_method':'semantic'}
        tok = alignments_tok[i] if i < len(alignments_tok) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'vice_versa_tokenized','align_method':'tgt_based_src_split'}
        spacy_tok = alignments_spacy_tok[i] if i < len(alignments_spacy_tok) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'spacy_tokenizer_fusion','align_method':'spacy_tokenizer_based_split'}
        
        if use_spacy_tokenizer and alignments_spacy_tok:
            # spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ì‚¬ìš©ì‹œ ê°€ì¤‘ì¹˜ ì¡°ì • (ê¸°ì¡´:ìˆœì°¨0.2+ì˜ë¯¸0.3+í† í¬ë‚˜ì´ì €0.2+spaCyí† í¬ë‚˜ì´ì €0.3)
            weighted_sim = seq['similarity']*0.2 + sem['similarity']*0.3 + tok['similarity']*0.2 + spacy_tok['similarity']*0.3
            
            if weighted_sim >= quality_threshold:
                # spaCy í† í¬ë‚˜ì´ì € ê²°ê³¼ ìš°ì„ 
                result = {
                    'ì›ë¬¸': spacy_tok['ì›ë¬¸'] if spacy_tok['ì›ë¬¸'] else (tok['ì›ë¬¸'] if tok['ì›ë¬¸'] else (sem['ì›ë¬¸'] if sem['ì›ë¬¸'] else seq['ì›ë¬¸'])),
                    'ë²ˆì—­ë¬¸': spacy_tok['ë²ˆì—­ë¬¸'] if spacy_tok['ë²ˆì—­ë¬¸'] else (tok['ë²ˆì—­ë¬¸'] if tok['ë²ˆì—­ë¬¸'] else (sem['ë²ˆì—­ë¬¸'] if sem['ë²ˆì—­ë¬¸'] else seq['ë²ˆì—­ë¬¸'])),
                    'similarity': weighted_sim,
                    'split_method': f"seq+sem+tok+spacy_tok",
                    'align_method': 'hybrid_with_spacy_tokenizer'
                }
            else:
                # spaCy í† í¬ë‚˜ì´ì € ê²°ê³¼ë§Œ ì±„íƒ
                result = spacy_tok.copy()
                result['align_method'] = 'spacy_tokenizer_fusion_only'
        else:
            # ê¸°ì¡´ 3ê°€ì§€ ë°©ì‹ ì‚¬ìš©
            weighted_sim = seq['similarity']*0.3 + sem['similarity']*0.4 + tok['similarity']*0.3
            
            if weighted_sim >= quality_threshold:
                result = {
                    'ì›ë¬¸': tok['ì›ë¬¸'] if tok['ì›ë¬¸'] else (sem['ì›ë¬¸'] if sem['ì›ë¬¸'] else seq['ì›ë¬¸']),
                    'ë²ˆì—­ë¬¸': tok['ë²ˆì—­ë¬¸'] if tok['ë²ˆì—­ë¬¸'] else (sem['ë²ˆì—­ë¬¸'] if sem['ë²ˆì—­ë¬¸'] else seq['ë²ˆì—­ë¬¸']),
                    'similarity': weighted_sim,
                    'split_method': f"seq+sem+tok",
                    'align_method': 'hybrid_with_tokenizer'
                }
            else:
                result = tok.copy()
                result['align_method'] = 'tokenizer_vice_versa_only'
        
        results.append(result)
    
    return results

def improved_align_paragraphs(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """
    ê¸°ì¡´ ìˆœì°¨ì  1:1 ì •ë ¬ (ê³µë°±/í¬ë§· 100% ë³´ì¡´) - ê·¸ëŒ€ë¡œ ìœ ì§€
    """
    if not tgt_sentences:
        return []
    
    # ì›ë¬¸ì„ ë²ˆì—­ë¬¸ ê°œìˆ˜ì— ë§ì¶° ìˆœì°¨ì ìœ¼ë¡œ ë¶„í• 
    aligned_src_chunks = split_source_by_whitespace_and_align(src_text, len(tgt_sentences))
    
    alignments = []
    for i in range(len(tgt_sentences)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[i] if i < len(aligned_src_chunks) else '',
            'ë²ˆì—­ë¬¸': tgt_sentences[i],
            'similarity': 1.0,  # ìˆœì°¨ì  ì •ë ¬ì´ë¯€ë¡œ ìœ ì‚¬ë„ëŠ” 1.0
            'split_method': 'punctuation',
            'align_method': 'sequential'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'punctuation',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments

def process_paragraph_file(
    input_file: str, 
    output_file: str, 
    embedder_name: str = 'bge',
    tokenizer_name: str = 'jieba',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8,
    use_spacy_tokenizer: bool = False,  # ìƒˆë¡œìš´ ì˜µì…˜ ì¶”ê°€
    verbose: bool = False,
    **kwargs
):
    """íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ - ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € + spaCy í† í¬ë‚˜ì´ì € ìœµí•© ì¶”ê°€"""
    print(f"ğŸ“‚ PA íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    if use_spacy_tokenizer:
        print(f"ğŸ”— ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € + spaCy í† í¬ë‚˜ì´ì € ìœµí•©")
    else:
        print(f"ğŸ”„ ê¸°ì¡´ ë°©ì‹ + Vice Versa í† í¬ë‚˜ì´ì € í†µí•©")
    print(f"âš™ï¸  í† í¬ë‚˜ì´ì €: {tokenizer_name}")
    print(f"âš™ï¸  ì„ë² ë”: {embedder_name}")
    print(f"ğŸ”—  spaCy ìœµí•©: {use_spacy_tokenizer}")
    
    try:
        df = pd.read_excel(input_file)
        print(f"ğŸ“„ {len(df)}ê°œ ë¬¸ë‹¨ ë¡œë“œë¨")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None
    
    all_results = []
    total = len(df)
    
    for idx, row in tqdm(df.iterrows(), total=total, desc="ğŸ“Š ë¬¸ë‹¨ ì²˜ë¦¬"):
        src_paragraph = str(row.get('ì›ë¬¸', ''))
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', ''))
        
        if src_paragraph.strip() and tgt_paragraph.strip():
            try:
                alignments = process_paragraph_alignment(
                    src_paragraph,
                    tgt_paragraph,
                    embedder_name=embedder_name,
                    tokenizer_name=tokenizer_name,
                    max_length=max_length,
                    similarity_threshold=similarity_threshold,
                    device=device,
                    quality_threshold=quality_threshold,
                    use_spacy_tokenizer=use_spacy_tokenizer  # ìƒˆë¡œìš´ ì˜µì…˜ ì „ë‹¬
                )
                
                # ë¬¸ë‹¨ì‹ë³„ì ë¶€ì—¬
                for a in alignments:
                    a['ë¬¸ë‹¨ì‹ë³„ì'] = idx + 1
                
                all_results.extend(alignments)
                
            except Exception as e:
                print(f"âŒ ë¬¸ë‹¨ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                if verbose:
                    import traceback
                    traceback.print_exc()
    
    if not all_results:
        print("âŒ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    result_df = pd.DataFrame(all_results)
    final_columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
    result_df = result_df[final_columns]

    # === ë¬´ê²°ì„± ê²€ì¦ ë° ë³´ì™„ (ê¸°ì¡´ê³¼ ë™ì¼) ===
    input_src_all = ''.join([str(row.get('ì›ë¬¸','')) for _, row in df.iterrows()])
    input_tgt_all = ''.join([str(row.get('ë²ˆì—­ë¬¸','')) for _, row in df.iterrows()])
    output_src_all = ''.join(result_df['ì›ë¬¸'].fillna(''))
    output_tgt_all = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna(''))
    
    # ì›ë¬¸ ë³´ì™„
    if input_src_all != output_src_all:
        print('âš ï¸ ì›ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: ëˆ„ë½/ì¤‘ë³µ ë³´ì • ì‹œë„')
        sm = SequenceMatcher(None, output_src_all, input_src_all)
        opcodes = sm.get_opcodes()
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ì›ë¬¸'] += input_src_all[j1:j2]
                else:
                    result_df.loc[len(result_df)] = [df.shape[0], input_src_all[j1:j2], '', 1.0, 'integrity', 'src_patch']
            elif tag == 'delete':
                if len(result_df) > 0:
                    last = result_df.at[result_df.index[-1], 'ì›ë¬¸']
                    result_df.at[result_df.index[-1], 'ì›ë¬¸'] = last.replace(output_src_all[i1:i2], '', 1)
    
    # ë²ˆì—­ë¬¸ ë³´ì™„
    if input_tgt_all != output_tgt_all:
        print('âš ï¸ ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: ëˆ„ë½/ì¤‘ë³µ ë³´ì • ì‹œë„')
        sm = SequenceMatcher(None, output_tgt_all, input_tgt_all)
        opcodes = sm.get_opcodes()
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸'] += input_tgt_all[j1:j2]
                else:
                    result_df.loc[len(result_df)] = [df.shape[0], '', input_tgt_all[j1:j2], 1.0, 'integrity', 'tgt_patch']
            elif tag == 'delete':
                if len(result_df) > 0:
                    last = result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸']
                    result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸'] = last.replace(output_tgt_all[i1:i2], '', 1)
    
    # ìµœì¢… ì €ì¥
    result_df = result_df[final_columns]
    result_df.to_excel(output_file, index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë¬¸ì¥ ìŒ ìƒì„±")
    if use_spacy_tokenizer:
        print(f"ğŸ”— spaCy + í† í¬ë‚˜ì´ì € ìœµí•© ë°©ì‹ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤")
    else:
        print(f"ğŸ”„ Vice Versa í† í¬ë‚˜ì´ì € ë°©ì‹ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    return result_df