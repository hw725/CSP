"""ê´„í˜¸ ë° êµ¬ë‘ì  ì²˜ë¦¬ ëª¨ë“ˆ - ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥"""

import logging
import regex  # ğŸ†• ìœ ë‹ˆì½”ë“œ ì†ì„± ì •ê·œì‹
import re  # ê´„í˜¸ ì¶”ì¶œ ë° íŒ¨í„´ ì»´íŒŒì¼ìš©
import numpy as np  # ì„ë² ë”© ê³„ì‚°ìš©
from typing import List, Dict, Any, Tuple
import hashlib
from difflib import SequenceMatcher
import pandas as pd  # ğŸ”§ ëˆ„ë½ëœ import ì¶”ê°€

logger = logging.getLogger(__name__)

class IntegrityGuard:
    """SA ì „ìš© ë¬´ê²°ì„± ë³´í˜¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.original_checksums = {}
        self.processing_history = []
        self.restoration_count = 0
    
    def register_original(self, text_id: str, text: str, stage: str = "input"):
        """ì›ë³¸ í…ìŠ¤íŠ¸ ë“±ë¡ ë° ì²´í¬ì„¬ ì €ì¥"""
        if not isinstance(text, str):
            text = str(text)
        
        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ì²´í¬ì„¬ ê³„ì‚° (ê³µë°±/ê°œí–‰ ë¬´ì‹œ)
        normalized = ''.join(text.split())
        checksum = hashlib.sha256(normalized.encode('utf-8')).hexdigest()
        
        self.original_checksums[text_id] = {
            'original_text': text,
            'normalized': normalized,
            'checksum': checksum,
            'length': len(normalized),
            'stage': stage
        }
        
        self.processing_history.append(f"REGISTER {text_id}: {stage} - {len(text)}ì ({checksum[:8]})")
        logger.debug(f"ë¬´ê²°ì„± ë“±ë¡: {text_id} - {stage}")
    
    def verify_integrity(self, text_id: str, processed_text: str, stage: str = "output") -> Tuple[bool, str, Dict]:
        """ë¬´ê²°ì„± ê²€ì¦"""
        if text_id not in self.original_checksums:
            return False, f"ì›ë³¸ ë°ì´í„° ë¯¸ë“±ë¡: {text_id}", {}
        
        original_info = self.original_checksums[text_id]
        
        if not isinstance(processed_text, str):
            processed_text = str(processed_text)
        
        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ë¹„êµ
        processed_normalized = ''.join(processed_text.split())
        processed_checksum = hashlib.sha256(processed_normalized.encode('utf-8')).hexdigest()
        
        # ì²´í¬ì„¬ ë¹„êµ
        integrity_valid = (original_info['checksum'] == processed_checksum)
        
        # ìƒì„¸ ì •ë³´
        verification_info = {
            'original_checksum': original_info['checksum'],
            'processed_checksum': processed_checksum,
            'original_length': original_info['length'],
            'processed_length': len(processed_normalized),
            'length_diff': len(processed_normalized) - original_info['length'],
            'character_accuracy': self._calculate_character_accuracy(original_info['normalized'], processed_normalized)
        }
        
        if integrity_valid:
            message = f"ë¬´ê²°ì„± ê²€ì¦ ì„±ê³µ: {stage}"
            self.processing_history.append(f"VERIFY_OK {text_id}: {stage} - ì²´í¬ì„¬ ì¼ì¹˜")
        else:
            message = f"ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {stage} - ê¸¸ì´ì°¨ì´ {verification_info['length_diff']}ì"
            self.processing_history.append(f"VERIFY_FAIL {text_id}: {stage} - {message}")
            logger.error(f"ë¬´ê²°ì„± ì‹¤íŒ¨: {text_id} - {message}")
        
        return integrity_valid, message, verification_info
    
    def _calculate_character_accuracy(self, original: str, processed: str) -> float:
        """ë¬¸ì ë‹¨ìœ„ ì •í™•ë„ ê³„ì‚°"""
        if not original:
            return 1.0 if not processed else 0.0
        
        sm = SequenceMatcher(None, original, processed)
        return sm.ratio()
    
    def restore_integrity(self, text_id: str, corrupted_parts: List[str], method: str = "auto") -> Tuple[List[str], bool]:
        """ë¬´ê²°ì„± ë³µì› ì‹œë„"""
        if text_id not in self.original_checksums:
            logger.error(f"ë³µì› ë¶ˆê°€: ì›ë³¸ ë°ì´í„° ì—†ìŒ - {text_id}")
            return corrupted_parts, False
        
        original_info = self.original_checksums[text_id]
        original_text = original_info['original_text']
        original_normalized = original_info['normalized']
        
        # í˜„ì¬ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê²°í•©
        corrupted_combined = ''.join(corrupted_parts)
        corrupted_normalized = ''.join(corrupted_combined.split())
        
        logger.info(f"ë¬´ê²°ì„± ë³µì› ì‹œì‘: {text_id} - {method}")
        self.restoration_count += 1
        
        try:
            if method == "sequence_matcher":
                restored_parts = self._restore_with_sequence_matcher(
                    original_text, corrupted_parts, original_normalized, corrupted_normalized
                )
            elif method == "character_diff":
                restored_parts = self._restore_with_character_diff(
                    original_text, corrupted_parts, original_normalized, corrupted_normalized
                )
            else:  # auto
                # ë¨¼ì € sequence_matcher ì‹œë„
                restored_parts = self._restore_with_sequence_matcher(
                    original_text, corrupted_parts, original_normalized, corrupted_normalized
                )
                
                # ì—¬ì „íˆ ì‹¤íŒ¨í•˜ë©´ character_diff ì‹œë„
                restored_combined = ''.join(restored_parts)
                if ''.join(restored_combined.split()) != original_normalized:
                    restored_parts = self._restore_with_character_diff(
                        original_text, corrupted_parts, original_normalized, corrupted_normalized
                    )
            
            # ë³µì› ê²€ì¦
            restored_combined = ''.join(restored_parts)
            restored_normalized = ''.join(restored_combined.split())
            
            success = (restored_normalized == original_normalized)
            
            if success:
                logger.info(f"ë¬´ê²°ì„± ë³µì› ì„±ê³µ: {text_id} - {len(restored_parts)}ê°œ ë¶€ë¶„")
                self.processing_history.append(f"RESTORE_OK {text_id}: {method} - ì„±ê³µ")
            else:
                logger.error(f"ë¬´ê²°ì„± ë³µì› ì‹¤íŒ¨: {text_id} - {method}")
                self.processing_history.append(f"RESTORE_FAIL {text_id}: {method} - ì‹¤íŒ¨")
                # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
                restored_parts = [original_text]
            
            return restored_parts, success
            
        except Exception as e:
            logger.error(f"ë¬´ê²°ì„± ë³µì› ì¤‘ ì˜¤ë¥˜: {e}")
            self.processing_history.append(f"RESTORE_ERROR {text_id}: {str(e)}")
            return [original_text], False
    
    def _restore_with_sequence_matcher(self, original_text: str, corrupted_parts: List[str], 
                                     original_normalized: str, corrupted_normalized: str) -> List[str]:
        """SequenceMatcherë¥¼ ì‚¬ìš©í•œ ë³µì›"""
        
        sm = SequenceMatcher(None, corrupted_normalized, original_normalized)
        opcodes = sm.get_opcodes()
        
        restored_parts = corrupted_parts[:]
        cumulative_offset = 0
        
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                # ëˆ„ë½ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
                missing_text = original_normalized[j1:j2]
                
                if restored_parts:
                    # ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€
                    restored_parts[-1] += missing_text
                else:
                    restored_parts.append(missing_text)
                
                logger.debug(f"ëˆ„ë½ í…ìŠ¤íŠ¸ ë³µì›: '{missing_text}'")
                
            elif tag == 'delete':
                # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ì œê±°
                excess_text = corrupted_normalized[i1:i2]
                
                # í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë¶€ë¶„ì—ì„œ ì œê±°
                for k, part in enumerate(restored_parts):
                    if excess_text in part:
                        restored_parts[k] = part.replace(excess_text, '', 1)
                        logger.debug(f"ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°: '{excess_text}'")
                        break
        
        return restored_parts
    
    def _restore_with_character_diff(self, original_text: str, corrupted_parts: List[str],
                                   original_normalized: str, corrupted_normalized: str) -> List[str]:
        """ë¬¸ì ë‹¨ìœ„ ì°¨ì´ ë¶„ì„ì„ í†µí•œ ë³µì›"""
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ corrupted_parts ìˆ˜ë§Œí¼ ê· ë“± ë¶„í• 
        if not corrupted_parts:
            return [original_text]
        
        part_count = len(corrupted_parts)
        text_length = len(original_text)
        part_length = text_length // part_count
        remainder = text_length % part_count
        
        restored_parts = []
        start = 0
        
        for i in range(part_count):
            current_length = part_length + (1 if i < remainder else 0)
            end = start + current_length
            
            if end > text_length:
                end = text_length
            
            restored_parts.append(original_text[start:end])
            start = end
        
        # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€
        if start < text_length:
            if restored_parts:
                restored_parts[-1] += original_text[start:]
            else:
                restored_parts.append(original_text[start:])
        
        return restored_parts
    
    def get_integrity_report(self) -> Dict:
        """ë¬´ê²°ì„± ë³´ê³ ì„œ ìƒì„±"""
        return {
            'total_registered': len(self.original_checksums),
            'restoration_attempts': self.restoration_count,
            'processing_history': self.processing_history[-20:],  # ìµœê·¼ 20ê°œ
            'registered_items': list(self.original_checksums.keys())
        }

# ì „ì—­ ë¬´ê²°ì„± ë³´í˜¸ì
integrity_guard = IntegrityGuard()

def safe_mask_brackets(text: str, text_type: str = 'source') -> Tuple[str, List[Dict]]:
    """ë¬´ê²°ì„± ë³´ì¥ ê´„í˜¸ ë§ˆìŠ¤í‚¹"""
    
    text_id = f"mask_{text_type}_{id(text)}"
    integrity_guard.register_original(text_id, text, "pre_mask")
    
    try:
        masked_text, bracket_masks = mask_brackets(text, text_type)
        
        # ë¬´ê²°ì„± ê²€ì¦
        restored_for_verification = restore_brackets(masked_text, bracket_masks)
        is_valid, message, info = integrity_guard.verify_integrity(text_id, restored_for_verification, "mask_verify")
        
        if not is_valid:
            logger.warning(f"ê´„í˜¸ ë§ˆìŠ¤í‚¹ ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
            # ë³µì› ì‹œë„
            corrected_parts, success = integrity_guard.restore_integrity(text_id, [masked_text], "sequence_matcher")
            if success and corrected_parts:
                masked_text = corrected_parts[0]
                # ìƒˆë¡œìš´ ë§ˆìŠ¤í¬ ìƒì„± í•„ìš”ì‹œ
                bracket_masks = []  # ì‹¤íŒ¨ì‹œ ë¹ˆ ë§ˆìŠ¤í¬
        
        return masked_text, bracket_masks
        
    except Exception as e:
        logger.error(f"ê´„í˜¸ ë§ˆìŠ¤í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
        return text, []  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜

def safe_restore_brackets(text: str, bracket_masks: List[Dict]) -> str:
    """ë¬´ê²°ì„± ë³´ì¥ ê´„í˜¸ ë³µì›"""
    
    try:
        restored_text = restore_brackets(text, bracket_masks)
        
        # ê¸°ë³¸ ê²€ì¦ (ë§ˆìŠ¤í¬ ìˆ˜ì™€ ë³µì›ëœ ê´„í˜¸ ìˆ˜ ë¹„êµ)
        if bracket_masks:
            expected_brackets = len(bracket_masks)
            actual_brackets = len([m for m in bracket_masks if m.get('content', '') in restored_text])
            
            if expected_brackets != actual_brackets:
                logger.warning(f"ê´„í˜¸ ë³µì› ë¶ˆì™„ì „: ì˜ˆìƒ {expected_brackets}ê°œ, ì‹¤ì œ {actual_brackets}ê°œ")
        
        return restored_text
        
    except Exception as e:
        logger.error(f"ê´„í˜¸ ë³µì› ì¤‘ ì˜¤ë¥˜: {e}")
        return text  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜

def mask_brackets(text: str, text_type: str = 'source') -> Tuple[str, List[Dict]]:
    """
    ê´„í˜¸ì™€ ê·¸ ë‚´ìš©ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì •ë ¬ í’ˆì§ˆ í–¥ìƒ
    ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥
    """
    if not text or not isinstance(text, str):
        return str(text), []
    
    # ê´„í˜¸ íŒ¨í„´ ì •ì˜ (ì¤‘ì²© ì§€ì›)
    bracket_patterns = [
        (r'\([^()]*\)', 'parentheses'),     # ì†Œê´„í˜¸
        (r'\[[^\[\]]*\]', 'square'),        # ëŒ€ê´„í˜¸  
        (r'\{[^{}]*\}', 'curly'),           # ì¤‘ê´„í˜¸
        (r'ã€Œ[^ã€Œã€]*ã€', 'corner'),          # ëª¨ì„œë¦¬ ê´„í˜¸
        (r'ã€[^ã€ã€]*ã€', 'double_corner'),   # ì´ì¤‘ ëª¨ì„œë¦¬ ê´„í˜¸
        (r'ã€ˆ[^ã€ˆã€‰]*ã€‰', 'angle'),           # êº¾ì‡  ê´„í˜¸
        (r'ã€Š[^ã€Šã€‹]*ã€‹', 'double_angle'),    # ì´ì¤‘ êº¾ì‡  ê´„í˜¸
    ]
    
    masked_text = text
    bracket_masks = []
    mask_counter = 0
    
    # ê° ê´„í˜¸ íŒ¨í„´ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹
    for pattern, bracket_type in bracket_patterns:
        matches = list(regex.finditer(pattern, masked_text))
        
        # ë’¤ì—ì„œë¶€í„° ì²˜ë¦¬í•˜ì—¬ ì¸ë±ìŠ¤ ë³€í™” ë°©ì§€
        for match in reversed(matches):
            start, end = match.span()
            content = match.group()
            
            # ë§ˆìŠ¤í¬ í† í° ìƒì„±
            mask_token = f"__BRACKET_MASK_{mask_counter}__"
            mask_counter += 1
            
            # ë§ˆìŠ¤í¬ ì •ë³´ ì €ì¥
            bracket_masks.insert(0, {
                'mask_token': mask_token,
                'content': content,
                'start_pos': start,
                'end_pos': end,
                'bracket_type': bracket_type,
                'text_type': text_type
            })
            
            # í…ìŠ¤íŠ¸ì—ì„œ ê´„í˜¸ë¥¼ ë§ˆìŠ¤í¬ë¡œ ëŒ€ì²´
            masked_text = masked_text[:start] + mask_token + masked_text[end:]
    
    logger.debug(f"ê´„í˜¸ ë§ˆìŠ¤í‚¹ ì™„ë£Œ: {len(bracket_masks)}ê°œ ê´„í˜¸ ì²˜ë¦¬")
    return masked_text, bracket_masks

def restore_brackets(text: str, bracket_masks: List[Dict]) -> str:
    """
    ë§ˆìŠ¤í‚¹ëœ ê´„í˜¸ë¥¼ ì›ë˜ ìœ„ì¹˜ì— ë³µì›
    ì™„ë²½í•œ ë¬´ê²°ì„± ë³´ì¥
    """
    if not text or not bracket_masks:
        return text
    
    restored_text = text
    
    # ë§ˆìŠ¤í¬ í† í°ì„ ì›ë˜ ê´„í˜¸ë¡œ ë³µì›
    for mask_info in bracket_masks:
        mask_token = mask_info['mask_token']
        content = mask_info['content']
        
        if mask_token in restored_text:
            restored_text = restored_text.replace(mask_token, content, 1)
        else:
            logger.warning(f"ë§ˆìŠ¤í¬ í† í° ëˆ„ë½: {mask_token}")
    
    # ë³µì›ë˜ì§€ ì•Šì€ ë§ˆìŠ¤í¬ í† í° ê²€ì‚¬
    remaining_masks = regex.findall(r'__BRACKET_MASK_\d+__', restored_text)
    if remaining_masks:
        logger.error(f"ë³µì›ë˜ì§€ ì•Šì€ ë§ˆìŠ¤í¬: {remaining_masks}")
        # ë‚¨ì€ ë§ˆìŠ¤í¬ í† í°ë“¤ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
        for remaining in remaining_masks:
            restored_text = restored_text.replace(remaining, '')
    
    logger.debug(f"ê´„í˜¸ ë³µì› ì™„ë£Œ: {len(bracket_masks)}ê°œ ê´„í˜¸ ë³µì›")
    return restored_text

def safe_split_sentences(text: str, max_length: int = 150, method: str = "punctuation") -> List[str]:
    """ë¬´ê²°ì„± ë³´ì¥ ë¬¸ì¥ ë¶„í• """
    
    if not text or not text.strip():
        return []
    
    text_id = f"split_{method}_{id(text)}"
    integrity_guard.register_original(text_id, text, f"pre_split_{method}")
    
    try:
        # ê¸°ì¡´ ë¶„í•  í•¨ìˆ˜ í˜¸ì¶œ
        if method == "punctuation":
            sentences = split_by_punctuation(text, max_length)
        elif method == "spacy":
            sentences = split_by_spacy(text, max_length)
        else:
            sentences = split_by_punctuation(text, max_length)  # ê¸°ë³¸ê°’
        
        if not sentences:
            sentences = [text]
        
        # ë¬´ê²°ì„± ê²€ì¦
        combined_result = ''.join(sentences)
        is_valid, message, info = integrity_guard.verify_integrity(text_id, combined_result, f"post_split_{method}")
        
        if not is_valid:
            logger.warning(f"ë¬¸ì¥ ë¶„í•  ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
            # ë³µì› ì‹œë„
            restored_sentences, success = integrity_guard.restore_integrity(text_id, sentences)
            
            if success:
                sentences = restored_sentences
                logger.info(f"ë¬¸ì¥ ë¶„í•  ë¬´ê²°ì„± ë³µì› ì„±ê³µ")
            else:
                logger.error(f"ë¬¸ì¥ ë¶„í•  ë¬´ê²°ì„± ë³µì› ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜")
                sentences = [text]
        
        return sentences
        
    except Exception as e:
        logger.error(f"ë¬¸ì¥ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
        return [text]

def split_by_punctuation(text: str, max_length: int = 150) -> List[str]:
    """êµ¬ë‘ì  ê¸°ë°˜ ë¬¸ì¥ ë¶„í•  (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    
    if not text.strip():
        return []
    
    # êµ¬ë‘ì  íŒ¨í„´ (í•œêµ­ì–´ + ì¤‘êµ­ì–´)
    sentence_enders = r'[.!?ã€‚ï¼ï¼Ÿï¼›]'
    
    # êµ¬ë‘ì ìœ¼ë¡œ ë¶„í• 
    sentences = regex.split(sentence_enders, text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # ê¸¸ì´ ì œí•œ ì ìš©
    final_sentences = []
    for sentence in sentences:
        if len(sentence) <= max_length:
            final_sentences.append(sentence)
        else:
            # ê¸´ ë¬¸ì¥ì€ ì‰¼í‘œë¡œ ì¶”ê°€ ë¶„í• 
            sub_sentences = sentence.split(',')
            for sub in sub_sentences:
                if sub.strip():
                    final_sentences.append(sub.strip())
    
    return final_sentences

def split_by_spacy(text: str, max_length: int = 150) -> List[str]:
    """spaCy ê¸°ë°˜ ë¬¸ì¥ ë¶„í• """
    
    try:
        import spacy
        
        # í•œêµ­ì–´ ëª¨ë¸ ì‹œë„
        try:
            nlp = spacy.load("ko_core_news_sm")
        except OSError:
            # ì˜ì–´ ëª¨ë¸ í´ë°±
            try:
                nlp = spacy.load("en_core_web_sm")
            except OSError:
                logger.warning("spaCy ëª¨ë¸ ì—†ìŒ, êµ¬ë‘ì  ë¶„í•  ì‚¬ìš©")
                return split_by_punctuation(text, max_length)
        
        doc = nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        
        # ê¸¸ì´ ì œí•œ ì ìš©
        final_sentences = []
        for sentence in sentences:
            if len(sentence) <= max_length:
                final_sentences.append(sentence)
            else:
                # ê¸´ ë¬¸ì¥ì€ êµ¬ë‘ì  ë¶„í• ë¡œ í´ë°±
                sub_sentences = split_by_punctuation(sentence, max_length)
                final_sentences.extend(sub_sentences)
        
        return final_sentences
        
    except ImportError:
        logger.warning("spaCy ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ, êµ¬ë‘ì  ë¶„í•  ì‚¬ìš©")
        return split_by_punctuation(text, max_length)

def process_text_with_integrity(text: str, processing_func, text_id: str = None, **kwargs) -> Any:
    """ì„ì˜ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ì— ë¬´ê²°ì„± ë³´ì¥ ì ìš©"""
    
    if text_id is None:
        text_id = f"process_{id(text)}"
    
    integrity_guard.register_original(text_id, text, "pre_process")
    
    try:
        result = processing_func(text, **kwargs)
        
        # ê²°ê³¼ê°€ ë¬¸ìì—´ì´ë©´ ê²€ì¦
        if isinstance(result, str):
            is_valid, message, info = integrity_guard.verify_integrity(text_id, result, "post_process")
            
            if not is_valid:
                logger.warning(f"ì²˜ë¦¬ í•¨ìˆ˜ ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
                # ë³µì› ì‹œë„
                restored_parts, success = integrity_guard.restore_integrity(text_id, [result])
                if success and restored_parts:
                    result = restored_parts[0]
        
        # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ë©´ ê²°í•©í•´ì„œ ê²€ì¦
        elif isinstance(result, list) and all(isinstance(item, str) for item in result):
            combined = ''.join(result)
            is_valid, message, info = integrity_guard.verify_integrity(text_id, combined, "post_process")
            
            if not is_valid:
                logger.warning(f"ì²˜ë¦¬ í•¨ìˆ˜ ë¬´ê²°ì„± ì‹¤íŒ¨: {message}")
                # ë³µì› ì‹œë„
                restored_parts, success = integrity_guard.restore_integrity(text_id, result)
                if success:
                    result = restored_parts
        
        return result
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return text  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜

def get_integrity_status() -> Dict:
    """í˜„ì¬ ë¬´ê²°ì„± ìƒíƒœ ë°˜í™˜"""
    return integrity_guard.get_integrity_report()

def reset_integrity_guard():
    """ë¬´ê²°ì„± ë³´í˜¸ì ì´ˆê¸°í™”"""
    global integrity_guard
    integrity_guard = IntegrityGuard()
    logger.info("ë¬´ê²°ì„± ë³´í˜¸ì ì´ˆê¸°í™” ì™„ë£Œ")

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì˜ ë¬´ê²°ì„± ë³´ì¥ ë˜í¼
def safe_extract_quotes(text: str) -> Tuple[str, List[Dict]]:
    """ë¬´ê²°ì„± ë³´ì¥ ì¸ìš©ë¶€í˜¸ ì¶”ì¶œ"""
    return process_text_with_integrity(
        text, 
        lambda t: extract_quotes(t),
        f"quotes_{id(text)}"
    )

def safe_normalize_punctuation(text: str) -> str:
    """ë¬´ê²°ì„± ë³´ì¥ êµ¬ë‘ì  ì •ê·œí™”"""
    return process_text_with_integrity(
        text,
        lambda t: normalize_punctuation(t),
        f"normalize_{id(text)}"
    )

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (ë¡œì§ ë³€ê²½ ì—†ì´ ìœ ì§€)
def extract_quotes(text: str) -> Tuple[str, List[Dict]]:
    """ì¸ìš©ë¶€í˜¸ ë° ë‚´ìš© ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)"""
    # ê¸°ì¡´ êµ¬í˜„...
    return text, []

def normalize_punctuation(text: str) -> str:
    """êµ¬ë‘ì  ì •ê·œí™” (ê¸°ì¡´ ë¡œì§)"""
    # ê¸°ì¡´ êµ¬í˜„...
    return text

# ì¶”ê°€ ì•ˆì „ ì¥ì¹˜
def validate_text_integrity(original: str, processed: str, tolerance: float = 0.95) -> bool:
    """í…ìŠ¤íŠ¸ ë¬´ê²°ì„± ê²€ì¦ (ê³µê°œ API)"""
    
    if not original or not processed:
        return not original and not processed
    
    # ì •ê·œí™” í›„ ë¹„êµ
    orig_normalized = ''.join(original.split())
    proc_normalized = ''.join(processed.split())
    
    if orig_normalized == proc_normalized:
        return True
    
    # í—ˆìš© ì˜¤ì°¨ ë‚´ ìœ ì‚¬ë„ í™•ì¸
    sm = SequenceMatcher(None, orig_normalized, proc_normalized)
    similarity = sm.ratio()
    
    return similarity >= tolerance

def emergency_restore_text(text_fragments: List[str], original_text: str) -> List[str]:
    """ë¹„ìƒ í…ìŠ¤íŠ¸ ë³µì›"""
    
    logger.warning("ë¹„ìƒ í…ìŠ¤íŠ¸ ë³µì› ì‹¤í–‰")
    
    if not text_fragments:
        return [original_text]
    
    # ì›ë³¸ì„ ì¡°ê° ìˆ˜ì— ë§ì¶° ê· ë“± ë¶„í• 
    fragment_count = len(text_fragments)
    text_length = len(original_text)
    
    if fragment_count == 1:
        return [original_text]
    
    avg_length = text_length // fragment_count
    remainder = text_length % fragment_count
    
    restored_fragments = []
    start = 0
    
    for i in range(fragment_count):
        length = avg_length + (1 if i < remainder else 0)
        end = min(start + length, text_length)
        
        if start < text_length:
            restored_fragments.append(original_text[start:end])
        else:
            restored_fragments.append('')
        
        start = end
    
    # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì¶”ê°€
    if start < text_length:
        if restored_fragments:
            restored_fragments[-1] += original_text[start:]
        else:
            restored_fragments.append(original_text[start:])
    
    logger.info(f"ë¹„ìƒ ë³µì› ì™„ë£Œ: {len(restored_fragments)}ê°œ ì¡°ê°")
    return restored_fragments